{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-30T09:53:48.401497994Z",
     "start_time": "2024-04-30T09:53:48.350126180Z"
    }
   },
   "outputs": [],
   "source": [
    "from libraries import *\n",
    "import spacy\n",
    "from tqdm import tqdm  "
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "doc = fitz.open(\"book.pdf\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-30T09:53:48.674074804Z",
     "start_time": "2024-04-30T09:53:48.633733459Z"
    }
   },
   "id": "9459a8b10c472851",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "chapter_starts_d = {\"Chapter 1\": 26,\n",
    "                  \"Chapter 2\": 70,\n",
    "                  \"Chapter 3\": 105,\n",
    "                  \"Chapter 4\": 145,\n",
    "                  \"Chapter 5\": 190,\n",
    "                  \"Chapter 6\": 219,\n",
    "                  \"Chapter 7\": 253,\n",
    "                  \"Chapter 8\": 300,\n",
    "                  \"Chapter 9\": 343,\n",
    "                  \"Chapter 10\": 353,\n",
    "                  \"Chapter 11\": 397,\n",
    "                  \"Chapter 12\": 437,\n",
    "                  \"Chapter 13\": 481,\n",
    "                  \"Chapter 14\": 513,\n",
    "                  \"Chapter 15\": 560,\n",
    "                  \"Chapter 16\": 600,\n",
    "                  \"Chapter 17\": 638,\n",
    "                  \"Chapter 18\": 681,\n",
    "                  \"Chapter 19\": 713,\n",
    "                  \"Chapter 20\": 732,\n",
    "                  \"Chapter 21\": 748,\n",
    "                  \"Out of chapter 21\" : 793}\n",
    "chapter_starts = list(chapter_starts_d.values())\n",
    "def extract_chapters_from_pdf(pdf_path, chapter_starts):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    chapter_dict = {}\n",
    "    num_chapters = len(chapter_starts) - 1  # On suppose que la dernière entrée est la fin du dernier chapitre\n",
    "\n",
    "    # Extraire le texte pour chaque chapitre\n",
    "    for i in range(num_chapters):\n",
    "        start_page = chapter_starts[i]\n",
    "        end_page = chapter_starts[i + 1] - 1  # La page de fin est la page de début du prochain chapitre moins 1\n",
    "        text = \"\"\n",
    "\n",
    "        # Concaténer le texte de chaque page du chapitre\n",
    "        for page_num in range(start_page, end_page + 1):  # +1 pour inclure la page de fin\n",
    "            page = doc.load_page(page_num)\n",
    "            text += page.get_text()\n",
    "\n",
    "        chapter_dict[i + 1] = text  # Utiliser un nom générique avec numéro de chapitre\n",
    "\n",
    "    return chapter_dict\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-30T09:53:48.817951468Z",
     "start_time": "2024-04-30T09:53:48.805348847Z"
    }
   },
   "id": "6062c81e9d55518c",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "chapter_dict = extract_chapters_from_pdf(pdf_path=\"book.pdf\", chapter_starts=chapter_starts)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-30T09:53:51.052688495Z",
     "start_time": "2024-04-30T09:53:48.993671487Z"
    }
   },
   "id": "b627c456383107bb",
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "'2\\u2002 \\u2002 Chapter 1 / Basic Concepts and Computer Evolution \\n\\t 1.1\\t Organization and Architecture\\nIn describing computers, a distinction is often made between computer architec-\\nture and computer organization. Although it is difficult to give precise definitions \\nfor these terms, a consensus exists about the general areas covered by each. For \\nexample, see [VRAN80], [SIEW82], and [BELL78a]; an interesting alternative view \\nis presented in [REDD76].\\nComputer architecture refers to those attributes of a system visible to a pro-\\ngrammer or, put another way, those attributes that have a direct impact on the \\nlogical execution of a program. A term that is often used interchangeably with com-\\nputer architecture is instruction set architecture (ISA). The ISA defines instruction \\nformats, instruction opcodes, registers, instruction and data memory; the effect of \\nexecuted instructions on the registers and memory; and an algorithm for control-\\nling instruction execution. Computer organization refers to the operational units \\nand their interconnections that realize the architectural specifications. Examples of \\narchitectural attributes include the instruction set, the number of bits used to repre-\\nsent various data types (e.g., numbers, characters), I/O mechanisms, and techniques \\nfor addressing memory. Organizational attributes include those hardware details \\ntransparent to the programmer, such as control signals; interfaces between the com-\\nputer and peripherals; and the memory technology used.\\nFor example, it is an architectural design issue whether a computer will have \\na multiply instruction. It is an organizational issue whether that instruction will be \\nimplemented by a special multiply unit or by a mechanism that makes repeated \\nuse of the add unit of the system. The organizational decision may be based on the \\nanticipated frequency of use of the multiply instruction, the relative speed of the \\ntwo approaches, and the cost and physical size of a special multiply unit.\\nHistorically, and still today, the distinction between architecture and organ-\\nization has been an important one. Many computer manufacturers offer a family of \\ncomputer models, all with the same architecture but with differences in organization. \\nConsequently, the different models in the family have different price and perform-\\nance characteristics. Furthermore, a particular architecture may span many years \\nand encompass a number of different computer models, its organization changing \\nwith changing technology. A prominent example of both these phenomena is the \\nIBM System/370 architecture. This architecture was first introduced in 1970 and \\nLearning Objectives\\nAfter studying this chapter, you should be able to:\\nr\\nr Explain the general functions and structure of a digital computer.\\nr\\nr Present an overview of the evolution of computer technology from early \\ndigital computers to the latest microprocessors.\\nr\\nr Present an overview of the evolution of the x86 architecture.\\nr\\nr Define embedded systems and list some of the requirements and constraints \\nthat various embedded systems must meet.\\n1.2 / Structure and Function\\u2002 \\u20023\\nincluded a number of models. The customer with modest requirements could buy a \\ncheaper, slower model and, if demand increased, later upgrade to a more expensive, \\nfaster model without having to abandon software that had already been developed. \\nOver the years, IBM has introduced many new models with improved technology \\nto replace older models, offering the customer greater speed, lower cost, or both. \\nThese newer models retained the same architecture so that the customer’s soft-\\nware investment was protected. Remarkably, the System/370 architecture, with a \\nfew enhancements, has survived to this day as the architecture of IBM’s mainframe \\nproduct line.\\nIn a class of computers called microcomputers, the relationship between archi-\\ntecture and organization is very close. Changes in technology not only influence \\norganization but also result in the introduction of more powerful and more complex \\narchitectures. Generally, there is less of a requirement for \\xad\\ngeneration-\\u200b\\n\\xad\\nto-\\u200b\\n\\xad\\ngeneration \\ncompatibility for these smaller machines. Thus, there is more interplay between \\norganizational and architectural design decisions. An intriguing example of this is \\nthe reduced instruction set computer (RISC), which we examine in Chapter\\xa015.\\nThis book examines both computer organization and computer architecture. \\nThe emphasis is perhaps more on the side of organization. However, because a \\ncomputer organization must be designed to implement a particular architectural \\nspecification, a thorough treatment of organization requires a detailed examination \\nof architecture as well.\\n\\t 1.2\\t Structure and Function\\nA computer is a complex system; contemporary computers contain millions of \\nelementary electronic components. How, then, can one clearly describe them? The \\nkey is to recognize the hierarchical nature of most complex systems, including the \\ncomputer [SIMO96]. A hierarchical system is a set of interrelated subsystems, each \\nof the latter, in turn, hierarchical in structure until we reach some lowest level of \\nelementary subsystem.\\nThe hierarchical nature of complex systems is essential to both their design \\nand their description. The designer need only deal with a particular level of the \\nsystem at a time. At each level, the system consists of a set of components and \\ntheir interrelationships. The behavior at each level depends only on a simplified, \\nabstracted characterization of the system at the next lower level. At each level, the \\ndesigner is concerned with structure and function:\\n■\\n■Structure: The way in which the components are interrelated.\\n■\\n■Function: The operation of each individual component as part of the structure.\\nIn terms of description, we have two choices: starting at the bottom and build-\\ning up to a complete description, or beginning with a top view and decomposing the \\nsystem into its subparts. Evidence from a number of fields suggests that the \\xad\\ntop-\\u200b\\n\\xad\\ndown approach is the clearest and most effective [WEIN75].\\nThe approach taken in this book follows from this viewpoint. The computer \\nsystem will be described from the top down. We begin with the major components \\nof a computer, describing their structure and function, and proceed to successively \\n4\\u2002 \\u2002 Chapter 1 / Basic Concepts and Computer Evolution \\nlower layers of the hierarchy. The remainder of this section provides a very brief \\noverview of this plan of attack.\\nFunction\\nBoth the structure and functioning of a computer are, in essence, simple. In general \\nterms, there are only four basic functions that a computer can perform:\\n■\\n■Data processing: Data may take a wide variety of forms, and the range of pro-\\ncessing requirements is broad. However, we shall see that there are only a few \\nfundamental methods or types of data processing.\\n■\\n■Data storage: Even if the computer is processing data on the fly (i.e., data \\ncome in and get processed, and the results go out immediately), the computer \\nmust temporarily store at least those pieces of data that are being worked on \\nat any given moment. Thus, there is at least a \\xad\\nshort-\\u200b\\n\\xad\\nterm data storage function. \\nEqually important, the computer performs a \\xad\\nlong-\\u200b\\n\\xad\\nterm data storage function. \\nFiles of data are stored on the computer for subsequent retrieval and update.\\n■\\n■Data movement: The computer’s operating environment consists of devices \\nthat serve as either sources or destinations of data. When data are received \\nfrom or delivered to a device that is directly connected to the computer, the \\nprocess is known as \\xad\\ninput–\\u200b\\n\\xad\\noutput (I/O), and the device is referred to as a \\nperipheral. When data are moved over longer distances, to or from a remote \\ndevice, the process is known as data communications.\\n■\\n■Control: Within the computer, a control unit manages the computer’s \\nresources and orchestrates the performance of its functional parts in response \\nto instructions.\\nThe preceding discussion may seem absurdly generalized. It is certainly \\npossible, even at a top level of computer structure, to differentiate a variety of func-\\ntions, but to quote [SIEW82]:\\nThere is remarkably little shaping of computer structure to fit the \\nfunction to be performed. At the root of this lies the \\xad\\ngeneral-\\u200b\\n\\xad\\npurpose \\nnature of computers, in which all the functional specialization occurs \\nat the time of programming and not at the time of design.\\nStructure\\nWe now look in a general way at the internal structure of a computer. We begin with \\na traditional computer with a single processor that employs a microprogrammed \\ncontrol unit, then examine a typical multicore structure.\\nsimple \\xad\\nsingle-\\u200b\\n\\xad\\nprocessor computer Figure\\xa01.1 provides a hierarchical view \\nof the internal structure of a traditional \\xad\\nsingle-\\u200b\\n\\xad\\nprocessor computer. There are four \\nmain structural components:\\n■\\n■Central processing unit (CPU): Controls the operation of the computer and \\nperforms its data processing functions; often simply referred to as processor.\\n■\\n■Main memory: Stores data.\\n1.2 / Structure and Function\\u2002 \\u20025\\n■\\n■I/O: Moves data between the computer and its external environment.\\n■\\n■System interconnection: Some mechanism that provides for communication \\namong CPU, main memory, and I/O.\\xa0A common example of system intercon-\\nnection is by means of a system bus, consisting of a number of conducting \\nwires to which all the other components attach.\\nThere may be one or more of each of the aforementioned components. Tra-\\nditionally, there has been just a single processor. In recent years, there has been \\nincreasing use of multiple processors in a single computer. Some design issues relat-\\ning to multiple processors crop up and are discussed as the text proceeds; Part Five \\nfocuses on such computers.\\nMain\\nmemory\\nI/O\\nCPU\\nCOMPUTER\\nSystem\\nbus\\nALU\\nRegisters\\nControl\\nunit\\nCPU\\nInternal\\nbus\\nControl unit\\nregisters and\\ndecoders\\nCONTROL\\nUNIT\\nSequencing\\nlogic\\nControl\\nmemory\\nFigure\\xa01.1\\u2003 The Computer: \\xad\\nTop-\\u200b\\n\\xad\\nLevel Structure\\n6\\u2002 \\u2002 Chapter 1 / Basic Concepts and Computer Evolution \\nEach of these components will be examined in some detail in Part Two. How-\\never, for our purposes, the most interesting and in some ways the most complex \\ncomponent is the\\xa0CPU.\\xa0Its major structural components are as follows:\\n■\\n■Control unit: Controls the operation of the CPU and hence the computer.\\n■\\n■Arithmetic and logic unit (ALU): Performs the computer’s data processing \\nfunctions.\\n■\\n■Registers: Provides storage internal to the CPU.\\n■\\n■CPU interconnection: Some mechanism that provides for communication \\namong the control unit, ALU, and registers.\\nPart Three covers these components, where we will see that complexity is added by \\nthe use of parallel and pipelined organizational techniques. Finally, there are sev-\\neral approaches to the implementation of the control unit; one common approach is \\na microprogrammed implementation. In essence, a microprogrammed control unit \\noperates by executing microinstructions that define the functionality of the control \\nunit. With this approach, the structure of the control unit can be depicted, as in \\n\\xad\\nFigure\\xa01.1. This structure is examined in Part Four.\\nmulticore computer structure As was mentioned, contemporary \\ncomputers generally have multiple processors. When these processors all reside \\non a single chip, the term multicore computer is used, and each processing unit \\n(consisting of a control unit, ALU, registers, and perhaps cache) is called a core. To \\nclarify the terminology, this text will use the following definitions.\\n■\\n■Central processing unit (CPU): That portion of a computer that fetches and \\nexecutes instructions. It consists of an ALU, a control unit, and registers. \\nIn a system with a single processing unit, it is often simply referred to as a \\nprocessor.\\n■\\n■Core: An individual processing unit on a processor chip. A core may be equiv-\\nalent in functionality to a CPU on a \\xad\\nsingle-\\u200b\\n\\xad\\nCPU system. Other specialized pro-\\ncessing units, such as one optimized for vector and matrix operations, are also \\nreferred to as cores.\\n■\\n■Processor: A physical piece of silicon containing one or more cores. The \\nprocessor is the computer component that interprets and executes instruc-\\ntions. If a processor contains multiple cores, it is referred to as a multicore \\nprocessor.\\nAfter about a decade of discussion, there is broad industry consensus on this usage.\\nAnother prominent feature of contemporary computers is the use of multiple \\nlayers of memory, called cache memory, between the processor and main memory. \\nChapter\\xa04 is devoted to the topic of cache memory. For our purposes in this section, \\nwe simply note that a cache memory is smaller and faster than main memory and is \\nused to speed up memory access, by placing in the cache data from main memory, \\nthat is likely to be used in the near future. A greater performance improvement may \\nbe obtained by using multiple levels of cache, with level 1 (L1) closest to the core \\nand additional levels (L2, L3, and so on) progressively farther from the core. In this \\nscheme, level n is smaller and faster than level n + 1.\\n1.2 / Structure and Function\\u2002 \\u20027\\nFigure\\xa01.2 is a simplified view of the principal components of a typical mul-\\nticore computer. Most computers, including embedded computers in smartphones \\nand tablets, plus personal computers, laptops, and workstations, are housed on a \\nmotherboard. Before describing this arrangement, we need to define some terms. \\nA printed circuit board (PCB) is a rigid, flat board that holds and interconnects \\nchips and other electronic components. The board is made of layers, typically two \\nto ten, that interconnect components via copper pathways that are etched into \\nthe board. The main printed circuit board in a computer is called a system board \\nor motherboard, while smaller ones that plug into the slots in the main board are \\ncalled expansion boards.\\nThe most prominent elements on the motherboard are the chips. A chip is \\na single piece of semiconducting material, typically silicon, upon which electronic \\ncircuits and logic gates are fabricated. The resulting product is referred to as an \\nintegrated circuit.\\nMOTHERBOARD\\nPROCESSOR CHIP\\nCORE\\nProcessor\\nchip\\nMain memory chips\\nI/O chips\\nCore\\nL3 cache\\nInstruction\\nlogic\\nL1 I-cache\\nL2 instruction\\ncache\\nL2 data\\ncache\\nL1 data cache\\nArithmetic\\nand logic\\nunit (ALU)\\nLoad/\\nstore logic\\nL3 cache\\nCore\\nCore\\nCore\\nCore\\nCore\\nCore\\nCore\\nFigure\\xa01.2\\u2003 Simplified View of Major Elements of a Multicore Computer\\n8\\u2002 \\u2002 Chapter 1 / Basic Concepts and Computer Evolution \\nThe motherboard contains a slot or socket for the processor chip, which typ-\\nically contains multiple individual cores, in what is known as a multicore processor. \\nThere are also slots for memory chips, I/O controller chips, and other key computer \\ncomponents. For desktop computers, expansion slots enable the inclusion of more \\ncomponents on expansion boards. Thus, a modern motherboard connects only a \\nfew individual chip components, with each chip containing from a few thousand up \\nto hundreds of millions of transistors.\\nFigure\\xa01.2 shows a processor chip that contains eight cores and an L3 cache. \\nNot shown is the logic required to control operations between the cores and the \\ncache and between the cores and the external circuitry on the motherboard. The \\nfigure indicates that the L3 cache occupies two distinct portions of the chip surface. \\nHowever, typically, all cores have access to the entire L3 cache via the aforemen-\\ntioned control circuits. The processor chip shown in Figure\\xa01.2 does not represent \\nany specific product, but provides a general idea of how such chips are laid out.\\nNext, we zoom in on the structure of a single core, which occupies a portion of \\nthe processor chip. In general terms, the functional elements of a core are:\\n■\\n■Instruction logic: This includes the tasks involved in fetching instructions, \\nand decoding each instruction to determine the instruction operation and the \\nmemory locations of any operands.\\n■\\n■Arithmetic and logic unit (ALU): Performs the operation specified by an \\ninstruction.\\n■\\n■Load/store logic: Manages the transfer of data to and from main memory via \\ncache.\\nThe core also contains an L1 cache, split between an instruction cache \\n \\n(\\xad\\nI-\\u200b\\n\\xad\\ncache) that is used for the transfer of instructions to and from main memory, and \\nan L1 data cache, for the transfer of operands and results. Typically, today’s pro-\\ncessor chips also include an L2 cache as part of the core. In many cases, this cache \\nis also split between instruction and data caches, although a combined, single L2 \\ncache is also used.\\nKeep in mind that this representation of the layout of the core is only intended \\nto give a general idea of internal core structure. In a given product, the functional \\nelements may not be laid out as the three distinct elements shown in Figure\\xa01.2, \\nespecially if some or all of these functions are implemented as part of a micropro-\\ngrammed control unit.\\nexamples It will be instructive to look at some \\xad\\nreal-\\u200b\\n\\xad\\nworld examples that \\nillustrate the hierarchical structure of computers. Figure\\xa01.3 is a photograph of the \\nmotherboard for a computer built around two Intel \\xad\\nQuad-\\u200b\\n\\xad\\nCore Xeon processor \\nchips. Many of the elements labeled on the photograph are discussed subsequently \\nin this book. Here, we mention the most important, in addition to the processor \\nsockets:\\n■\\n■\\xad\\nPCI-\\u200b\\n\\xad\\nExpress slots for a \\xad\\nhigh-\\u200b\\n\\xad\\nend display adapter and for additional peripher-\\nals (Section\\xa03.6 describes PCIe).\\n■\\n■Ethernet controller and Ethernet ports for network connections.\\n■\\n■USB sockets for peripheral devices.\\n1.2 / Structure and Function\\u2002 \\u20029\\n2x Quad-Core Intel® Xeon® Processors\\nwith Integrated Memory Controllers\\nSix Channel DDR3-1333 Memory\\nInterfaces Up to 48GB\\nIntel® 3420\\nChipset\\nSerial ATA/300 (SATA)\\nInterfaces\\n2x USB 2.0\\nInternal\\n2x Ethernet Ports\\n10/100/1000Base-T\\nEthernet Controller\\nClock\\nPCI Express®\\nConnector A\\nPCI Express®\\nConnector B\\nPower & Backplane I/O\\nConnector C\\nVGA Video Output\\nBIOS\\n2x USB 2.0\\nExternal\\nFigure\\xa01.3\\u2003 Motherboard with Two Intel \\xad\\nQuad-\\u200b\\n\\xad\\nCore Xeon Processors\\nSource: Chassis Plans, www.chassis-plans.com\\n■\\n■Serial ATA (SATA) sockets for connection to disk memory (Section\\xa07.7 \\n\\xad\\ndiscusses Ethernet, USB, and SATA).\\n■\\n■Interfaces for DDR (double data rate) main memory chips (Section\\xa0 5.3 \\n\\xad\\ndiscusses DDR).\\n■\\n■Intel 3420 chipset is an I/O controller for direct memory access operations \\nbetween peripheral devices and main memory (Section\\xa07.5 discusses DDR).\\nFollowing our \\xad\\ntop-\\u200b\\n\\xad\\ndown strategy, as illustrated in Figures 1.1 and 1.2, we can \\nnow zoom in and look at the internal structure of a processor chip. For variety, we \\nlook at an IBM chip instead of the Intel processor chip. Figure\\xa01.4 is a photograph \\nof the processor chip for the IBM zEnterprise EC12 mainframe computer. This chip \\nhas 2.75\\xa0billion transistors. The superimposed labels indicate how the silicon real \\nestate of the chip is allocated. We see that this chip has six cores, or processors. \\nIn addition, there are two large areas labeled L3 cache, which are shared by all six \\nprocessors. The L3 control logic controls traffic between the L3 cache and the cores \\nand between the L3 cache and the external environment. Additionally, there is stor-\\nage control (SC) logic between the cores and the L3 cache. The memory controller \\n(MC) function controls access to memory external to the chip. The GX I/O bus \\ncontrols the interface to the channel adapters \\xad\\naccessing the I/O.\\nGoing down one level deeper, we examine the internal structure of a single \\ncore, as shown in the photograph of Figure\\xa01.5. Keep in mind that this is a portion \\nof the silicon surface area making up a \\xad\\nsingle-\\u200b\\n\\xad\\nprocessor chip. The main \\xad\\nsub-\\u200b\\n\\xad\\nareas \\nwithin this core area are the following:\\n■\\n■ISU (instruction sequence unit): Determines the sequence in which instructions \\nare executed in what is referred to as a superscalar architecture (Chapter\\xa016).\\n■\\n■IFU (instruction fetch unit): Logic for fetching instructions.\\n10\\u2002 \\u2002 Chapter 1 / Basic Concepts and Computer Evolution \\n■\\n■IDU (instruction decode unit): The IDU is fed from the IFU buffers, and is \\nresponsible for the parsing and decoding of all z/Architecture operation codes.\\n■\\n■LSU (\\xad\\nload-\\u200b\\n\\xad\\nstore unit): The LSU contains the 96-kB L1 data cache,1 and man-\\nages data traffic between the L2 data cache and the functional execution \\nunits. It is responsible for handling all types of operand accesses of all lengths, \\nmodes, and formats as defined in the z/Architecture.\\n■\\n■XU (translation unit):  This unit translates logical addresses from instructions \\ninto physical addresses in main memory. The XU also contains a translation \\nlookaside buffer (TLB) used to speed up memory access. TLBs are discussed \\nin Chapter\\xa08.\\n■\\n■FXU (\\xad\\nfixed-\\u200b\\n\\xad\\npoint unit): The FXU executes \\xad\\nfixed-\\u200b\\n\\xad\\npoint arithmetic operations.\\n■\\n■BFU (binary \\xad\\nfloating-\\u200b\\n\\xad\\npoint unit): The BFU handles all binary and hexadeci-\\nmal \\xad\\nfloating-\\u200b\\n\\xad\\npoint operations, as well as \\xad\\nfixed-\\u200b\\n\\xad\\npoint multiplication operations.\\n■\\n■DFU (decimal \\xad\\nfloating-\\u200b\\n\\xad\\npoint unit): The DFU handles both \\xad\\nfixed-\\u200b\\n\\xad\\npoint and \\n\\xad\\nfloating-\\u200b\\n\\xad\\npoint operations on numbers that are stored as decimal digits.\\n■\\n■RU (recovery unit): The RU keeps a copy of the complete state of the sys-\\ntem that includes all registers, collects hardware fault signals, and manages the \\nhardware recovery actions.\\nFigure\\xa01.4\\u2003 zEnterprise EC12 Processor Unit \\n(PU) chip diagram\\nSource: IBM zEnterprise EC12 Technical Guide, \\nDecember\\xa02013, SG24-8049-01. IBM, Reprinted by \\nPermission\\nFigure\\xa01.5\\u2003 zEnterprise EC12 Core layout\\nSource: IBM zEnterprise EC12 Technical Guide, \\nDecember\\xa02013, SG24-8049-01. IBM, Reprinted by \\nPermission\\n1kB = kilobyte = 2048 bytes. Numerical prefixes are explained in a document under the “Other Useful” \\ntab at ComputerScienceStudent.com.\\n1.3 / A Brief History of Computers\\u2002 \\u200211\\n■\\n■COP (dedicated \\xad\\nco-\\u200b\\n\\xad\\nprocessor): The COP is responsible for data compression \\nand encryption functions for each core.\\n■\\n■\\xad\\nI-\\u200b\\n\\xad\\ncache: This is a 64-kB L1 instruction cache, allowing the IFU to prefetch \\ninstructions before they are needed.\\n■\\n■L2 control: This is the control logic that manages the traffic through the two \\nL2 caches.\\n■\\n■\\xad\\nData-\\u200b\\n\\xad\\nL2: A 1-MB L2 data cache for all memory traffic other than instructions.\\n■\\n■\\xad\\nInstr-\\u200b\\n\\xad\\nL2: A 1-MB L2 instruction cache.\\nAs we progress through the book, the concepts introduced in this section will \\nbecome clearer.\\n\\t 1.3\\t A Brief History of Computers2\\nIn this section, we provide a brief overview of the history of the development of \\ncomputers. This history is interesting in itself, but more importantly, provides a basic \\nintroduction to many important concepts that we deal with throughout the book.\\nThe First Generation: Vacuum Tubes\\nThe first generation of computers used vacuum tubes for digital logic elements and \\nmemory. A number of research and then commercial computers were built using \\nvacuum tubes. For our purposes, it will be instructive to examine perhaps the most \\nfamous \\xad\\nfirst-\\u200b\\n\\xad\\ngeneration computer, known as the IAS computer.\\nA fundamental design approach first implemented in the IAS computer is \\nknown as the \\xad\\nstored-\\u200b\\n\\xad\\nprogram concept. This idea is usually attributed to the mathem-\\natician John von Neumann. Alan Turing developed the idea at about the same time. \\nThe first publication of the idea was in a 1945 proposal by von Neumann for a new \\ncomputer, the EDVAC (Electronic Discrete Variable Computer).3\\nIn 1946, von Neumann and his colleagues began the design of a new \\xad\\nstored-\\u200b\\n\\xad\\nprogram computer, referred to as the IAS computer, at the Princeton Institute for \\nAdvanced Studies. The IAS computer, although not completed until 1952, is the \\nprototype of all subsequent \\xad\\ngeneral-\\u200b\\n\\xad\\npurpose computers.4\\nFigure\\xa01.6 shows the structure of the IAS computer (compare with Figure\\xa01.1). \\nIt consists of\\n■\\n■A main memory, which stores both data and instructions5\\n■\\n■An arithmetic and logic unit (ALU) capable of operating on binary data\\n2\\u200a\\nThis book’s Companion Web site (WilliamStallings.com/ComputerOrganization) contains several links \\nto sites that provide photographs of many of the devices and components discussed in this section.\\n4A 1954 report [GOLD54] describes the implemented IAS machine and lists the final instruction set. It \\nis available at box.com/COA10e.\\n3The 1945 report on EDVAC is available at box.com/COA10e.\\n5In this book, unless otherwise noted, the term instruction refers to a machine instruction that is directly \\ninterpreted and executed by the processor, in contrast to a statement in a \\xad\\nhigh-\\u200b\\n\\xad\\nlevel language, such as Ada \\nor C++, which must first be compiled into a series of machine instructions before being executed.\\n12\\u2002 \\u2002 Chapter 1 / Basic Concepts and Computer Evolution \\n■\\n■A control unit, which interprets the instructions in memory and causes them \\nto be executed\\n■\\n■\\xad\\nInput–\\u200b\\n\\xad\\noutput (I/O) equipment operated by the control unit\\nThis structure was outlined in von Neumann’s earlier proposal, which is worth \\nquoting in part at this point [VONN45]:\\n2.2 First: Since the device is primarily a computer, it will \\nhave to perform the elementary operations of arithmetic most fre-\\nquently. These are addition, subtraction, multiplication, and divi-\\nsion. It is therefore reasonable that it should contain specialized \\norgans for just these operations.\\nControl\\ncircuits\\nAddresses\\nControl\\nsignals\\nInstructions\\nand data\\nAC: Accumulator register\\nMQ: multiply-quotient register\\nMBR: memory buffer register\\nIBR: instruction buffer register\\nPC: program counter\\nMAR: memory address register\\nIR: insruction register\\nInstructions\\nand data\\nM(0)\\nM(1)\\nM(2)\\nM(3)\\nM(4)\\nM(4095)\\nM(4093)\\nM(4092)\\nMBR\\nArithmetic-logic unit (CA)\\nCentral processing unit (CPU)\\nProgram control unit (CC)\\nInput-\\noutput\\nequipment\\n(I, O)\\nMain\\nmemory\\n(M)\\nAC\\nMQ\\nArithmetic-logic\\ncircuits\\nIBR\\nPC\\nIR\\nMAR\\nFigure\\xa01.6\\u2003 IAS Structure\\nIt must be observed, however, that while this principle as such \\nis probably sound, the specific way in which it is realized requires \\nclose scrutiny. At any rate a central arithmetical part of the device will \\nprobably have to exist, and this constitutes the first specific part:\\xa0CA.\\n2.3\\xa0 Second: The logical control of the device, that is, the \\nproper sequencing of its operations, can be most efficiently car-\\nried out by a central control organ. If the device is to be elastic, \\nthat is, as nearly as possible all purpose, then a distinction must \\nbe made between the specific instructions given for and defining \\na particular problem, and the general control organs that see to it \\nthat these \\xad\\ninstructions—\\u200b\\n\\xad\\nno matter what they \\xad\\nare—\\u200b\\n\\xad\\nare carried out. \\nThe former must be stored in some way; the latter are represented \\nby definite operating parts of the device. By the central control we \\nmean this latter function only, and the organs that perform it form \\nthe second specific part:\\xa0CC.\\n2.4 Third: Any device that is to carry out long and compli-\\ncated sequences of operations (specifically of calculations) must \\nhave a considerable memory . . .\\nThe instructions which govern a complicated problem may \\nconstitute considerable material, particularly so if the code is cir-\\ncumstantial (which it is in most arrangements). This material must \\nbe remembered.\\nAt any rate, the total memory constitutes the third specific \\npart of the device:\\xa0M.\\n2.6 The three specific parts CA, CC (together C), and M cor-\\nrespond to the associative neurons in the human nervous system. It \\nremains to discuss the equivalents of the sensory or afferent and the \\nmotor or efferent neurons. These are the input and output organs of \\nthe device.\\nThe device must be endowed with the ability to maintain \\ninput and output (sensory and motor) contact with some specific \\nmedium of this type. The medium will be called the outside record-\\ning medium of the device:\\xa0R.\\n2.7 Fourth: The device must have organs to transfer informa-\\ntion from R into its specific parts C and\\xa0M.\\xa0These organs form its \\ninput, the fourth specific part:\\xa0I.\\xa0It will be seen that it is best to make \\nall transfers from R (by I) into M and never directly from\\xa0C.\\n2.8 Fifth: The device must have organs to transfer from its \\nspecific parts C and M into\\xa0R.\\xa0These organs form its output, the \\nfifth specific part:\\xa0O.\\xa0It will be seen that it is again best to make all \\ntransfers from M (by O) into R, and never directly from\\xa0C.\\nWith rare exceptions, all of today’s computers have this same general structure \\nand function and are thus referred to as von Neumann machines. Thus, it is worth-\\nwhile at this point to describe briefly the operation of the IAS computer [BURK46, \\nGOLD54]. Following [HAYE98], the terminology and notation of von Neumann \\n1.3 / A Brief History of Computers\\u2002 \\u200213\\n14\\u2002 \\u2002 Chapter 1 / Basic Concepts and Computer Evolution \\nare changed in the following to conform more closely to modern usage; the exam-\\nples accompanying this discussion are based on that latter text.\\nThe memory of the IAS consists of 4,096 storage locations, called words, of \\n40\\xa0binary digits (bits) each.6 Both data and instructions are stored there. Numbers are \\nrepresented in binary form, and each instruction is a binary code. Figure\\xa01.7 illustrates \\nthese formats. Each number is represented by a sign bit and a 39-bit value. A\\xa0word \\nmay alternatively contain two 20-bit instructions, with each instruction consisting \\nof an 8-bit operation code (opcode) specifying the operation to be performed and \\na 12-bit address designating one of the words in memory (numbered from 0 to 999).\\nThe control unit operates the IAS by fetching instructions from memory \\nand executing them one at a time. We explain these operations with reference to \\n\\xad\\nFigure\\xa01.6. This figure reveals that both the control unit and the ALU contain stor-\\nage locations, called registers, defined as follows:\\n■\\n■Memory buffer register (MBR):  Contains a word to be stored in memory or sent \\nto the I/O unit, or is used to receive a word from memory or from the I/O unit.\\n■\\n■Memory address register (MAR): Specifies the address in memory of the word \\nto be written from or read into the\\xa0MBR.\\n■\\n■Instruction register (IR):  Contains the 8-bit opcode instruction being executed.\\n■\\n■Instruction buffer register (IBR): Employed to hold temporarily the \\xad\\nright-\\u200b\\n\\xad\\nhand instruction from a word in memory.\\n■\\n■Program counter (PC): Contains the address of the next instruction pair to be \\nfetched from memory.\\n■\\n■Accumulator (AC) and multiplier quotient (MQ): Employed to hold tem-\\nporarily operands and results of ALU operations. For example, the result \\n6There is no universal definition of the term word. In general, a word is an ordered set of bytes or bits \\nthat is the normal unit in which information may be stored, transmitted, or operated on within a given \\ncomputer. Typically, if a processor has a \\xad\\nfixed-\\u200b\\n\\xad\\nlength instruction set, then the instruction length equals \\nthe word length.\\n(a) Number word\\nsign bit\\n0\\n39\\n(b) Instruction word\\nopcode (8 bits)\\naddress (12 bits)\\nleft instruction (20 bits)\\n0\\n8\\n20\\n28\\n39\\n1\\nright instruction (20 bits)\\nopcode (8 bits)\\naddress (12 bits)\\nFigure\\xa01.7\\u2003 IAS Memory Formats\\nof multiplying two 40-bit numbers is an 80-bit number; the most significant \\n40\\xa0bits are stored in the AC and the least significant in the\\xa0MQ.\\nThe IAS operates by repetitively performing an instruction cycle, as shown in \\nFigure\\xa01.8. Each instruction cycle consists of two subcycles. During the fetch cycle, \\nthe opcode of the next instruction is loaded into the IR and the address portion is \\nloaded into the\\xa0MAR.\\xa0This instruction may be taken from the IBR, or it can be \\nobtained from memory by loading a word into the MBR, and then down to the IBR, \\nIR, and\\xa0MAR.\\nWhy the indirection? These operations are controlled by electronic circuitry \\nand result in the use of data paths. To simplify the electronics, there is only one reg-\\nister that is used to specify the address in memory for a read or write and only one \\nregister used for the source or destination.\\n1.3 / A Brief History of Computers\\u2002 \\u200215\\nStart\\nIs next\\ninstruction\\nin IBR?\\nMAR \\n   \\n    PC\\nMBR \\n   \\n    M(MAR)\\nIR    \\n    IBR (0:7)\\nMAR \\n  \\n     IBR (8:19)\\nIR    \\n     MBR (20:27)\\nMAR    \\n    MBR (28:39)\\nLeft\\ninstruction\\nrequired?\\nIBR \\n    \\n   MBR (20:39)\\nIR     \\n   MBR (0:7)\\nMAR \\n   \\n    MBR (8:19)\\nPC    \\n    PC + 1\\nYes\\nYes\\nYes\\nNo\\nNo\\nNo\\nM(X) = contents of memory location whose address is X\\n(i:j) = bits i through j\\nNo memory\\naccess\\nrequired\\nDecode instruction in IR\\nAC   \\n     M(X)\\nGo to M(X, 0:19)\\nIf AC > 0 then\\ngo to M(X, 0:19)\\nAC \\n   \\n    AC + M(X)\\nIs AC > 0?\\nMBR   \\n    M(MAR)\\nMBR \\n  \\n    M(MAR)\\nPC    \\n    MAR\\nAC    \\n    MBR\\nAC \\n   \\n    AC + MBR\\nFetch\\ncycle\\nExecution\\ncycle\\nFigure\\xa01.8\\u2003 Partial Flowchart of IAS Operation\\n16\\u2002 \\u2002 Chapter 1 / Basic Concepts and Computer Evolution \\nOnce the opcode is in the IR, the execute cycle is performed. Control circuitry \\ninterprets the opcode and executes the instruction by sending out the appropri-\\nate control signals to cause data to be moved or an operation to be performed by \\nthe\\xa0ALU.\\nThe IAS computer had a total of 21 instructions, which are listed in Table\\xa01.1. \\nThese can be grouped as follows:\\n■\\n■Data transfer: Move data between memory and ALU registers or between two \\nALU registers.\\n■\\n■Unconditional branch: Normally, the control unit executes instructions in \\nsequence from memory. This sequence can be changed by a branch instruc-\\ntion, which facilitates repetitive operations.\\nTable\\xa01.1\\u2003 The IAS Instruction Set\\nInstruction \\nType\\nOpcode\\nSymbolic \\nRepresentation\\nDescription\\nData transfer\\n00001010\\nLOAD MQ\\nTransfer contents of register MQ to the accumulator AC\\n00001001\\nLOAD MQ,M(X)\\nTransfer contents of memory location X to MQ\\n00100001\\nSTOR M(X)\\nTransfer contents of accumulator to memory location X\\n00000001\\nLOAD M(X)\\nTransfer M(X) to the accumulator\\n00000010\\nLOAD –M(X)\\nTransfer –M(X) to the accumulator\\n00000011\\nLOAD |M(X)|\\nTransfer absolute value of M(X) to the accumulator\\n00000100\\nLOAD –|M(X)|\\nTransfer –|M(X)| to the accumulator\\nUnconditional  \\nbranch\\n00001101\\nJUMP M(X,0:19)\\nTake next instruction from left half of M(X)\\n00001110\\nJUMP M(X,20:39)\\nTake next instruction from right half of M(X)\\nConditional \\nbranch\\n00001111\\nJUMP + M(X,0:19)\\nIf number in the accumulator is nonnegative, take next \\ninstruction from left half of M(X)\\n00010000\\nJUMP + M(X,20:39)\\nIf number in the accumulator is nonnegative, take next \\ninstruction from right half of M(X)\\nArithmetic\\n00000101\\nADD M(X)\\nAdd M(X) to AC; put the result in AC\\n00000111\\nADD |M(X)|\\nAdd |M(X)| to AC; put the result in AC\\n00000110\\nSUB M(X)\\nSubtract M(X) from AC; put the result in AC\\n00001000\\nSUB |M(X)|\\nSubtract |M(X)| from AC; put the remainder in AC\\n00001011\\nMUL M(X)\\nMultiply M(X) by MQ; put most significant bits of result \\nin AC, put least significant bits in MQ\\n00001100\\nDIV M(X)\\nDivide AC by M(X); put the quotient in MQ and the \\nremainder in AC\\n00010100\\nLSH\\nMultiply accumulator by 2; that is, shift left one bit position\\n00010101\\nRSH\\nDivide accumulator by 2; that is, shift right one position\\nAddress \\nmodify\\n00010010\\nSTOR M(X,8:19)\\nReplace left address field at M(X) by 12 rightmost bits \\nof AC\\n00010011\\nSTOR M(X,28:39)\\nReplace right address field at M(X) by 12 rightmost bits \\nof AC\\n■\\n■Conditional branch: The branch can be made dependent on a condition, thus \\nallowing decision points.\\n■\\n■Arithmetic: Operations performed by the\\xa0ALU.\\n■\\n■Address modify: Permits addresses to be computed in the ALU and then \\ninserted into instructions stored in memory. This allows a program consider-\\nable addressing flexibility.\\nTable\\xa01.1 presents instructions (excluding I/O instructions) in a symbolic, \\n\\xad\\neasy-\\u200b\\n\\xad\\nto-\\u200b\\n\\xad\\nread form. In binary form, each instruction must conform to the format of \\nFigure\\xa01.7b. The opcode portion (first 8 bits) specifies which of the 21 instructions is \\nto be executed. The address portion (remaining 12 bits) specifies which of the 4,096 \\nmemory locations is to be involved in the execution of the instruction.\\nFigure\\xa01.8 shows several examples of instruction execution by the control unit. \\nNote that each operation requires several steps, some of which are quite elaborate. \\nThe multiplication operation requires 39 suboperations, one for each bit position \\nexcept that of the sign bit.\\nThe Second Generation: Transistors\\nThe first major change in the electronic computer came with the replacement of the \\nvacuum tube by the transistor. The transistor, which is smaller, cheaper, and gener-\\nates less heat than a vacuum tube, can be used in the same way as a vacuum tube to \\nconstruct computers. Unlike the vacuum tube, which requires wires, metal plates, a \\nglass capsule, and a vacuum, the transistor is a \\xad\\nsolid-\\u200b\\n\\xad\\nstate device, made from silicon.\\nThe transistor was invented at Bell Labs in 1947 and by the 1950s had launched \\nan electronic revolution. It was not until the late 1950s, however, that fully transis-\\ntorized computers were commercially available. The use of the transistor defines \\nthe second generation of computers. It has become widely accepted to classify com-\\nputers into generations based on the fundamental hardware technology employed \\n(Table\\xa01.2). Each new generation is characterized by greater processing perfor-\\nmance, larger memory capacity, and smaller size than the previous one.\\nBut there are other changes as well. The second generation saw the intro-\\nduction of more complex arithmetic and logic units and control units, the use of \\n\\xad\\nhigh-\\u200b\\n\\xad\\nlevel programming languages, and the provision of system software with the \\n1.3 / A Brief History of Computers\\u2002 \\u200217\\nTable\\xa01.2\\u2003 Computer Generations\\nGeneration\\nApproximate \\nDates\\nTechnology\\nTypical Speed  \\n(operations per second)\\n1\\n1946–1957\\nVacuum tube\\n40,000\\n2\\n1957–1964\\nTransistor\\n200,000\\n3\\n1965–1971\\n\\xad\\nSmall-\\u200b\\n\\xad\\n and \\xad\\nmedium-\\u200b\\n\\xad\\nscale \\nintegration\\n1,000,000\\n4\\n1972–1977\\nLarge scale integration\\n10,000,000\\n5\\n1978–1991\\nVery large scale integration\\n100,000,000\\n6\\n1991–\\nUltra large scale integration\\n>1,000,000,000\\n18\\u2002 \\u2002 Chapter 1 / Basic Concepts and Computer Evolution \\ncomputer. In broad terms, system software provided the ability to load programs, \\nmove data to peripherals, and libraries to perform common computations, similar \\nto what modern operating systems, such as Windows and Linux, do.\\nIt will be useful to examine an important member of the second generation: the \\nIBM 7094 [BELL71]. From the introduction of the 700 series in 1952 to the introduc-\\ntion of the last member of the 7000 series in 1964, this IBM product line underwent \\nan evolution that is typical of computer products. Successive members of the product \\nline showed increased performance, increased capacity, and/or lower cost.\\nThe size of main memory, in multiples of 210 36-bit words, grew from \\n \\n2k (1k = 210) to 32k words,7 while the time to access one word of memory, the mem-\\nory cycle time, fell from 30 ms to 1.4 ms. The number of opcodes grew from a modest \\n24 to 185.\\nAlso, over the lifetime of this series of computers, the relative speed of the \\nCPU increased by a factor of 50. Speed improvements are achieved by improved \\nelectronics (e.g., a transistor implementation is faster than a vacuum tube imple-\\nmentation) and more complex circuitry. For example, the IBM 7094 includes an \\nInstruction Backup Register, used to buffer the next instruction. The control unit \\nfetches two adjacent words from memory for an instruction fetch. Except for the \\noccurrence of a branching instruction, which is relatively infrequent (perhaps 10 to \\n15%), this means that the control unit has to access memory for an instruction on \\nonly half the instruction cycles. This prefetching significantly reduces the average \\ninstruction cycle time.\\nFigure\\xa01.9 shows a large (many peripherals) configuration for an IBM 7094, \\nwhich is representative of \\xad\\nsecond-\\u200b\\n\\xad\\ngeneration computers. Several differences from \\nthe IAS computer are worth noting. The most important of these is the use of data \\nchannels. A data channel is an independent I/O module with its own processor and \\ninstruction set. In a computer system with such devices, the CPU does not execute \\ndetailed I/O instructions. Such instructions are stored in a main memory to be \\nexecuted by a \\xad\\nspecial-\\u200b\\n\\xad\\npurpose processor in the data channel itself. The CPU initi-\\nates an I/O transfer by sending a control signal to the data channel, instructing it to \\nexecute a sequence of instructions in memory. The data channel performs its task \\nindependently of the CPU and signals the CPU when the operation is complete. \\nThis arrangement relieves the CPU of a considerable processing burden.\\nAnother new feature is the multiplexor, which is the central termination \\npoint for data channels, the CPU, and memory. The multiplexor schedules access \\nto the memory from the CPU and data channels, allowing these devices to act \\nindependently.\\nThe Third Generation: Integrated Circuits\\nA single, \\xad\\nself-\\u200b\\n\\xad\\ncontained transistor is called a discrete component. Throughout \\n \\nthe 1950s and early 1960s, electronic equipment was composed largely of discrete \\n\\xad\\ncomponents—\\u200b\\n\\xad\\ntransistors, resistors, capacitors, and so on. Discrete components were \\nmanufactured separately, packaged in their own containers, and soldered or wired \\n7A discussion of the uses of numerical prefixes, such as kilo and giga, is contained in a supporting docu-\\nment at the Computer Science Student Resource Site at ComputerScienceStudent.com.\\ntogether onto \\xad\\nMasonite-\\u200b\\n\\xad\\nlike circuit boards, which were then installed in computers, \\noscilloscopes, and other electronic equipment. Whenever an electronic device called \\nfor a transistor, a little tube of metal containing a \\xad\\npinhead-\\u200b\\n\\xad\\nsized piece of silicon had \\nto be soldered to a circuit board. The entire manufacturing process, from transistor \\nto circuit board, was expensive and cumbersome.\\nThese facts of life were beginning to create problems in the computer indus-\\ntry. Early \\xad\\nsecond-\\u200b\\n\\xad\\ngeneration computers contained about 10,000 transistors. This \\nfigure grew to the hundreds of thousands, making the manufacture of newer, more \\npowerful machines increasingly difficult.\\nIn 1958 came the achievement that revolutionized electronics and started the \\nera of microelectronics: the invention of the integrated circuit. It is the integrated \\ncircuit that defines the third generation of computers. In this section, we provide a \\nbrief introduction to the technology of integrated circuits. Then we look at perhaps \\nthe two most important members of the third generation, both of which were intro-\\nduced at the beginning of that era: the IBM System/360 and the DEC \\xad\\nPDP-\\u200b\\n\\xad\\n8.\\nmicroelectronics Microelectronics means, literally, “small electronics.” Since the \\nbeginnings of digital electronics and the computer industry, there has been a persistent \\nand consistent trend toward the reduction in size of digital electronic circuits. Before \\nexamining the implications and benefits of this trend, we need to say something about \\nthe nature of digital electronics. A more detailed discussion is found in Chapter\\xa011.\\nCPU\\nMemory\\nIBM 7094 computer\\nPeripheral devices\\nData\\nchannel\\nMag tape\\nunits\\nCard\\npunch\\nLine\\nprinter\\nCard\\nreader\\nDrum\\nDisk\\nDisk\\nHyper-\\ntapes\\nTeleprocessing\\nequipment\\nData\\nchannel\\nData\\nchannel\\nData\\nchannel\\nMulti-\\nplexor\\nFigure\\xa01.9\\u2003 An IBM 7094 Configuration\\n1.3 / A Brief History of Computers\\u2002 \\u200219\\n20\\u2002 \\u2002 Chapter 1 / Basic Concepts and Computer Evolution \\nThe basic elements of a digital computer, as we know, must perform data stor-\\nage, movement, processing, and control functions. Only two fundamental types of \\ncomponents are required (Figure\\xa01.10): gates and memory cells. A gate is a device \\nthat implements a simple Boolean or logical function. For example, an AND gate \\nwith inputs A and B and output C implements the expression IF A AND B ARE \\nTRUE THEN C IS\\xa0TRUE.\\xa0Such devices are called gates because they control data \\nflow in much the same way that canal gates control the flow of water. The memory \\ncell is a device that can store 1 bit of data; that is, the device can be in one of two \\nstable states at any time. By interconnecting large numbers of these fundamental \\ndevices, we can construct a computer. We can relate this to our four basic functions \\nas follows:\\n■\\n■Data storage:  Provided by memory cells.\\n■\\n■Data processing:  Provided by gates.\\n■\\n■Data movement:  The paths among components are used to move data from \\nmemory to memory and from memory through gates to memory.\\n■\\n■Control:  The paths among components can carry control signals. For example, \\na gate will have one or two data inputs plus a control signal input that activates \\nthe gate. When the control signal is ON, the gate performs its function on the \\ndata inputs and produces a data output. Conversely, when the control signal \\nis OFF, the output line is null, such as the one produced by a high impedance \\nstate. Similarly, the memory cell will store the bit that is on its input lead when \\nthe WRITE control signal is ON and will place the bit that is in the cell on its \\noutput lead when the READ control signal is\\xa0ON.\\nThus, a computer consists of gates, memory cells, and interconnections among \\nthese elements. The gates and memory cells are, in turn, constructed of simple elec-\\ntronic components, such as transistors and capacitors.\\nThe integrated circuit exploits the fact that such components as transistors, \\nresistors, and conductors can be fabricated from a semiconductor such as silicon. \\nIt is merely an extension of the \\xad\\nsolid-\\u200b\\n\\xad\\nstate art to fabricate an entire circuit in a tiny \\npiece of silicon rather than assemble discrete components made from separate \\npieces of silicon into the same circuit. Many transistors can be produced at the same \\ntime on a single wafer of silicon. Equally important, these transistors can be con-\\nnected with a process of metallization to form circuits.\\nBoolean\\nlogic\\nfunction\\nInput\\nActivate\\nsignal\\n(a) Gate\\nOutput\\n•\\n•\\n•\\nBinary\\nstorage\\ncell\\nInput\\nRead\\nWrite\\n(b) Memory cell\\nOutput\\nFigure\\xa01.10\\u2003 Fundamental Computer Elements\\nFigure\\xa01.11 depicts the key concepts in an integrated circuit. A thin wafer of \\nsilicon is divided into a matrix of small areas, each a few millimeters square. The \\nidentical circuit pattern is fabricated in each area, and the wafer is broken up into \\nchips. Each chip consists of many gates and/or memory cells plus a number of input \\nand output attachment points. This chip is then packaged in housing that protects \\nit and provides pins for attachment to devices beyond the chip. A number of these \\npackages can then be interconnected on a printed circuit board to produce larger \\nand more complex circuits.\\nInitially, only a few gates or memory cells could be reliably manufactured and \\npackaged together. These early integrated circuits are referred to as \\xad\\nsmall-\\u200b\\n\\xad\\nscale \\nintegration (SSI). As time went on, it became possible to pack more and more com-\\nponents on the same chip. This growth in density is illustrated in Figure\\xa01.12; it is \\none of the most remarkable technological trends ever recorded.8 This figure reflects \\nthe famous Moore’s law, which was propounded by Gordon Moore, cofounder of \\nIntel, in 1965 [MOOR65]. Moore observed that the number of transistors that could \\nbe put on a single chip was doubling every year, and correctly predicted that this \\npace would continue into the near future. To the surprise of many, including Moore, \\nthe pace continued year after year and decade after decade. The pace slowed to a \\ndoubling every 18\\xa0months in the 1970s but has sustained that rate ever since.\\nThe consequences of Moore’s law are profound:\\n1.\\t The cost of a chip has remained virtually unchanged during this period of rapid \\ngrowth in density. This means that the cost of computer logic and memory cir-\\ncuitry has fallen at a dramatic rate.\\nWafer\\nChip\\nGate\\nPackaged\\nchip\\nFigure\\xa01.11\\u2003 Relationship among \\nWafer, Chip, and Gate\\n1.3 / A Brief History of Computers\\u2002 \\u200221\\n8Note that the vertical axis uses a log scale. A basic review of log scales is in the math refresher document \\nat the Computer Science Student Resource Site at ComputerScienceStudent.com.\\n22\\u2002 \\u2002 Chapter 1 / Basic Concepts and Computer Evolution \\n2.\\t Because logic and memory elements are placed closer together on more \\ndensely packed chips, the electrical path length is shortened, increasing oper-\\nating speed.\\n3.\\t The computer becomes smaller, making it more convenient to place in a vari-\\nety of environments.\\n4.\\t There is a reduction in power requirements.\\n5.\\t The interconnections on the integrated circuit are much more reliable than \\nsolder connections. With more circuitry on each chip, there are fewer inter-\\nchip connections.\\nibm system/360 By 1964, IBM had a firm grip on the computer market with \\nits 7000 series of machines. In that year, IBM announced the System/360, a new \\nfamily of computer products. Although the announcement itself was no surprise, it \\ncontained some unpleasant news for current IBM customers: the 360 product line \\nwas incompatible with older IBM machines. Thus, the transition to the 360 would \\nbe difficult for the current customer base, but IBM felt this was necessary to break \\nout of some of the constraints of the 7000 architecture and to produce a system \\ncapable of evolving with the new integrated circuit technology [PADE81, GIFF87]. \\nThe strategy paid off both financially and technically. The 360 was the success of \\nthe decade and cemented IBM as the overwhelmingly dominant computer vendor, \\nwith a market share above 70%. And, with some modifications and extensions, the \\narchitecture of the 360 remains to this day the architecture of IBM’s mainframe9 \\ncomputers. Examples using this architecture can be found throughout this text.\\nThe System/360 was the industry’s first planned family of computers. The family \\ncovered a wide range of performance and cost. The models were compatible in the \\n1\\n1947\\nFirst working\\ntransistor\\nMoore’s law\\npromulgated\\nInvention of\\nintegrated circuit\\n50\\n55\\n60\\n65\\n70\\n75\\n80\\n85\\n90\\n95\\n2000\\n05\\n11\\n10\\n100\\n1,000\\n10,000\\n100,000\\n10 m\\n100 m\\n1 bn\\n10 bn\\n100 bn\\nFigure\\xa01.12\\u2003 Growth in Transistor Count on Integrated Circuits\\n9The term mainframe is used for the larger, most powerful computers other than supercomputers. Typical \\ncharacteristics of a mainframe are that it supports a large database, has elaborate I/O hardware, and is \\nused in a central data processing facility.\\nsense that a program written for one model should be capable of being executed by \\nanother model in the series, with only a difference in the time it takes to execute.\\nThe concept of a family of compatible computers was both novel and extremely \\nsuccessful. A customer with modest requirements and a budget to match could start \\nwith the relatively inexpensive Model 30. Later, if the customer’s needs grew, it was \\npossible to upgrade to a faster machine with more memory without sacrificing the \\ninvestment in \\xad\\nalready-\\u200b\\n\\xad\\ndeveloped software. The characteristics of a family are as follows:\\n■\\n■Similar or identical instruction set: In many cases, the exact same set of \\nmachine instructions is supported on all members of the family. Thus, a pro-\\ngram that executes on one machine will also execute on any other. In some \\ncases, the lower end of the family has an instruction set that is a subset of \\nthat of the top end of the family. This means that programs can move up but \\nnot down.\\n■\\n■Similar or identical operating system: The same basic operating system is \\navailable for all family members. In some cases, additional features are added \\nto the \\xad\\nhigher-\\u200b\\n\\xad\\nend members.\\n■\\n■Increasing speed: The rate of instruction execution increases in going from \\nlower to higher family members.\\n■\\n■Increasing number of I/O ports: The number of I/O ports increases in going \\nfrom lower to higher family members.\\n■\\n■Increasing memory size: The size of main memory increases in going from \\nlower to higher family members.\\n■\\n■Increasing cost: At a given point in time, the cost of a system increases in going \\nfrom lower to higher family members.\\nHow could such a family concept be implemented? Differences were achieved \\nbased on three factors: basic speed, size, and degree of simultaneity [STEV64]. For \\nexample, greater speed in the execution of a given instruction could be gained by \\nthe use of more complex circuitry in the ALU, allowing suboperations to be car-\\nried out in parallel. Another way of increasing speed was to increase the width of \\nthe data path between main memory and the\\xa0CPU.\\xa0On the Model 30, only 1 byte \\n(8 bits) could be fetched from main memory at a time, whereas 8 bytes could be \\nfetched at a time on the Model 75.\\nThe System/360 not only dictated the future course of IBM but also had a pro-\\nfound impact on the entire industry. Many of its features have become standard on \\nother large computers.\\ndec \\xad\\npdp-\\u200b\\n\\xad\\n8 In the same year that IBM shipped its first System/360, another \\nmomentous first shipment occurred: \\xad\\nPDP-\\u200b\\n\\xad\\n8 from Digital Equipment Corporation \\n(DEC). At a time when the average computer required an \\xad\\nair-\\u200b\\n\\xad\\nconditioned room, \\nthe \\xad\\nPDP-\\u200b\\n\\xad\\n8 (dubbed a minicomputer by the industry, after the miniskirt of the day) \\nwas small enough that it could be placed on top of a lab bench or be built into \\nother equipment. It could not do everything the mainframe could, but at $16,000, it \\nwas cheap enough for each lab technician to have one. In contrast, the System/360 \\nseries of mainframe computers introduced just a few months before cost hundreds \\nof thousands of dollars.\\n1.3 / A Brief History of Computers\\u2002 \\u200223\\n24\\u2002 \\u2002 Chapter 1 / Basic Concepts and Computer Evolution \\nThe low cost and small size of the \\xad\\nPDP-\\u200b\\n\\xad\\n8 enabled another manufacturer to \\npurchase a \\xad\\nPDP-\\u200b\\n\\xad\\n8 and integrate it into a total system for resale. These other manu-\\nfacturers came to be known as original equipment manufacturers (OEMs), and the \\nOEM market became and remains a major segment of the computer marketplace.\\nIn contrast to the \\xad\\ncentral-\\u200b\\n\\xad\\nswitched architecture (Figure\\xa01.9) used by IBM on its \\n700/7000 and 360 systems, later models of the \\xad\\nPDP-\\u200b\\n\\xad\\n8 used a structure that became vir-\\ntually universal for microcomputers: the bus structure. This is illustrated in Figure\\xa01.13. \\nThe \\xad\\nPDP-\\u200b\\n\\xad\\n8 bus, called the Omnibus, consists of 96 separate signal paths, used to carry \\ncontrol, address, and data signals. Because all system components share a common \\nset of signal paths, their use can be controlled by the\\xa0CPU.\\xa0This architecture is highly \\nflexible, allowing modules to be plugged into the bus to create various configurations. \\nIt is only in recent years that the bus structure has given way to a structure known as \\n\\xad\\npoint-\\u200b\\n\\xad\\nto-\\u200b\\n\\xad\\npoint interconnect, described in Chapter\\xa03.\\nLater Generations\\nBeyond the third generation there is less general agreement on defining generations \\nof computers. Table\\xa01.2 suggests that there have been a number of later generations, \\nbased on advances in integrated circuit technology. With the introduction of \\xad\\nlarge-\\u200b\\n\\xad\\nscale integration (LSI), more than 1,000 components can be placed on a single inte-\\ngrated circuit chip. \\xad\\nVery-\\u200b\\n\\xad\\nlarge-\\u200b\\n\\xad\\nscale integration (VLSI) achieved more than 10,000 \\ncomponents per chip, while current \\xad\\nultra-\\u200b\\n\\xad\\nlarge-\\u200b\\n\\xad\\nscale integration (ULSI) chips can \\ncontain more than one billion components.\\nWith the rapid pace of technology, the high rate of introduction of new prod-\\nucts, and the importance of software and communications as well as hardware, the \\nclassification by generation becomes less clear and less meaningful. In this section, \\nwe mention two of the most important of developments in later generations.\\nsemiconductor memory The first application of integrated circuit technology \\nto computers was the construction of the processor (the control unit and the \\narithmetic and logic unit) out of integrated circuit chips. But it was also found that \\nthis same technology could be used to construct memories.\\nIn the 1950s and 1960s, most computer memory was constructed from tiny \\nrings of ferromagnetic material, each about a sixteenth of an inch in diameter. \\nThese rings were strung up on grids of fine wires suspended on small screens inside \\nthe computer. Magnetized one way, a ring (called a core) represented a one; mag-\\nnetized the other way, it stood for a zero. \\xad\\nMagnetic-\\u200b\\n\\xad\\ncore memory was rather fast; \\nit took as little as a millionth of a second to read a bit stored in memory. But it was \\nConsole\\ncontroller\\nCPU\\nOmnibus\\nMain\\nmemory\\nI/O\\nmodule\\nI/O\\nmodule\\n• • •\\nFigure\\xa01.13\\u2003 \\xad\\nPDP-\\u200b\\n\\xad\\n8 Bus Structure\\nexpensive and bulky, and used destructive readout: The simple act of reading a core \\nerased the data stored in it. It was therefore necessary to install circuits to restore \\nthe data as soon as it had been extracted.\\nThen, in 1970, Fairchild produced the first relatively capacious semiconductor \\nmemory. This chip, about the size of a single core, could hold 256 bits of memory. It \\nwas nondestructive and much faster than core. It took only 70\\xa0billionths of a second \\nto read a bit. However, the cost per bit was higher than for that of core.\\nIn 1974, a seminal event occurred: The price per bit of semiconductor memory \\ndropped below the price per bit of core memory. Following this, there has been a con-\\ntinuing and rapid decline in memory cost accompanied by a corresponding increase in \\nphysical memory density. This has led the way to smaller, faster machines with mem-\\nory sizes of larger and more expensive machines from just a few years earlier. Devel-\\nopments in memory technology, together with developments in processor technology \\nto be discussed next, changed the nature of computers in less than a decade. Although \\nbulky, expensive computers remain a part of the landscape, the computer has also \\nbeen brought out to the “end user,” with office machines and personal computers.\\nSince 1970, semiconductor memory has been through 13 generations: 1k, 4k, \\n16k, 64k, 256k, 1M, 4M, 16M, 64M, 256M, 1G, 4G, and, as of this writing, 8 Gb \\non a single chip (1 k = 210, 1 M = 220, 1 G = 230). Each generation has provided \\nincreased storage density, accompanied by declining cost per bit and declining \\naccess time. Densities are projected to reach 16 Gb by 2018 and 32 Gb by 2023 \\n[ITRS14].\\nmicroprocessors Just as the density of elements on memory chips has continued \\nto rise, so has the density of elements on processor chips. As time went on, more \\nand more elements were placed on each chip, so that fewer and fewer chips were \\nneeded to construct a single computer processor.\\nA breakthrough was achieved in 1971, when Intel developed its 4004. The \\n4004 was the first chip to contain all of the components of a CPU on a single chip: \\nThe microprocessor was born.\\nThe 4004 can add two 4-bit numbers and can multiply only by repeated addi-\\ntion. By today’s standards, the 4004 is hopelessly primitive, but it marked the begin-\\nning of a continuing evolution of microprocessor capability and power.\\nThis evolution can be seen most easily in the number of bits that the processor \\ndeals with at a time. There is no \\xad\\nclear-\\u200b\\n\\xad\\ncut measure of this, but perhaps the best meas-\\nure is the data bus width: the number of bits of data that can be brought into or sent \\nout of the processor at a time. Another measure is the number of bits in the accumu-\\nlator or in the set of \\xad\\ngeneral-\\u200b\\n\\xad\\npurpose registers. Often, these measures coincide, but \\nnot always. For example, a number of microprocessors were developed that operate \\non 16-bit numbers in registers but can only read and write 8 bits at a time.\\nThe next major step in the evolution of the microprocessor was the introduc-\\ntion in 1972 of the Intel 8008. This was the first 8-bit microprocessor and was almost \\ntwice as complex as the 4004.\\nNeither of these steps was to have the impact of the next major event: the \\nintroduction in 1974 of the Intel 8080. This was the first \\xad\\ngeneral-\\u200b\\n\\xad\\npurpose micropro-\\ncessor. Whereas the 4004 and the 8008 had been designed for specific applications, \\nthe 8080 was designed to be the CPU of a \\xad\\ngeneral-\\u200b\\n\\xad\\npurpose microcomputer. Like the \\n1.3 / A Brief History of Computers\\u2002 \\u200225\\n26\\u2002 \\u2002 Chapter 1 / Basic Concepts and Computer Evolution \\nTable\\xa01.3\\u2003 Evolution of Intel Microprocessors (page 1 of 2)\\n(a) 1970s Processors\\n4004\\n8008\\n8080\\n8086\\n8088\\nIntroduced\\n1971\\n1972\\n1974\\n1978\\n1979\\nClock speeds\\n108 kHz\\n108 kHz\\n2 MHz\\n5 MHz, 8 MHz, 10 MHz\\n5 MHz, 8 MHz\\nBus width\\n4 bits\\n8 bits\\n8 bits\\n16 bits\\n8 bits\\nNumber of transistors\\n2,300\\n3,500\\n6,000\\n29,000\\n29,000\\nFeature size (mm)\\n10\\n8\\n6\\n3\\n6\\nAddressable memory\\n640 bytes\\n16 KB\\n64 KB\\n1 MB\\n1 MB\\n(b) 1980s Processors\\n80286\\n386TM DX\\n386TM SX\\n486TM DX CPU\\nIntroduced\\n1982\\n1985\\n1988\\n1989\\nClock speeds\\n6–12.5 MHz\\n16–33 MHz\\n16–33 MHz\\n25–50 MHz\\nBus width\\n16 bits\\n32 bits\\n16 bits\\n32 bits\\nNumber of transistors\\n134,000\\n275,000\\n275,000\\n1.2\\xa0million\\nFeature size (\\u2009µm)\\n1.5\\n1\\n1\\n0.8–1\\nAddressable memory\\n16 MB\\n4 GB\\n16 MB\\n4 GB\\nVirtual memory\\n1 GB\\n64 TB\\n64 TB\\n64 TB\\nCache\\n—\\n—\\n—\\n8 kB\\n(c) 1990s Processors\\n486TM SX\\nPentium\\nPentium Pro\\nPentium II\\nIntroduced\\n1991\\n1993\\n1995\\n1997\\nClock speeds\\n16–33 MHz\\n60–166 MHz,\\n150–200 MHz\\n200–300 MHz\\nBus width\\n32 bits\\n32 bits\\n64 bits\\n64 bits\\nNumber of transistors\\n1.185\\xa0million\\n3.1\\xa0million\\n5.5\\xa0million\\n7.5\\xa0million\\nFeature size (\\u2009µm)\\n1\\n0.8\\n0.6\\n0.35\\nAddressable memory\\n4 GB\\n4 GB\\n64 GB\\n64 GB\\nVirtual memory\\n64 TB\\n64 TB\\n64 TB\\n64 TB\\nCache\\n8 kB\\n8 kB\\n512 kB L1 and  \\n1 MB L2\\n512 kB L2\\n8008, the 8080 is an 8-bit microprocessor. The 8080, however, is faster, has a richer \\ninstruction set, and has a large addressing capability.\\nAbout the same time, 16-bit microprocessors began to be developed. How-\\never, it was not until the end of the 1970s that powerful, \\xad\\ngeneral-\\u200b\\n\\xad\\npurpose 16-bit \\nmicroprocessors appeared. One of these was the 8086. The next step in this trend \\noccurred in 1981, when both Bell Labs and \\xad\\nHewlett-\\u200b\\n\\xad\\nPackard developed 32-bit, \\n\\xad\\nsingle-\\u200b\\n\\xad\\nchip microprocessors. Intel introduced its own 32-bit microprocessor, the \\n80386, in 1985 (Table\\xa01.3).\\n1.4 / The Evolution of the Intel x86 Architecture\\u2002 \\u200227\\n(d) Recent Processors\\nPentium III\\nPentium 4\\nCore 2 Duo\\nCore i7 EE 4960X\\nIntroduced\\n1999\\n2000\\n2006\\n2013\\nClock speeds\\n450–660 MHz\\n1.3–1.8 GHz\\n1.06–1.2 GHz\\n4 GHz\\nBus width\\n64 bits\\n64 bits\\n64 bits\\n64 bits\\nNumber of transistors\\n9.5\\xa0million\\n42\\xa0million\\n167\\xa0million\\n1.86\\xa0billion\\nFeature size (nm)\\n250\\n180\\n65\\n22\\nAddressable memory\\n64 GB\\n64 GB\\n64 GB\\n64 GB\\nVirtual memory\\n64 TB\\n64 TB\\n64 TB\\n64 TB\\nCache\\n512 kB L2\\n256 kB L2\\n2 MB L2\\n1.5 MB L2/15 MB L3\\nNumber of cores\\n1\\n1\\n2\\n6\\n\\t 1.4\\t The Evolution of the Intel x86 Architecture\\nThroughout this book, we rely on many concrete examples of computer design and \\nimplementation to illustrate concepts and to illuminate \\xad\\ntrade-\\u200b\\n\\xad\\noffs. Numerous sys-\\ntems, both contemporary and historical, provide examples of important computer \\narchitecture design features. But the book relies principally on examples from two \\nprocessor families: the Intel x86 and the ARM architectures. The current x86 offer-\\nings represent the results of decades of design effort on complex instruction set com-\\nputers (CISCs). The x86 incorporates the sophisticated design principles once found \\nonly on mainframes and supercomputers and serves as an excellent example of CISC \\ndesign. An alternative approach to processor design is the reduced instruction set \\ncomputer (RISC). The ARM architecture is used in a wide variety of embedded sys-\\ntems and is one of the most powerful and \\xad\\nbest-\\u200b\\n\\xad\\ndesigned \\xad\\nRISC-\\u200b\\n\\xad\\nbased systems on the \\nmarket. In this section and the next, we provide a brief overview of these two systems.\\nIn terms of market share, Intel has ranked as the number one maker of micro-\\nprocessors for \\xad\\nnon-\\u200b\\n\\xad\\nembedded systems for decades, a position it seems unlikely to \\nyield. The evolution of its flagship microprocessor product serves as a good indica-\\ntor of the evolution of computer technology in general.\\nTable\\xa01.3 shows that evolution. Interestingly, as microprocessors have grown \\nfaster and much more complex, Intel has actually picked up the pace. Intel used \\nto develop microprocessors one after another, every four years. But Intel hopes \\nto keep rivals at bay by trimming a year or two off this development time, and has \\ndone so with the most recent x86 generations.10\\n10Intel refers to this as the \\xad\\ntick-\\u200b\\n\\xad\\ntock model. Using this model, Intel has successfully delivered \\xad\\nnext-\\u200b\\n\\xad\\ngeneration silicon technology as well as new processor microarchitecture on alternating years for the \\npast several years. See http://www.intel.com/content/www/us/en/\\xad\\nsilicon-\\xad\\ninnovations/intel-tick-tock- \\nmodel-general.html.\\n28\\u2002 \\u2002 Chapter 1 / Basic Concepts and Computer Evolution \\nIt is worthwhile to list some of the highlights of the evolution of the Intel prod-\\nuct line:\\n■\\n■8080: The world’s first \\xad\\ngeneral-\\u200b\\n\\xad\\npurpose microprocessor. This was an 8-bit \\nmachine, with an 8-bit data path to memory. The 8080 was used in the first \\npersonal computer, the Altair.\\n■\\n■8086: A far more powerful, 16-bit machine. In addition to a wider data path \\nand larger registers, the 8086 sported an instruction cache, or queue, that \\nprefetches a few instructions before they are executed. A variant of this pro-\\ncessor, the 8088, was used in IBM’s first personal computer, securing the suc-\\ncess of Intel. The 8086 is the first appearance of the x86 architecture.\\n■\\n■80286: This extension of the 8086 enabled addressing a 16-MB memory instead \\nof just 1\\xa0MB.\\n■\\n■80386: Intel’s first 32-bit machine, and a major overhaul of the product. With \\na 32-bit architecture, the 80386 rivaled the complexity and power of minicom-\\nputers and mainframes introduced just a few years earlier. This was the first \\nIntel processor to support multitasking, meaning it could run multiple pro-\\ngrams at the same time.\\n■\\n■80486: The 80486 introduced the use of much more sophisticated and power-\\nful cache technology and sophisticated instruction pipelining. The 80486 also \\noffered a \\xad\\nbuilt-\\u200b\\n\\xad\\nin math coprocessor, offloading complex math operations from \\nthe main\\xa0CPU.\\n■\\n■Pentium: With the Pentium, Intel introduced the use of superscalar tech-\\nniques, which allow multiple instructions to execute in parallel.\\n■\\n■Pentium Pro: The Pentium Pro continued the move into superscalar organiza-\\ntion begun with the Pentium, with aggressive use of register renaming, branch \\nprediction, data flow analysis, and speculative execution.\\n■\\n■Pentium II: The Pentium II incorporated Intel MMX technology, which is \\ndesigned specifically to process video, audio, and graphics data efficiently.\\n■\\n■Pentium III: The Pentium III incorporates additional \\xad\\nfloating-\\u200b\\n\\xad\\npoint instruc-\\ntions: The Streaming SIMD Extensions (SSE) instruction set extension added \\n70 new instructions designed to increase performance when exactly the same \\noperations are to be performed on multiple data objects. Typical applications \\nare digital signal processing and graphics processing.\\n■\\n■Pentium 4: The Pentium 4 includes additional \\xad\\nfloating-\\u200b\\n\\xad\\npoint and other \\nenhancements for multimedia.11\\n■\\n■Core: This is the first Intel x86 microprocessor with a dual core, referring to \\nthe implementation of two cores on a single chip.\\n■\\n■Core 2: The Core 2 extends the Core architecture to 64 bits. The Core 2 Quad \\nprovides four cores on a single chip. More recent Core offerings have up to 10 \\ncores per chip. An important addition to the architecture was the Advanced \\nVector Extensions instruction set that provided a set of 256-bit, and then 512-\\nbit, instructions for efficient processing of vector data.\\n11With the Pentium 4, Intel switched from Roman numerals to Arabic numerals for model numbers.\\n1.5 / Embedded Systems\\u2002 \\u200229\\nAlmost 40 years after its introduction in 1978, the x86 architecture continues to \\ndominate the processor market outside of embedded systems. Although the organiza-\\ntion and technology of the x86 machines have changed dramatically over the decades, \\nthe instruction set architecture has evolved to remain backward compatible with ear-\\nlier versions. Thus, any program written on an older version of the x86 architecture \\ncan execute on newer versions. All changes to the instruction set architecture have \\ninvolved additions to the instruction set, with no subtractions. The rate of change has \\nbeen the addition of roughly one instruction per month added to the architecture \\n[ANTH08], so that there are now thousands of instructions in the instruction set.\\nThe x86 provides an excellent illustration of the advances in computer hard-\\nware over the past 35 years. The 1978 8086 was introduced with a clock speed of \\n5\\xa0MHz and had 29,000 transistors. A \\xad\\nsix-\\u200b\\n\\xad\\ncore Core i7 EE 4960X introduced in 2013 \\noperates at 4 GHz, a speedup of a factor of 800, and has 1.86\\xa0billion transistors, \\nabout 64,000 times as many as the 8086. Yet the Core i7 EE 4960X is in only a \\nslightly larger package than the 8086 and has a comparable cost.\\n\\t 1.5\\t Embedded Systems\\nThe term embedded system refers to the use of electronics and software within a \\nproduct, as opposed to a \\xad\\ngeneral-\\u200b\\n\\xad\\npurpose computer, such as a laptop or desktop sys-\\ntem. Millions of computers are sold every year, including laptops, personal comput-\\ners, workstations, servers, mainframes, and supercomputers. In contrast, billions of \\ncomputer systems are produced each year that are embedded within larger devices. \\nToday, many, perhaps most, devices that use electric power have an embedded com-\\nputing system. It is likely that in the near future virtually all such devices will have \\nembedded computing systems.\\nTypes of devices with embedded systems are almost too numerous to list. \\nExamples include cell phones, digital cameras, video cameras, calculators, micro-\\nwave ovens, home security systems, washing machines, lighting systems, ther-\\nmostats, printers, various automotive systems (e.g., transmission control, cruise \\ncontrol, fuel injection, \\xad\\nanti-\\u200b\\n\\xad\\nlock brakes, and suspension systems), tennis rack-\\nets, toothbrushes, and numerous types of sensors and actuators in automated \\nsystems.\\nOften, embedded systems are tightly coupled to their environment. This can \\ngive rise to \\xad\\nreal-\\u200b\\n\\xad\\ntime constraints imposed by the need to interact with the environ-\\nment. Constraints, such as required speeds of motion, required precision of meas-\\nurement, and required time durations, dictate the timing of software operations. If \\nmultiple activities must be managed simultaneously, this imposes more complex \\n\\xad\\nreal-\\u200b\\n\\xad\\ntime constraints.\\nFigure\\xa01.14 shows in general terms an embedded system organization. In addi-\\ntion to the processor and memory, there are a number of elements that differ from \\nthe typical desktop or laptop computer:\\n■\\n■There may be a variety of interfaces that enable the system to measure, manip-\\nulate, and otherwise interact with the external environment. Embedded sys-\\ntems often interact (sense, manipulate, and communicate) with external world \\nthrough sensors and actuators and hence are typically reactive systems; a \\n30\\u2002 \\u2002 Chapter 1 / Basic Concepts and Computer Evolution \\nreactive system is in continual interaction with the environment and executes \\nat a pace determined by that environment.\\n■\\n■The human interface may be as simple as a flashing light or as complicated as \\n\\xad\\nreal-\\u200b\\n\\xad\\ntime robotic vision. In many cases, there is no human interface.\\n■\\n■The diagnostic port may be used for diagnosing the system that is being \\n\\xad\\ncontrolled—\\u200b\\n\\xad\\nnot just for diagnosing the computer.\\n■\\n■\\xad\\nSpecial-\\u200b\\n\\xad\\npurpose field programmable (FPGA), \\xad\\napplication-\\u200b\\n\\xad\\nspecific (ASIC), or \\neven nondigital hardware may be used to increase performance or reliability.\\n■\\n■Software often has a fixed function and is specific to the application.\\n■\\n■Efficiency is of paramount importance for embedded systems. They are opti-\\nmized for energy, code size, execution time, weight and dimensions, and cost.\\nThere are several noteworthy areas of similarity to \\xad\\ngeneral-\\u200b\\n\\xad\\npurpose computer \\nsystems as well:\\n■\\n■Even with nominally fixed function software, the ability to field upgrade to fix \\nbugs, to improve security, and to add functionality, has become very important \\nfor embedded systems, and not just in consumer devices.\\n■\\n■One comparatively recent development has been of embedded system plat-\\nforms that support a wide variety of apps. Good examples of this are smart-\\nphones and audio/visual devices, such as smart TVs.\\nThe Internet of Things\\nIt is worthwhile to separately callout  one of the major drivers in the \\xad\\nproliferation of \\nembedded systems. The Internet of things (IoT) is a term that refers to the expanding \\nMemory\\nCustom\\nlogic\\nHuman\\ninterface\\nDiagnostic\\nport\\nProcessor\\nD/A\\nConversion\\nActuators/\\nindicators\\nA/D\\nconversion\\nSensors\\nFigure\\xa01.14\\u2003 Possible Organization of an Embedded \\nSystem\\n1.5 / Embedded Systems\\u2002 \\u200231\\ninterconnection of smart devices, ranging from appliances to tiny sensors. A domi-\\nnant theme is the embedding of \\xad\\nshort-\\u200b\\n\\xad\\nrange mobile transceivers into a wide array of \\ngadgets and everyday items, enabling new forms of communication between people \\nand things, and between things themselves. The Internet now supports the intercon-\\nnection of billions of industrial and personal objects, usually through cloud systems. \\nThe objects deliver sensor information, act on their environment, and, in some cases, \\nmodify themselves, to create overall management of a larger system, like a factory \\nor city.\\nThe IoT is primarily driven by deeply embedded devices (defined below). \\nThese devices are \\xad\\nlow-\\u200b\\n\\xad\\nbandwidth, \\xad\\nlow-\\u200b\\n\\xad\\nrepetition \\xad\\ndata-\\u200b\\n\\xad\\ncapture, and \\xad\\nlow-\\u200b\\n\\xad\\nbandwidth \\n\\xad\\ndata-\\u200b\\n\\xad\\nusage appliances that communicate with each other and provide data via user \\ninterfaces. Embedded appliances, such as \\xad\\nhigh-\\u200b\\n\\xad\\nresolution video security cameras, \\nvideo VoIP phones, and a handful of others, require \\xad\\nhigh-\\u200b\\n\\xad\\nbandwidth streaming \\ncapabilities. Yet countless products simply require packets of data to be intermit-\\ntently delivered.\\nWith reference to the end systems supported, the Internet has gone through \\nroughly four generations of deployment culminating in the IoT:\\n1.\\t Information technology (IT): PCs, servers, routers, firewalls, and so on, bought \\nas IT devices by enterprise IT people and primarily using wired connectivity.\\n2.\\t Operational technology (OT): Machines/appliances with embedded IT built \\nby \\xad\\nnon-\\u200b\\n\\xad\\nIT companies, such as medical machinery, SCADA (supervisory con-\\ntrol and data acquisition), process control, and kiosks, bought as appliances by \\nenterprise OT people and primarily using wired connectivity.\\n3.\\t Personal technology: Smartphones, tablets, and eBook readers bought as IT \\ndevices by consumers (employees) exclusively using wireless connectivity and \\noften multiple forms of wireless connectivity.\\n4.\\t Sensor/actuator technology: \\xad\\nSingle-\\u200b\\n\\xad\\npurpose devices bought by consumers, IT, \\nand OT people exclusively using wireless connectivity, generally of a single \\nform, as part of larger systems.\\nIt is the fourth generation that is usually thought of as the IoT, and it is marked \\nby the use of billions of embedded devices.\\nEmbedded Operating Systems\\nThere are two general approaches to developing an embedded operating system \\n(OS). The first approach is to take an existing OS and adapt it for the embedded \\napplication. For example, there are embedded versions of Linux, Windows, and \\nMac, as well as other commercial and proprietary operating systems specialized for \\nembedded systems. The other approach is to design and implement an OS intended \\nsolely for embedded use. An example of the latter is TinyOS, widely used in wireless \\nsensor networks. This topic is explored in depth in [STAL15].\\nApplication Processors versus Dedicated Processors\\nIn this subsection, and the next two, we briefly introduce some terms commonly \\nfound in the literature on embedded systems. Application processors are defined \\n32\\u2002 \\u2002 Chapter 1 / Basic Concepts and Computer Evolution \\nby the processor’s ability to execute complex operating systems, such as Linux, \\nAndroid, and Chrome. Thus, the application processor is \\xad\\ngeneral-\\u200b\\n\\xad\\npurpose in nature. \\nA good example of the use of an embedded application processor is the smartphone. \\nThe embedded system is designed to support numerous apps and perform a wide \\nvariety of functions.\\nMost embedded systems employ a dedicated processor, which, as the name \\nimplies, is dedicated to one or a small number of specific tasks required by the host \\ndevice. Because such an embedded system is dedicated to a specific task or tasks, \\nthe processor and associated components can be engineered to reduce size and cost.\\nMicroprocessors versus Microcontrollers\\nAs we have seen, early microprocessor chips included registers, an ALU, and some \\nsort of control unit or instruction processing logic. As transistor density increased, it \\nbecame possible to increase the complexity of the instruction set architecture, and \\nultimately to add memory and more than one processor. Contemporary micropro-\\ncessor chips, as shown in Figure\\xa01.2, include multiple cores and a substantial amount \\nof cache memory.\\nA microcontroller chip makes a substantially different use of the logic space \\navailable. Figure\\xa01.15 shows in general terms the elements typically found on a \\nmicrocontroller chip. As shown, a microcontroller is a single chip that contains the \\nprocessor, \\xad\\nnon-\\u200b\\n\\xad\\nvolatile memory for the program (ROM), volatile memory for input \\nand output (RAM), a clock, and an I/O control unit. The processor portion of the \\nmicrocontroller has a much lower silicon area than other microprocessors and much \\nhigher energy efficiency. We examine microcontroller organization in more detail \\nin Section\\xa01.6.\\nAlso called a “computer on a chip,” billions of microcontroller units are \\nembedded each year in myriad products from toys to appliances to automobiles. For \\nexample, a single vehicle can use 70 or more microcontrollers. Typically, especially \\nfor the smaller, less expensive microcontrollers, they are used as dedicated proces-\\nsors for specific tasks. For example, microcontrollers are heavily utilized in automa-\\ntion processes. By providing simple reactions to input, they can control machinery, \\nturn fans on and off, open and close valves, and so forth. They are integral parts of \\nmodern industrial technology and are among the most inexpensive ways to produce \\nmachinery that can handle extremely complex functionalities.\\nMicrocontrollers come in a range of physical sizes and processing power. Pro-\\ncessors range from 4-bit to 32-bit architectures. Microcontrollers tend to be much \\nslower than microprocessors, typically operating in the MHz range rather than the \\nGHz speeds of microprocessors. Another typical feature of a microcontroller is that \\nit does not provide for human interaction. The microcontroller is programmed for a \\nspecific task, embedded in its device, and executes as and when required.\\nEmbedded versus Deeply Embedded Systems\\nWe have, in this section, defined the concept of an embedded system. A subset of \\nembedded systems, and a quite numerous subset, is referred to as deeply embed-\\nded systems. Although this term is widely used in the technical and commercial \\n1.6 / ARM Architecture\\u2002 \\u200233\\nliterature, you will search the Internet in vain (or at least I did) for a straightfor-\\nward definition. Generally, we can say that a deeply embedded system has a proces-\\nsor whose behavior is difficult to observe both by the programmer and the user. \\n \\nA deeply embedded system uses a microcontroller rather than a microprocessor, is \\nnot programmable once the program logic for the device has been burned into ROM \\n(\\xad\\nread-\\u200b\\n\\xad\\nonly memory), and has no interaction with a user.\\nDeeply embedded systems are dedicated, \\xad\\nsingle-\\u200b\\n\\xad\\npurpose devices that detect \\nsomething in the environment, perform a basic level of processing, and then do some-\\nthing with the results. Deeply embedded systems often have wireless capability and \\nappear in networked configurations, such as networks of sensors deployed over a large \\narea (e.g., factory, agricultural field). The Internet of things depends heavily on deeply \\nembedded systems. Typically, deeply embedded systems have extreme resource con-\\nstraints in terms of memory, processor size, time, and power consumption.\\n\\t 1.6\\t ARM Architecture\\nThe ARM architecture refers to a processor architecture that has evolved from \\nRISC design principles and is used in embedded systems. Chapter\\xa0 15 examines \\nRISC design principles in detail. In this section, we give a brief overview of the \\nARM architecture.\\nA/D\\nconverter\\nAnalog data\\nacquisition\\nTemporary\\ndata\\nProcessor\\nSystem\\nbus\\nRAM\\nD/A\\nconverter\\nROM\\nSerial I/O\\nports\\nEEPROM\\nParallel I/O\\nports\\nTIMER\\nProgram\\nand data\\nPermanent\\ndata\\nTiming\\nfunctions\\nAnalog data\\ntransmission\\nSend/receive\\ndata\\nPeripheral\\ninterfaces\\nFigure\\xa01.15\\u2003 Typical Microcontroller Chip Elements\\n34\\u2002 \\u2002 Chapter 1 / Basic Concepts and Computer Evolution \\nARM Evolution\\nARM is a family of \\xad\\nRISC-\\u200b\\n\\xad\\nbased microprocessors and microcontrollers designed by \\nARM Holdings, Cambridge, England. The company doesn’t make processors but \\ninstead designs microprocessor and multicore architectures and licenses them to man-\\nufacturers. Specifically, ARM Holdings has two types of licensable products: proces-\\nsors and processor architectures. For processors, the customer buys the rights to use \\n\\xad\\nARM-\\u200b\\n\\xad\\nsupplied design in their own chips. For a processor architecture, the customer \\nbuys the rights to design their own processor compliant with ARM’s architecture.\\nARM chips are \\xad\\nhigh-\\u200b\\n\\xad\\nspeed processors that are known for their small die size \\nand low power requirements. They are widely used in smartphones and other hand-\\nheld devices, including game systems, as well as a large variety of consumer prod-\\nucts. ARM chips are the processors in Apple’s popular iPod and iPhone devices, \\nand are used in virtually all Android smartphones as well. ARM is probably the \\nmost widely used embedded processor architecture and indeed the most widely \\nused processor architecture of any kind in the world [VANC14].\\nThe origins of ARM technology can be traced back to the \\xad\\nBritish-\\u200b\\n\\xad\\nbased Acorn \\nComputers company. In the early 1980s, Acorn was awarded a contract by the Brit-\\nish Broadcasting Corporation (BBC) to develop a new microcomputer architecture \\nfor the BBC Computer Literacy Project. The success of this contract enabled Acorn \\nto go on to develop the first commercial RISC processor, the Acorn RISC Machine \\n(ARM). The first version, ARM1, became operational in 1985 and was used for \\ninternal research and development as well as being used as a coprocessor in the \\nBBC machine.\\nIn this early stage, Acorn used the company VLSI Technology to do the actual \\nfabrication of the processor chips. VLSI was licensed to market the chip on its own \\nand had some success in getting other companies to use the ARM in their products, \\nparticularly as an embedded processor.\\nThe ARM design matched a growing commercial need for a \\xad\\nhigh-\\u200b\\n\\xad\\nperformance, \\n\\xad\\nlow-\\u200b\\n\\xad\\npower-\\u200b\\n\\xad\\nconsumption, \\xad\\nsmall-\\u200b\\n\\xad\\nsize, and \\xad\\nlow-\\u200b\\n\\xad\\ncost processor for embedded appli-\\ncations. But further development was beyond the scope of Acorn’s capabilities. \\nAccordingly, a new company was organized, with Acorn, VLSI, and Apple Com-\\nputer as founding partners, known as ARM Ltd. The Acorn RISC Machine became \\nAdvanced RISC Machines.12\\nInstruction Set Architecture\\nThe ARM instruction set is highly regular, designed for efficient implementation of \\nthe processor and efficient execution. All instructions are 32 bits long and follow a \\nregular format. This makes the ARM ISA suitable for implementation over a wide \\nrange of products.\\nAugmenting the basic ARM ISA is the Thumb instruction set, which is a \\xad\\nre-\\u200b\\n\\xad\\nencoded subset of the ARM instruction set. Thumb is designed to increase the per-\\nformance of ARM implementations that use a 16-bit or narrower memory data bus, \\n12The company dropped the designation Advanced RISC Machines in the late 1990s. It is now simply \\nknown as the ARM architecture.\\n1.6 / ARM Architecture\\u2002 \\u200235\\nand to allow better code density than provided by the ARM instruction set. The \\nThumb instruction set contains a subset of the ARM 32-bit instruction set recoded \\ninto 16-bit instructions. The current defined version is \\xad\\nThumb-\\u200b\\n\\xad\\n2.\\nThe ARM and \\xad\\nThumb-\\u200b\\n\\xad\\n2 ISAs are discussed in Chapters\\xa012 and 13.\\nARM Products\\nARM Holdings licenses a number of specialized microprocessors and related tech-\\nnologies, but the bulk of their product line is the Cortex family of microprocessor \\narchitectures. There are three Cortex architectures, conveniently labeled with the \\ninitials A, R, and\\xa0M.\\n\\xad\\ncortex-\\u200b\\n\\xad\\na/\\xad\\ncortex-\\u200b\\n\\xad\\na50 The \\xad\\nCortex-\\u200b\\n\\xad\\nA and \\xad\\nCortex-\\u200b\\n\\xad\\nA50 are application \\nprocessors, intended for mobile devices such as smartphones and eBook readers, \\nas well as consumer devices such as digital TV and home gateways (e.g., DSL and \\ncable Internet modems). These processors run at higher clock frequency (over \\n \\n1 GHz), and support a memory management unit (MMU), which is required for full \\nfeature OSs such as Linux, Android, MS Windows, and mobile OSs. An MMU is \\na hardware module that supports virtual memory and paging by translating virtual \\naddresses into physical addresses; this topic is explored in Chapter\\xa08.\\nThe two architectures use both the ARM and \\xad\\nThumb-\\u200b\\n\\xad\\n2 instruction sets; the \\nprincipal difference is that the \\xad\\nCortex-\\u200b\\n\\xad\\nA is a 32-bit machine, and the \\xad\\nCortex-\\u200b\\n\\xad\\nA50 is \\na 64-bit machine.\\n\\xad\\ncortex-\\u200b\\n\\xad\\nr The \\xad\\nCortex-\\u200b\\n\\xad\\nR is designed to support \\xad\\nreal-\\u200b\\n\\xad\\ntime applications, in which \\nthe timing of events needs to be controlled with rapid response to events. They can \\nrun at a fairly high clock frequency (e.g., 200MHz to 800MHz) and have very low \\nresponse latency. The \\xad\\nCortex-\\u200b\\n\\xad\\nR includes enhancements both to the instruction set \\nand to the processor organization to support deeply embedded \\xad\\nreal-\\u200b\\n\\xad\\ntime devices. \\nMost of these processors do not have MMU; the limited data requirements and \\nthe limited number of simultaneous processes eliminates the need for elaborate \\nhardware and software support for virtual memory. The \\xad\\nCortex-\\u200b\\n\\xad\\nR does have a \\nMemory Protection Unit (MPU), cache, and other memory features designed for \\nindustrial applications. An MPU is a hardware module that prohibits one program \\nin memory from accidentally accessing memory assigned to another active program. \\nUsing various methods, a protective boundary is created around the program, and \\ninstructions within the program are prohibited from referencing data outside of that \\nboundary.\\nExamples of embedded systems that would use the \\xad\\nCortex-\\u200b\\n\\xad\\nR are automotive \\nbraking systems, mass storage controllers, and networking and printing devices.\\n\\xad\\ncortex-\\u200b\\n\\xad\\nm \\xad\\nCortex-\\u200b\\n\\xad\\nM series processors have been developed primarily for the \\nmicrocontroller domain where the need for fast, highly deterministic interrupt \\nmanagement is coupled with the desire for extremely low gate count and \\nlowest possible power consumption. As with the \\xad\\nCortex-\\u200b\\n\\xad\\nR series, the \\xad\\nCortex-\\u200b\\n\\xad\\nM \\narchitecture has an MPU but no\\xa0MMU.\\xa0The \\xad\\nCortex-\\u200b\\n\\xad\\nM uses only the \\xad\\nThumb-\\u200b\\n\\xad\\n2 \\ninstruction set. The market for the \\xad\\nCortex-\\u200b\\n\\xad\\nM includes IoT devices, wireless \\nsensor/actuator networks used in factories and other enterprises, automotive \\nbody electronics, and so on.\\n36\\u2002 \\u2002 Chapter 1 / Basic Concepts and Computer Evolution \\nThere are currently four versions of the \\xad\\nCortex-\\u200b\\n\\xad\\nM series:\\n■\\n■\\xad\\nCortex-\\u200b\\n\\xad\\nM0: Designed for 8- and 16-bit applications, this model emphasizes low \\ncost, ultra low power, and simplicity. It is optimized for small silicon die size \\n(starting from 12k gates) and use in the lowest cost chips.\\n■\\n■\\xad\\nCortex-\\u200b\\n\\xad\\nM0+: An enhanced version of the M0 that is more energy efficient.\\n■\\n■\\xad\\nCortex-\\u200b\\n\\xad\\nM3: Designed for 16- and 32-bit applications, this model emphasizes \\nperformance and energy efficiency. It also has comprehensive debug and trace \\nfeatures to enable software developers to develop their applications quickly.\\n■\\n■\\xad\\nCortex-\\u200b\\n\\xad\\nM4: This model provides all the features of the \\xad\\nCortex-\\u200b\\n\\xad\\nM3, with addi-\\ntional instructions to support digital signal processing tasks.\\nIn this text, we will primarily use the ARM \\xad\\nCortex-\\u200b\\n\\xad\\nM3 as our example embed-\\nded system processor. It is the best suited of all ARM models for \\xad\\ngeneral-\\u200b\\n\\xad\\npurpose \\nmicrocontroller use. The \\xad\\nCortex-\\u200b\\n\\xad\\nM3 is used by a variety of manufacturers of micro-\\ncontroller products. Initial microcontroller devices from lead partners already \\ncombine the \\xad\\nCortex-\\u200b\\n\\xad\\nM3 processor with flash, SRAM, and multiple peripherals to \\nprovide a competitive offering at the price of just $1.\\nFigure\\xa01.16 provides a block diagram of the EFM32 microcontroller from Sil-\\nicon Labs. The figure also shows detail of the \\xad\\nCortex-\\u200b\\n\\xad\\nM3 processor and core com-\\nponents. We examine each level in turn.\\nThe \\xad\\nCortex-\\u200b\\n\\xad\\nM3 core makes use of separate buses for instructions and data. \\nThis arrangement is sometimes referred to as a Harvard architecture, in contrast \\nwith the von Neumann architecture, which uses the same signal buses and mem-\\nory for both instructions and data. By being able to read both an instruction and \\ndata from memory at the same time, the \\xad\\nCortex-\\u200b\\n\\xad\\nM3 processor can perform many \\noperations in parallel, speeding application execution. The core contains a decoder \\nfor Thumb instructions, an advanced ALU with support for hardware multiply and \\ndivide, control logic, and interfaces to the other components of the processor. In \\nparticular, there is an interface to the nested vector interrupt controller (NVIC) and \\nthe embedded trace macrocell (ETM) module.\\nThe core is part of a module called the \\xad\\nCortex-\\u200b\\n\\xad\\nM3 processor. This term is \\nsomewhat misleading, because typically in the literature, the terms core and pro-\\ncessor are viewed as equivalent. In addition to the core, the processor includes the \\nfollowing elements:\\n■\\n■NVIC: Provides configurable interrupt handling abilities to the processor. It \\nfacilitates \\xad\\nlow-\\u200b\\n\\xad\\nlatency exception and interrupt handling, and controls power \\nmanagement.\\n■\\n■ETM: An optional debug component that enables reconstruction of program \\nexecution. The ETM is designed to be a \\xad\\nhigh-\\u200b\\n\\xad\\nspeed, \\xad\\nlow-\\u200b\\n\\xad\\npower debug tool \\nthat only supports instruction trace.\\n■\\n■Debug access port (DAP): This provides an interface for external debug \\naccess to the processor.\\n■\\n■Debug logic: Basic debug functionality includes processor halt, \\xad\\nsingle-\\u200b\\n\\xad\\nstep, \\nprocessor core register access, unlimited software breakpoints, and full system \\nmemory access.\\nCortex-M3 Core\\nMicrocontroller Chip\\nCortex-M3\\nProcessor \\nNVIC\\ninterface\\nETM\\ninterface\\nHardware\\ndivider\\n32-bit\\nmultiplier\\n32-bit ALU\\nControl\\nlogic\\nThumb\\ndecode\\nInstruction\\ninterface\\nData\\ninterface\\nICode\\ninterface\\nDebug logic\\nARM\\ncore\\nDAP\\nNVIC\\nETM\\nMemory\\nprotection unit\\nBus matrix\\nSRAM &\\nperipheral I/F\\nSecurity\\nAnalog Interfaces\\nTimers & Triggers\\nParallel I/O Ports\\nSerial Interfaces\\nPeripheral bus\\nCore and memory\\nClock management\\nEnergy management\\nCortex-M3 processor\\nMemory\\nprotec-\\ntion unit\\nFlash\\nmemory\\n64 kB\\nVoltage\\nregula-\\ntor\\nPower-\\non reset\\nBrown-\\nout de-\\ntector\\nVoltage\\ncompar-\\nator\\nHigh fre-\\nquency RC\\noscillator\\nLow fre-\\nquency RC\\noscillator\\nHigh freq\\ncrystal\\noscillator\\nLow freq\\ncrystal\\noscillator\\nSRAM\\nmemory\\n64 kB\\nDebug\\ninter-\\nface\\nDMA\\ncontrol-\\nler\\nPulse\\ncounter\\nWatch-\\ndog tmr\\nLow\\nenergy\\nReal\\ntime ctr\\nPeriph\\nbus int\\nTimer/\\ncounter\\nGeneral\\npurpose\\nI/O\\nExternal\\nInter-\\nrupts\\nUART\\nUSART\\nLow-\\nenergy\\nUART\\nUSB\\nPin\\nreset\\n32-bit bus\\nA/D\\ncon-\\nverter\\nHard-\\nware\\nAES\\nD/A\\ncon-\\nverter\\nFigure\\xa01.16\\u2003 Typical Microcontroller Chip Based on \\xad\\nCortex-\\u200b\\n\\xad\\nM3\\n37\\n38\\u2002 \\u2002 Chapter 1 / Basic Concepts and Computer Evolution \\n■\\n■ICode interface: Fetches instructions from the code memory space.\\n■\\n■SRAM & peripheral interface: Read/write interface to data memory and \\nperipheral devices.\\n■\\n■Bus matrix: Connects the core and debug interfaces to external buses on the \\nmicrocontroller.\\n■\\n■Memory protection unit: Protects critical data used by the operating system \\nfrom user applications, separating processing tasks by disallowing access \\nto each other’s data, disabling access to memory regions, allowing memory \\nregions to be defined as \\xad\\nread-\\u200b\\n\\xad\\nonly, and detecting unexpected memory accesses \\nthat could potentially break the system.\\nThe upper part of Figure\\xa01.16 shows the block diagram of a typical micro-\\ncontroller built with the \\xad\\nCortex-\\u200b\\n\\xad\\nM3, in this case the EFM32 microcontroller. This \\nmicrocontroller is marketed for use in a wide variety of devices, including energy, \\ngas, and water metering; alarm and security systems; industrial automation devices; \\nhome automation devices; smart accessories; and health and fitness devices. The sil-\\nicon chip consists of 10 main areas:13\\n■\\n■Core and memory: This region includes the \\xad\\nCortex-\\u200b\\n\\xad\\nM3 processor, static RAM \\n(SRAM) data memory,14 and flash memory15 for storing program instructions \\nand nonvarying application data. Flash memory is nonvolatile (data is not lost \\nwhen power is shut off) and so is ideal for this purpose. The SRAM stores \\nvariable data. This area also includes a debug interface, which makes it easy to \\nreprogram and update the system in the field.\\n■\\n■Parallel I/O ports: Configurable for a variety of parallel I/O schemes.\\n■\\n■Serial interfaces: Supports various serial I/O schemes.\\n■\\n■Analog interfaces: \\xad\\nAnalog-\\u200b\\n\\xad\\nto-\\u200b\\n\\xad\\ndigital and \\xad\\ndigital-\\u200b\\n\\xad\\nto-\\u200b\\n\\xad\\nanalog logic to support \\nsensors and actuators.\\n■\\n■Timers and triggers: Keeps track of timing and counts events, generates out-\\nput waveforms, and triggers timed actions in other peripherals.\\n■\\n■Clock management: Controls the clocks and oscillators on the chip. Multiple \\nclocks and oscillators are used to minimize power consumption and provide \\nshort startup times.\\n■\\n■Energy management: Manages the various \\xad\\nlow-\\u200b\\n\\xad\\nenergy modes of operation of \\nthe processor and peripherals to provide \\xad\\nreal-\\u200b\\n\\xad\\ntime management of the energy \\nneeds so as to minimize energy consumption.\\n■\\n■Security: The chip includes a hardware implementation of the Advanced \\nEncryption Standard (AES).\\n13This discussion does not go into details about all of the individual modules; for the interested reader, an \\n\\xad\\nin-\\u200b\\n\\xad\\ndepth discussion is provided in the document EFM32G200.pdf, available at box.com/COA10e.\\n14Static RAM (SRAM) is a form of \\xad\\nrandom-\\u200b\\n\\xad\\naccess memory used for cache memory; see Chapter\\xa05.\\n15Flash memory is a versatile form of memory used both in microcontrollers and as external memory; it \\nis discussed in Chapter\\xa06.\\n1.7 / Cloud Computing\\u2002 \\u200239\\n■\\n■32-bit bus: Connects all of the components on the chip.\\n■\\n■Peripheral bus: A network which lets the different peripheral module commu-\\nnicate directly with each other without involving the processor. This supports \\n\\xad\\ntiming-\\u200b\\n\\xad\\ncritical operation and reduces software overhead.\\nComparing Figure\\xa01.16 with Figure\\xa01.2, you will see many similarities and \\nthe same general hierarchical structure. Note, however, that the top level of a \\nmicrocontroller computer system is a single chip, whereas for a multicore com-\\nputer, the top level is a motherboard containing a number of chips. Another note-\\nworthy difference is that there is no cache, neither in the \\xad\\nCortex-\\u200b\\n\\xad\\nM3 processor \\nnor in the microcontroller as a whole, which plays an important role if the code or \\ndata resides in external memory. Though the number of cycles to read the instruc-\\ntion or data varies depending on cache hit or miss, the cache greatly improves the \\nperformance when external memory is used. Such overhead is not needed for a \\nmicrocontroller.\\n\\t 1.7\\t Cloud Computing\\nAlthough the general concepts for cloud computing go back to the 1950s, cloud \\ncomputing services first became available in the early 2000s, particularly targeted \\nat large enterprises. Since then, cloud computing has spread to small and medium \\nsize businesses, and most recently to consumers. Apple’s iCloud was launched in \\n2012 and had 20\\xa0million users within a week of launch. Evernote, the \\xad\\ncloud-\\u200b\\n\\xad\\nbased \\nnotetaking and archiving service, launched in 2008, approached 100\\xa0million users \\nin less than 6\\xa0years. In this section, we provide a brief overview. Cloud computing is \\nexamined in more detail in Chapter\\xa017\\n.\\nBasic Concepts\\nThere is an increasingly prominent trend in many organizations to move a substantial \\nportion or even all information technology (IT) operations to an \\xad\\nInternet-\\u200b\\n\\xad\\nconnected \\ninfrastructure known as enterprise cloud computing. At the same time, individual \\nusers of PCs and mobile devices are relying more and more on cloud computing \\nservices to backup data, synch devices, and share, using personal cloud computing. \\nNIST defines cloud computing, in NIST \\xad\\nSP-\\u200b\\n\\xad\\n800-145 (The NIST Definition of Cloud \\nComputing), as follows:\\nCloud computing: A model for enabling ubiquitous, convenient, \\xad\\non-\\u200b\\n\\xad\\ndemand network \\naccess to a shared pool of configurable computing resources (e.g., networks, servers, \\nstorage, applications, and services) that can be rapidly provisioned and released with \\nminimal management effort or service provider interaction.\\nBasically, with cloud computing, you get economies of scale, professional \\nnetwork management, and professional security management. These features can \\nbe attractive to companies large and small, government agencies, and individual \\nPC and mobile users. The individual or company only needs to pay for the storage \\n40\\u2002 \\u2002 Chapter 1 / Basic Concepts and Computer Evolution \\ncapacity and services they need. The user, be it company or individual, doesn’t have \\nthe hassle of setting up a database system, acquiring the hardware they need, doing \\nmaintenance, and backing up the \\xad\\ndata—\\u200b\\n\\xad\\nall these are part of the cloud service.\\nIn theory, another big advantage of using cloud computing to store your data \\nand share it with others is that the cloud provider takes care of security. Alas, the \\ncustomer is not always protected. There have been a number of security failures \\namong cloud providers. Evernote made headlines in early 2013 when it told all of its \\nusers to reset their passwords after an intrusion was discovered.\\nCloud networking refers to the networks and network management function-\\nality that must be in place to enable cloud computing. Most cloud computing solu-\\ntions rely on the Internet, but that is only a piece of the networking infrastructure. \\nOne example of cloud networking is the provisioning of \\xad\\nhigh-\\u200b\\n\\xad\\nperformance and/or \\n\\xad\\nhigh-\\u200b\\n\\xad\\nreliability networking between the provider and subscriber. In this case, some \\nor all of the traffic between an enterprise and the cloud bypasses the Internet and \\nuses dedicated private network facilities owned or leased by the cloud service pro-\\nvider. More generally, cloud networking refers to the collection of network capa-\\nbilities required to access a cloud, including making use of specialized services over \\nthe Internet, linking enterprise data centers to a cloud, and using firewalls and other \\nnetwork security devices at critical points to enforce access security policies.\\nWe can think of cloud storage as a subset of cloud computing. In essence, cloud \\nstorage consists of database storage and database applications hosted remotely on \\ncloud servers. Cloud storage enables small businesses and individual users to take \\nadvantage of data storage that scales with their needs and to take advantage of a \\nvariety of database applications without having to buy, maintain, and manage the \\nstorage assets.\\nCloud Services\\nThe essential purpose of cloud computing is to provide for the convenient rental \\nof computing resources. A cloud service provider (CSP) maintains computing and \\ndata storage resources that are available over the Internet or private networks. \\nCustomers can rent a portion of these resources as needed. Virtually all cloud ser-\\nvice is provided using one of three models (Figure\\xa01.17): SaaS, PaaS, and IaaS, which \\nwe examine in this section.\\nsoftware as a service (SaaS) As the name implies, a SaaS cloud provides \\nservice to customers in the form of software, specifically application software, \\nrunning on and accessible in the cloud. SaaS follows the familiar model of Web \\nservices, in this case applied to cloud resources. SaaS enables the customer to use \\nthe cloud provider’s applications running on the provider’s cloud infrastructure. The \\napplications are accessible from various client devices through a simple interface \\nsuch as a Web browser. Instead of obtaining desktop and server licenses for \\nsoftware products it uses, an enterprise obtains the same functions from the cloud \\nservice. SaaS saves the complexity of software installation, maintenance, upgrades, \\nand patches. Examples of services at this level are Gmail, Google’s \\xad\\ne-\\u200b\\n\\xad\\nmail service, \\nand Salesforce.com, which help firms keep track of their customers.\\nCommon subscribers to SaaS are organizations that want to provide their \\nemployees with access to typical office productivity software, such as document \\n1.7 / Cloud Computing\\u2002 \\u200241\\nmanagement and email. Individuals also commonly use the SaaS model to acquire \\ncloud resources. Typically, subscribers use specific applications on demand. The \\ncloud provider also usually offers \\xad\\ndata-\\u200b\\n\\xad\\nrelated features such as automatic backup \\nand data sharing between subscribers.\\nplatform as a service (PaaS) A PaaS cloud provides service to customers in \\nthe form of a platform on which the customer’s applications can run. PaaS enables \\nthe customer to deploy onto the cloud infrastructure containing \\xad\\ncustomer-\\u200b\\n\\xad\\ncreated \\nor acquired applications. A PaaS cloud provides useful software building blocks, \\nplus a number of development tools, such as programming languages, \\xad\\nrun-\\u200b\\n\\xad\\ntime \\nenvironments, and other tools that assist in deploying new applications. In effect, \\nPaaS is an operating system in the cloud. PaaS is useful for an organization that \\nwants to develop new or tailored applications while paying for the needed computing \\nresources only as needed and only for as long as needed. Google App Engine and \\nthe Salesforce1 Platform from Salesforce.com are examples of PaaS.\\nApplications\\nInfrastructure as\\na service (IaaS)\\nTraditional IT\\narchitecture\\nPlatform as a\\nservice (PaaS)\\nSoftware as a\\nservice (SaaS)\\nManaged by client\\nApplication\\nFramework\\nCompilers\\nRun-time\\nenvironment\\nDatabases\\nOperating\\nsystem\\nVirtual\\nmachine\\nServer\\nhardware\\nStorage\\nNetworking\\nApplications\\nApplication\\nFramework\\nCompilers\\nRun-time\\nenvironment\\nDatabases\\nOperating\\nsystem\\nVirtual\\nmachine\\nServer\\nhardware\\nStorage\\nNetworking\\nMore complex\\nMore upfront cost\\nLess scalable\\nMore customizable\\nLess complex\\nLower upfront cost\\nMore scalable\\nLess customizable\\nIT = information technology\\nCSP = cloud service provider\\nManaged by CSP\\nApplications\\nManaged\\nby client\\nApplication\\nFramework\\nCompilers\\nRun-time\\nenvironment\\nDatabases\\nOperating\\nsystem\\nVirtual\\nmachine\\nServer\\nhardware\\nStorage\\nNetworking\\nManaged by CSP\\nApplications\\nApplication\\nFramework\\nCompilers\\nRun-time\\nenvironment\\nDatabases\\nOperating\\nsystem\\nVirtual\\nmachine\\nServer\\nhardware\\nStorage\\nNetworking\\nManaged by CSP\\nFigure\\xa01.17\\u2003 Alternative Information Technology Architectures\\n42\\u2002 \\u2002 Chapter 1 / Basic Concepts and Computer Evolution \\ninfrastructure as a service (IaaS) With IaaS, the customer has access to the \\nunderlying cloud infrastructure. IaaS provides virtual machines and other abstracted \\nhardware and operating systems, which may be controlled through a service \\napplication programming interface (API). IaaS offers the customer processing, \\nstorage, networks, and other fundamental computing resources so that the customer \\nis able to deploy and run arbitrary software, which can include operating systems \\nand applications. IaaS enables customers to combine basic computing services, \\nsuch as number crunching and data storage, to build highly adaptable computer \\nsystems. Examples of IaaS are Amazon Elastic Compute Cloud (Amazon EC2) and \\nWindows Azure.\\n\\t 1.8\\t Key Terms, Review Questions, and Problems\\nKey Terms\\napplication processor\\narithmetic and logic unit \\n(ALU)\\nARM\\ncentral processing unit  \\n(CPU)\\nchip\\ncloud computing\\ncloud networking\\ncloud storage\\ncomputer architecture\\ncomputer organization\\ncontrol unit\\ncore\\ndedicated processor\\ndeeply embedded system\\nembedded system\\ngate\\ninfrastructure as a service \\n(IaaS)\\n\\xad\\ninput–\\u200b\\n\\xad\\noutput (I/O)\\ninstruction set architecture \\n(ISA)\\nintegrated circuit\\nIntel x86\\nInternet of things (IoT)\\nmain memory\\nmemory cell\\nmemory management unit \\n(MMU)\\nmemory protection unit  \\n(MPU)\\nmicrocontroller\\nmicroelectronics\\nmicroprocessor\\nmotherboard\\nmulticore\\nmulticore processor\\noriginal equipment  \\nmanufacturer (OEM)\\nplatform as a service  \\n(PaaS)\\nprinted circuit board\\nprocessor\\nregisters\\nsemiconductor\\nsemiconductor memory\\nsoftware as a service (SaaS)\\nsystem bus\\nsystem interconnection\\nvacuum tubes\\nReview Questions\\n\\t 1.1\\t\\nWhat, in general terms, is the distinction between computer organization and com-\\nputer architecture?\\n\\t 1.2\\t\\nWhat, in general terms, is the distinction between computer structure and computer \\nfunction?\\n\\t 1.3\\t\\nWhat are the four main functions of a computer?\\n\\t 1.4\\t\\nList and briefly define the main structural components of a computer.\\n\\t 1.5\\t\\nList and briefly define the main structural components of a processor.\\n\\t 1.6\\t\\nWhat is a stored program computer?\\n\\t 1.7\\t\\nExplain Moore’s law.\\n\\t 1.8\\t\\nList and explain the key characteristics of a computer family.\\n\\t 1.9\\t\\nWhat is the key distinguishing feature of a microprocessor?\\n1.8 / Key Terms, Review Questions, and Problems\\u2002 \\u200243\\nProblems\\n\\t 1.1\\t\\nYou are to write an IAS program to compute the results of the following equation.\\nY = a\\nN\\nX=1\\nX\\nAssume that the computation does not result in an arithmetic overflow and that X, Y, \\nand N are positive integers with N ≥ 1. Note: The IAS did not have assembly language, \\nonly machine language.\\na.\\t Use the equation Sum(Y) =\\nN(N + 1)\\n2\\n when writing the IAS program.\\nb.\\t Do it the “hard way,” without using the equation from part (a).\\n\\t\\n1.2\\t\\na.\\t \\x07\\nOn the IAS, what would the machine code instruction look like to load the con-\\ntents of memory address 2 to the accumulator?\\n\\t\\n\\t\\nb.\\t \\x07\\nHow many trips to memory does the CPU need to make to complete this instruc-\\ntion during the instruction cycle?\\n\\t 1.3\\t\\nOn the IAS, describe in English the process that the CPU must undertake to read a \\nvalue from memory and to write a value to memory in terms of what is put into the \\nMAR, MBR, address bus, data bus, and control bus.\\n\\t 1.4\\t\\nGiven the memory contents of the IAS computer shown below,\\nAddress\\nContents\\n08A\\n010FA210FB\\n08B\\n010FA0F08D\\n08C\\n020FA210FB\\nshow the assembly language code for the program, starting at address 08A.\\xa0Explain \\nwhat this program does.\\n\\t 1.5\\t\\nIn Figure\\xa01.6, indicate the width, in bits, of each data path (e.g., between AC and ALU).\\n\\t 1.6\\t\\nIn the IBM 360 Models 65 and 75, addresses are staggered in two separate main mem-\\nory units (e.g., all \\xad\\neven-\\u200b\\n\\xad\\nnumbered words in one unit and all \\xad\\nodd-\\u200b\\n\\xad\\nnumbered words in \\nanother). What might be the purpose of this technique?\\n\\t 1.7\\t\\nThe relative performance of the IBM 360 Model 75 is 50 times that of the 360 Model \\n30, yet the instruction cycle time is only 5 times as fast. How do you account for this \\ndiscrepancy?\\n\\t 1.8\\t\\nWhile browsing at Billy Bob’s computer store, you overhear a customer asking Billy \\nBob what is the fastest computer in the store that he can buy. Billy Bob replies, “You’re \\nlooking at our Macintoshes. The fastest Mac we have runs at a clock speed of 1.2 GHz. \\nIf you really want the fastest machine, you should buy our 2.4-GHz Intel Pentium IV \\ninstead.” Is Billy Bob correct? What would you say to help this customer?\\n\\t 1.9\\t\\nThe ENIAC, a precursor to the ISA machine, was a decimal machine, in which each \\nregister was represented by a ring of 10 vacuum tubes. At any time, only one vacuum \\ntube was in the ON state, representing one of the 10 decimal digits. Assuming that \\nENIAC had the capability to have multiple vacuum tubes in the ON and OFF state \\nsimultaneously, why is this representation “wasteful” and what range of integer values \\ncould we represent using the 10 vacuum tubes?\\n\\t 1.10\\t\\nFor each of the following examples, determine whether this is an embedded system, \\nexplaining why or why not.\\na.\\t Are programs that understand physics and/or hardware embedded? For example, \\none that uses \\xad\\nfinite-\\u200b\\n\\xad\\nelement methods to predict fluid flow over airplane wings?\\nb.\\t Is the internal microprocessor controlling a disk drive an example of an embedded \\nsystem?\\n44\\u2002 \\u2002 Chapter 1 / Basic Concepts and Computer Evolution \\nc.\\t I/O drivers control hardware, so does the presence of an I/O driver imply that the \\ncomputer executing the driver is embedded?\\nd.\\t Is a PDA (Personal Digital Assistant) an embedded system?\\ne.\\t Is the microprocessor controlling a cell phone an embedded system?\\nf.\\t Are the computers in a big \\xad\\nphased-\\u200b\\n\\xad\\narray radar considered embedded? These \\nradars are 10-story buildings with one to three 100-foot diameter radiating patches \\non the sloped sides of the building.\\ng.\\t Is a traditional flight management system (FMS) built into an airplane cockpit \\nconsidered embedded?\\nh.\\t Are the computers in a \\xad\\nhardware-\\u200b\\n\\xad\\nin-\\u200b\\n\\xad\\nthe-\\u200b\\n\\xad\\nloop (HIL) simulator embedded?\\ni.\\t\\nIs the computer controlling a pacemaker in a person’s chest an embedded \\ncomputer?\\nj.\\t Is the computer controlling fuel injection in an automobile engine embedded?\\n45\\nChapter\\nPerformance Issues\\n2.1\\t\\nDesigning for Performance\\t\\nMicroprocessor Speed\\nPerformance Balance\\nImprovements in Chip Organization and Architecture\\n2.2\\t\\nMulticore, MICs, and GPGPUs\\t\\n2.3\\t\\nTwo Laws that Provide Insight: Amdahl’s Law and Little’s Law\\t\\nAmdahl’s Law\\nLittle’s Law\\n2.4\\t\\nBasic Measures of Computer Performance\\t\\nClock Speed\\nInstruction Execution Rate\\n2.5\\t\\nCalculating the Mean\\t\\nArithmetic Mean\\nHarmonic Mean\\nGeometric Mean\\n2.6\\t\\nBenchmarks and SPEC\\t\\nBenchmark Principles\\nSPEC Benchmarks\\n2.7\\t\\nKey Terms, Review Questions, and Problems\\t\\n'"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chapter_dict[1]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-30T09:53:51.077069414Z",
     "start_time": "2024-04-30T09:53:51.038576817Z"
    }
   },
   "id": "369a7441efec674a",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "109820"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chapter_dict[1])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-30T09:53:51.078877356Z",
     "start_time": "2024-04-30T09:53:51.047368814Z"
    }
   },
   "id": "cb26b7831c5127ef",
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# load spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-30T09:53:51.521399201Z",
     "start_time": "2024-04-30T09:53:51.054675809Z"
    }
   },
   "id": "d9f4c8c976b1d8c5",
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "doc = nlp(chapter_dict[1])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-30T09:53:54.472642742Z",
     "start_time": "2024-04-30T09:53:51.542145493Z"
    }
   },
   "id": "b6d8194e5083bbb7",
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "sentences = [sent.text for sent in doc.sents if len(sent.text.strip()) > 0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-30T09:53:54.495812179Z",
     "start_time": "2024-04-30T09:53:54.492992496Z"
    }
   },
   "id": "8210c478b412dc2d",
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Summarizing sentences: 100%|██████████| 789/789 [36:38<00:00,  2.79s/it]\n"
     ]
    }
   ],
   "source": [
    "summaries = []\n",
    "for sentence in tqdm(sentences, desc=\"Summarizing sentences\"):\n",
    "    summaries.append(summarize(sentence))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-30T10:30:33.402273639Z",
     "start_time": "2024-04-30T09:53:54.496909800Z"
    }
   },
   "id": "55f9c8860894aa63",
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In describing computers, a distinction is often made between computer architec-ture and computer organization. Computer organization refers to the operational units and their interconnections that realize the architectural specifications. Computer architecture refers to those attributes of a system visible to a pro-grammer.\n"
     ]
    }
   ],
   "source": [
    "final_summary = summarize(\" \".join(summaries))\n",
    "print(final_summary)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-30T10:30:36.892108053Z",
     "start_time": "2024-04-30T10:30:33.401409253Z"
    }
   },
   "id": "5a8303d42f2c77d4",
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "'2\\u2002 \\u2002 Chapter 1 / Basic Concepts and Computer Evolution \\t 1.1\\t Organization and Architecture of the Computer World. 1.2\\u2009 \\u2002 \\u2009 \\u2009 Basic Concepts of Computer Evolution. \\u2009 Organization and architecture of the computer world. In describing computers, a distinction is often made between computer architec-ture and computer organization. A distinction is made between \"architec\" and \"organization\" in terms of the way computers are used. The term is often used to refer to computers that are used for business purposes. Although it is difficult to give precise definitions for these terms, a consensus exists about the general areas covered by each. For example, the term \"sociology\" is used to refer to a range of fields of study. For more information on the terms, see:\\xa0\"Sociology.com\" For  example, see [VRAN80], [SIEW82], and [BELL78a]; an interesting alternative view is presented in [REDD76]. For example, \\xa0for example, [VRan80] \\xa0is presented as an alternative view of VRAN. \\xa0For example, see \\xa0VAN80. Computer architecture refers to those attributes of a system visible to a pro-grammer or, put another way, those attributes that have a direct impact on the logical execution of a program. Computer architecture can also be used to refer to the design of a computer system. A term that is often used interchangeably with com-puter architecture is instruction set architecture (ISA) ISA is a type of computer architecture. ISA can be used to refer to a number of different types of computer systems. For example, ISA may refer to: The ISA defines instruction formats, instruction opcodes, registers, instruction and data memory. It also defines the effect of executed instructions on the registers and memory; and an algorithm for control-ling instruction execution. The ISA was created by the National Institute of Standards and Technology. Computer organization refers to the operational units and their interconnections that realize the architectural specifications. Computer organization can also refer to the\\xa0operational\\xa0units and their\\xa0interconnections\\xa0that realize the\\xa0architectural\\xa0 specifications. For more information on computer organization, see Computer Organization. Examples of architectural attributes include the instruction set, the number of bits used to repre-sent various data types, I/O mechanisms, and techniques for addressing memory. Examples of  architectural attributes include: \\xa0the number ofbits used to send data types (e.g., numbers, characters), \\xa0I/O\\xa0mechanism\\xa0and techniques  for addressing memory\\xa0in a computer. Organizational attributes include those hardware details transparent to the programmer. These include control signals, interfaces between the com-puter and peripherals; and the memory technology used. For more information on how to get started with your own computer, visit www.computerworld.com. For example, it is an architectural design issue whether a computer will have a multiply instruction. It is also a question of whether the computer will be able to read and write data at the same time. For more information, go to: http://www.cnn.com/2013/01/26/technology/technology-design/how-to-design-a-computer-with-multiply- instruction. It is an organizational issue whether that instruction will be implemented by a special multiply unit or by a mechanism that makes repeated use of the add unit of the system. It is also an issue whether the multiply unit will be used in conjunction with another unit. The organizational decision may be based on the anticipated frequency of use of the multiply instruction, the relative speed of the two approaches, and the cost and physical size of a special multiply unit. For more information on how to use a multiply unit, visit multiplyunit.org. Historically, and still today, the distinction between architecture and organ-ization has been an important one. The distinction between the two has been important in the history of architecture. It has also been important for the development of the art form of architecture and design. It is still an important distinction today. Many computer manufacturers offer a family of computer models, all with the same architecture but with differences in organization. Many computer manufacturers. offer aFamily of Computer Models. All have the same\\xa0architecture\\xa0but\\xa0different\\xa0organization\\xa0and different\\xa0models\\xa0of\\xa0the computer. Consequently, the different models in the family have different price and perform-ance characteristics. For more information, visit the official website of the BMW M3 and M4 family. For information on the BMW X3, visit\\xa0the official\\xa0website\\xa0of the BMW\\xa0M3 family. A particular architecture may span many years and encompass a number of different computer models, its organization changing with changing technology.Furthermore, a particular architecture\\xa0may span\\xa0many years  and encompass a\\xa0number\\xa0of\\xa0different computer models. For more information on how to build a computer, visit the Computer Architecture Institute\\'s website. A prominent example of both these phenomena is the IBM System/370 architecture. The architecture is based on the\\xa0IBM\\xa0System/370 system, which was designed in the 1980s and 1990s. It was designed to be able to run multiple versions of the same software at once. This architecture was first introduced in 1970 and is based on the design of the World Trade Center in New York City. It was designed to be the tallest building in the world. The goal of this chapter is to teach you how to use this architecture to build skyscrapers. Explain the difference between the two words \"cocaine\" and \"cannabis\" in the U.S. \"Cocaine is a form of cannabis. It is legal in the United States, but not in most other countries. Cocaine can be used to help people with depression and other mental illnesses. the general functions and structure of a digital computer. A digital computer is a computer with a built-in operating system. A computer can be programmed to do a range of tasks. It can also be used to make decisions about what it should do. The computer can also make decisions on how much data it should store. Present an overview of the evolution of computer technology from early digital computers to the latest microprocessors. Present a selection of the most popular computer games of the past 20 years. Present the latest developments in computer technology, such as the latest processors and operating systems. For more information, visit the Computer History Museum\\'s website. The x86 architecture. Present an overview of the evolution of the x86. architecture. The x86-RISC architecture. Show how it has evolved over the last 20 years. Show the progress of x86 in the last 10 years. See www.x86.org for more information. RPC: Define embedded systems and list some of the requirements and constraints  that various embedded systems must meet. RPC: Structure and Function: List of requirements, constraints and constraints for embedded systems. Rpc: Structure, Function, Structure and Structure, Structure, and Function. The customer with modest requirements could buy a cheaper, slower model and, if demand increased, later upgrade to a more expensive, faster model without having to abandon software that had already been developed.  The software was developed by a team of engineers based in London. Over the years, IBM has introduced many new models with improved technology to replace older models. The new models offer the customer greater speed, lower cost, or both.  IBM has launched a range of new products and services to help customers get the most out of their new computers. These newer models retained the same architecture so that the customer’s soft-ware investment was protected. These newer models retain the same Architecture so that they can be easily updated and updated with the latest software. These new models have been released in the U.S. and Europe. Remarkably, the System/370 architecture, with a few enhancements, has survived to this day as the architecture of IBM’s mainframe product line. It is still used today in a number of IBM products, including the IBM System/360. In a class of computers called microcomputers, the relationship between archi-tecture and organization is very close. In the world of microcomputing, this is particularly true for the development of software. In this case, the goal is to make software that can be used to help people in need. Changes in technology not only influence organization but also result in the introduction of more powerful and more complex architectures. Changes in technology have led to an increase in the number of people involved in the construction of new buildings. This has led to a rise in the use of more complex and complex designs. Generally, there is less of a requirement for \\xad\\xa0compatibility\\xa0for smaller machines. For example, there are fewer requirements for \\xad˚Ś˚\\xa0‚œ˚’‚\\u2009\\u2009compatibility for smaller machines than larger machines. Thus, there is more interplay between organizational and architectural design decisions. There is also more\\xa0interplay\\xa0between\\xa0architectural\\xa0design\\xa0decisions and\\xa0organizational\\xa0decision\\xa0making. There are also more \\xa0interactions\\xa0between  organizational and architectural\\xa0design decisions. An intriguing example of this is the reduced instruction set computer (RISC), which we examine in Chapter\\xa015. The RISC computer is based on a\\xa0reduced\\xa0 instruction set processor. RISC can be used to build computers with a number of different\\xa0programs. This book examines both computer organization and computer architecture. It is written in the form of a series of short articles. The book is published by Simon & Schuster, Inc. (USA) and is available in hardback and e-book. For more information on this book, visit www.simonandschuster.com. The emphasis is perhaps more on the side of organization. The focus is on the organization, rather than the individual. The emphasis is on organization, not the individual, as in this case. The goal is to get the best out of the team, not just the best players. A computer is a complex system; contemporary computers contain millions of elementary electronic components. A computer organization must be designed to implement a particular architectural specification. A thorough treatment of organization requires a detailed examination of architecture as well.\\t 1.2\\t Structure and Function How, then, can one clearly describe them? How can one describe a person? How do you describe a human being? What do you mean by \\'human\\' or \\'person\\'? How do we define them? What are they? What am I supposed to say? What can I say? The key is to recognize the hierarchical nature of most complex systems, including the computer [SIMO96]. The key  is torecognize the  hierarchical nature  of most\\xa0complex\\xa0systems, including\\xa0the\\xa0computer. The  key is to\\xa0recognize\\xa0the hierarchical nature\\xa0of\\xa0most complex systems. A hierarchical system is a set of interrelated subsystems, each  of the latter, in turn, hierarchical in structure until we reach some lowest level of elementary subsystem. A hierarchical system can be described as a series of interconnected systems, or a hierarchy of them. The word \"hierarchy\" comes from the Greek word for \"system\" The hierarchical nature of complex systems is essential to both their design and their description. The hierarchy of a complex system is essential for both its design and its description. A hierarchical system is a system that is designed to be hierarchical. A complex system can be described as a series of hierarchical systems. The designer need only deal with a particular level of the system at a time. The designer need not deal with all aspects of the system at once. A designer can focus on one aspect of the\\xa0system\\xa0at a time, for example, by focusing on a particular area of it. At each level, the system consists of a set of components and  their interrelationships. At each level of the system, the components are connected by a series of links. The system is designed to work at all levels of the human brain. It is intended to work in a variety of different ways, depending on the user\\'s needs. The behavior at each level depends only on a simplified, abstracted characterization of the system at the next lower level. The behavior at the lower levels is based on a\\xa0simplified, \\xa0describing\\xa0the behavior of the whole system. The system can be described in terms of a series of lower levels of abstraction. At each level, the designer is concerned with structure and function. The way in which the components are interrelated. At each level of the  design process, the\\xa0designer\\xa0is\\xa0concerned\\xa0with structure\\xa0and function. For more information, visit www.cnn.com/design. The operation of each individual component as part of the structure. The operation of the entire structure as a whole. The function of each component as a part of a larger system. The functions of each of the components. The operations of the whole structure. the functions of the individual components. the function of the overall structure. In terms of description, we have two choices: starting at the bottom and build-ing up to a complete description, or beginning with a top view and decomposing the  system into its subparts. We are going to start with the bottom view and work our way up. Evidence from a number of fields suggests that the \\xadtop-\\u200bcentric\\xaddown approach is the clearest and most effective [WEIN75]. This approach is based on the idea that the top of a person\\'s body is the most important part of their body. It is also called the \\'top-centric\\' or \\'topical\\' approach. The approach taken in this book follows from this viewpoint. The book is written from the point of view of a young boy. It is written in the form of a letter written by a boy to his father. The letter is titled \"Letter from a Young Boy to his Father\" The computer system will be described from the top down. The system will be designed from the bottom up. It will be based on a set of rules that can be changed at any time. The rules will be written to suit the needs of the computer system. The computer system will also be designed to be flexible. We begin with the major components of a computer, describing their structure and function. We then proceed to the lower layers of the hierarchy. We conclude with a discussion of the role of computers in society and society in general. We end with a review of the history of computers and their development. The remainder of this section provides a very brief overview of this plan of attack. It is intended to provide a brief overview of what is to come in the next few days. The plan of action is to take out a large number of targets in the first few days of September. Both the structure and functioning of a computer are, in essence, simple. The structure of the computer is simple, and so is the functioning of it. The computer is designed to work in a very simple way. It is designed for the purpose of computing, not for any other purpose. In general  terms, there are only four basic functions that a computer can perform:Data processing: Data may take a wide variety of forms, and the range of pro- \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0cessing requirements is broad. Data analysis: Data can take a variety of different forms, including text, images, and video. However, we shall see that there are only a few fundamental methods or types of data processing. First, we will look at how the data is processed. Then, we\\'ll look at some of the methods used to analyse the data. We\\'ll conclude with a look at the results of the data analysis. Even if a computer is processing data on the fly, it must temporarily store at least those pieces of data that are being worked on at any given moment. Even if the results go out immediately, the computer must temporarily storage at least some of the data that is being processed. Thus, there is at least a \\xad short-\\u200bterm and long-term data storage function. The system is designed to store data for as long as a decade or more. It can be used to store up to five years of data at a time. For more information on the system, visit the company\\'s website. Equally important, the computer performs a \\xadlong-\\u200b \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xadterm data storage function. The computer is designed to store data for up to 10 years. The data is stored in a variety of formats, including text, images, and video. It can also be used to store audio and video files. Files of data are stored on the computer for subsequent retrieval and update. Files of data can be retrieved from the computer and stored on a hard drive. Files can also be uploaded to the cloud for later retrieval or update. For more information on how to download files, visit www.cnn.com/download. Data movement: The computer’s operating environment consists of devices that serve as either sources or destinations of data. Data movement: Devices can be used to move data from one place to another. The term ‘data movement’ is used to refer to the movement of data on a computer. When data are received  from or delivered to a device that is directly connected to the computer, the  process is known as \\xad \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0input–\\u200b\\xad \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0output (I/O) and the device is referred to as a peripheral. The data is then sent to a computer that is able to process it. When data are moved over longer distances, to or from a remote device, the process is known as data communications. This is when data is sent from one device to another over a long distance. The process is called data communications and is used to send data over long distances. For more information on data communications, visit the Data Communications website. Within the computer, a control unit manages the computer’s resources and orchestrates the performance of its functional parts in response to instructions. A control unit is a part of a computer that controls its functions, such as the display of a screen or the sound of its speakers. The preceding discussion may seem absurdly generalized. It is meant to be a starting point for a discussion of the role of religion in the culture of the U.S. and the world at large. The following discussion is meant as a beginning and end to the discussion of religion. It is certainly possible, even at a top level of computer structure, to differentiate a variety of func- \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0tions, but to quote [SIEW82]: \"There is remarkably little shaping of computer Structure to fit the function to be performed\" It is certainly\\xa0possible\\xa0to\\xa0differentiate\\xa0a variety of\\xa0frequencies\\xa0on a computer. At the root of this lies the \\xad˚general-\\u200b\\xadpurpose nature of computers. All the functional specialization occurs at the time of programming and not at theTime of design. This is the reason why computers are so difficult to program. It is also why they are so hard to use. Structure of the U.S. government is complex and complex. The U.N. is a complex organization with multiple levels of government. It is the largest government in the world, with a population of more than 1.2 billion people. The United States is one of the largest countries in the United Nations. We now look in a general way at the internal structure of a computer. We now look at it in a more general way. We can see how it works in a number of different ways. We are now able to look at the structure of the computer in a different way. We begin with a traditional computer with a single processor that employs a microprogrammed control unit. Then examine a typical multicore structure. We begin with  a traditional computers with a one processor and a micro-programmed control unit. We then examine a Typical multicore Structure. We conclude with a Typical Multicore Structure with a Microprogrammed Control Unit. simple \\xad774single-\\u200b \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0“simple”processor computer. Figure\\xa01.1 provides a hierarchical view  of the internal structure of a traditional \\xad774 single-processor\\xa0computer. Figure 1.1: A hierarchical view of the\\xa0internal\\xa0structures of a\\xa0traditional \\xad774Single-Processor\\xa0Computer. There are four main structural components of the computer: CPU, memory, storage, memory and storage. There is also a central processing unit (CPU) for the computer\\'s main processing unit. There are four\\xa0main structural\\xa0components\\xa0of the computer, including the\\xa0memory, storage and memory. Controls the operation of the computer and performs its data processing functions; often simply referred to as processor. Also known as a computer administrator or computer administrator. Can also be called a computer operator or a computer\\xa0administrator\\xa0and can control the\\xa0operations\\xa0of the computer. ’s main memory: Stores data. I/O: Moves data between the computer and its external environment. Structure and Function: Structure, Function, Structure and Structure. ’s structure and function is a set of rules for how the computer should look and behave. System interconnection: Some mechanism that provides for communication among CPU, main memory, and I/O. A common example of system intercon-nection is by means of a system bus. System bus consists of a number of conducting wires to which all the other components attach. There may be one or more of each of the aforementioned components. For example, there may be two or more components to the word \"council\" There may be more than one component to a word. For instance, there could be two components to \"carpenters\" Tra-ditionally, there has been just a single processor. The new processor is expected to have a much more powerful processor than its predecessor. The processor will be based on a new chip that has a much larger processor. It will also have a new processor that can run multiple processors. In recent years, there has been anincreasing use of multiple processors in a single computer. This has led to the use of different processors in different parts of the same computer. For more information on how to use multiple processors, visit Computerworld.com. For further information on using multiple processors on a computer, visit the Computerworld website. Some design issues relat- \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0ing to multiple processors crop up and are discussed as the text proceeds. Part Five focuses on such computers. Part Six will focus on computers with multiple processors. Part Seven will look at computers with a single processor. Part Eight will discuss computers with two or more processors. Each of these components will be examined in some detail in Part Two. The Computer: \\xad \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0Top-\\u200b \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xad \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xadLevel Structure\\xa0‘The Computer’: ‘The Basic Concepts and Computer Evolution’ ‘‘’ The computer’s’\\xa0’’Level Structure’ – ‘Top’, ‘Level’. How-ever, for our purposes, the most interesting and in some ways the most complex  component is the CPU. The CPU is the most important part of a computer. The most complex and most complex part of the computer is the\\xa0CPU. The\\xa0CPU\\xa0is the most\\xa0intense\\xa0and\\xa0complex\\xa0part of the\\xa0computer. Its major structural components are as follows: Control unit, control unit, and control unit. Control unit: controls the operation of the CPU and hence the computer. control unit: Controls the operation\\xa0of the CPU, and thus the computer\\'s\\xa0operations. Control Unit: controls\\xa0the operation of\\xa0the\\xa0 CPU and the computer, and hence its operation. Control\\xa0unit: Controls\\xa0operates\\xa0the CPU and\\xa0the computer. Arithmetic and logic unit (ALU): Performs the computer’s data processing functions. Alu: Performs arithmetic, logic, and data processing functions of the computer. AlU: Performed the computer\\'s data processing and data analysis functions. Registers: Provides storage internal to the CPU. Provides storage for data stored on the CPU\\'s internal memory. System can also store data on external hard drives and external hard drive storage. It can store data for up to two years on a single drive. It has a total storage capacity of 1.2GB. CPU interconnection: Some mechanism that provides for communication among the control unit, ALU, and registers. CPU interconnection may also be called CPU-ALU interconnection or CPU-Alu interconnection. It can also be known as CPU-A, CPU-U, orCPU-A-U. Part Three covers these components, where we will see that complexity is added by the use of parallel and pipelined organizational techniques. Part Three covers the use of\\xa0parallel\\xa0and\\xa0pipelined\\xa0organization techniques. We will then look at how these techniques can be used in a variety of different ways. Finally, there are sev-eral approaches to the implementation of the control unit. One common approach is \\xa0a microprogrammed implementation. For more information on how to implement a control unit, see the Control Unit Handbook. For information on microprogramming, visit the Microprogrammed Implementation Handbook. In essence, a microprogrammed control unit operates by executing microinstructions that define the functionality of the control unit. A control unit is a device that can be programmed to follow a set of instructions. The instructions are designed to allow the control unit to operate in a specific way. With this approach, the structure of the control unit can be depicted, as in Figure\\xa01.1. The structure of a control unit is shown in the form of a circle. The shape of the unit can also be seen in the shape of a triangle. multicore computer structure As was mentioned, contemporary computers generally have multiple processors. This structure is examined in Part Four of this series. For more information on the Multicore Computer Structure, visit the Computer Structures website. For further information about the Multi-Core Computer structure, please visit the Computers and Multicores website. When these processors all reside on a single chip, the term multicore computer is used. Each processing unit (consisting of a control unit, ALU, registers, and perhaps cache) is called a core. The term core is used to refer to a computer with multiple processing units. To clarify the terminology, this text will use the following definitions. The definitions are intended to clarify the language used in the book. The book will be published in hardback and paperback editions. For more information on the book, visit the publisher’s website. Central processing unit (CPU): That portion of a computer that fetches and executes instructions. It is the part of the computer that executes and fetches instructions. The CPU is the most important part of any computer, and is responsible for much of its functionality. It consists of an ALU, a control unit, and registers.  It was used in the U.S. Air Force for the first time in the 1980s. It is still used in some parts of the Air Force today. It was also used by the US Air Force in the 1990s. In a system with a single processing unit, it is often simply referred to as a processor. In a system without a single processor, the unit is called a processing unit. It can also be called a single-processor or a\\xa0single-processing\\xa0unit. Core: An individual processing unit on a processor chip. Processor: A chip that processes data from other chips on the same chip. processor chip: The chip\\'s processing unit is the core of the chip. chip chip: It\\'s the processing unit that processes information from other chip chips. A core may be equiv- \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0alent in functionality to a CPU on a \\xadsingle-\\u200b \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xadCPU system. A core may also be\\xa0equivalent\\xa0to a core on a single-core CPU system. For example, a core may not be as powerful as a core, but it may be more efficient. Other specialized pro-cessing units, such as one optimized for vector and matrix operations, are also referred to as cores. A core is a unit of power that can be used to power a computer. The word \"core\" comes from the Latin word for core, and is used to refer to a special type of core. Processor: A physical piece of silicon containing one or more cores. Device: A piece of technology or equipment that can be used to make a product or service. Technology: A device that can make a device or service that can also be used for other purposes. Devices: A set of chips, a piece of equipment, or a set of processes. The processor is the computer component that interprets and executes instruc-tions. It is the part of the computer system that runs the computer\\'s software. It can be used to run programs such as Microsoft Word, Excel, and other programs. The processor can also be used as a source of data for software. If a processor contains multiple cores, it is referred to as a multicore processor. For more information on multicore processors, see\\xa0multicore\\xa0processor\\xa0and\\xa0multiprocessors. For a list of processors with more than one core, see the list of processor\\xa0architectures. After about a decade of discussion, there is broad industry consensus on this usage. There is also an industry consensus that this is a good thing for the environment. There are no plans for this to be changed in the near future, however. It will be used in a variety of ways in the future. Another prominent feature of contemporary computers is the use of multiple layers of memory, called cache memory, between the processor and main memory.  cache memory is used to store data between processor and memory. It is a form of memory management that allows computers to store large amounts of data in one place.  chapter 4 is devoted to the topic of cache memory. Chapter\\xa04 is devoted\\xa0to the topic\\xa0of cache memory in the context of the history of memory. The book is published by Oxford University Press and is available in hardback and paperback editions. The first edition of the book was published in 1998 and the second edition in 1999. A cache memory is smaller and faster than main memory. It is used to speed up memory access, by placing in the cache data from main memory, that is likely to be used in the near future. For our purposes in this section, \\xa0we simply note that a cache memory \\xa0is\\xa0used to speed\\xa0up memory access. A greater performance improvement may be obtained by using multiple levels of cache, with level 1 (L1) closest to the core  and additional levels (L2, L3, and so on) progressively farther from the core. For more information, see cache. In this scheme, level n is smaller and faster than level n + 1.1.2. Figure 1.2 is a simplified view of the principal components of a typical mul-tesqueticore computer. In this\\xa0scheme\\xa0level n is\\xa0smaller\\xa0and\\xa0faster\\xa0than level n - 1.0. Most computers, including embedded computers in smartphones and tablets, plus personal computers, laptops, and workstations, are housed on a motherboard. Most computers  housed on  motherboards are called\\'motherboards\\' and are used by computers and other computers. The motherboard is a piece of equipment that houses the computer\\'s operating system. Before describing this arrangement, we need to define some terms. We need to first define what we mean by the term ‘couples’ and ‘partners’. We also need to explain the ‘relationship’ between a couple and their partner. A printed circuit board (PCB) is a rigid, flat board that holds and interconnects chips and other electronic components. A printed circuit boards is a flat board with a flat surface. It is used to hold and connect electronic components such as chips and other components. The board is made of layers, typically two to ten, that interconnect components via copper pathways that are etched into the board. The board is typically two to ten layers in size, depending on the size of the component. The layers are connected by copper pathways etched into the board. The main printed circuit board in a computer is called a system board or motherboard. The smaller ones that plug into the slots in the main board are called expansion boards. The main board is called the \"motherboard\" and the smaller ones are the \"expansion boards\" The most prominent elements on the motherboard are the chips. The chips are the most important part of the motherboard. The most prominent parts of the chip are the memory and memory chips. They are also known as the motherboards or motherboards. The motherboards are the main components of the computer system. A chip is a single piece of semiconducting material, typically silicon, upon which electronic circuits and logic gates are fabricated. A chip is used to make electronic devices, such as computers and mobile phones, as well as mobile phones and other mobile devices. The resulting product is referred to as an integrated circuit. It can be used to make a variety of different types of electronic devices. For more information on how to make an integrated circuit, visit\\xa0http://www.electroniccircuit.com/integrated-circuits. The motherboard contains a slot or socket for the processor chip, which contains multiple individual cores, in what is known as a multicore processor. The processor chip is called a core or core chip. The motherboard contains memory chips and I/O chips. The processors are called memory chips or memory chips. There are slots for memory chips, I-O controller chips, and other key computer components. There are also slots for keyboards, mice, keyboards, and gamepads. There is also a slot for keyboards and mice, as well as a place for the keyboard and mouse. For desktop computers, expansion slots enable the inclusion of more components on expansion boards. For laptop expansion boards, expansion boards can be used to add more components to the computer. For mobile expansion boards the expansion board slot can be added to a mobile phone or tablet. For laptops, expansion board slots can be fitted to a desktop computer. A modern motherboard connects only a few individual chip components, with each chip containing from a few thousand up to hundreds of millions of transistors. Each chip contains up to a million transistors, and each chip can be as small as a few hundred thousand of them. Figure\\xa01.2 shows a processor chip that contains eight cores and an L3 cache. The chip is used in the Apple iPhone 5S and 5S Plus. The processor chip is also used by the Samsung Galaxy S5 and S5 Plus. It has a total of eight cores, eight threads and a L3 Cache. Not shown is the logic required to control operations between the cores and the cache. Not shown are the logic needed to control the external circuitry on the motherboard. This includes the logic between the core and the external\\xa0circumstances\\xa0on the motherboard, and between the\\xa0cores\\xa0and the\\xa0external\\xa0circuitry. The figure indicates that the L3 cache occupies two distinct portions of the chip surface.  The L3 Cache occupies two separate areas on the chip. The cache is located on the left and the right of the main chip. It is located in a region known as the L2 cache. It occupies the space between the L1 and L2 caches. All cores have access to the entire L3 cache via the aforemen-tioned control circuits. However, typically, all cores have Access to the L3 Cache via the Control Circuits (CCs) and not the L2 Cache (L2 Cache) The L2 cache is used to cache data for the next generation of processors. The processor chip shown in Figure\\xa01.2 does not represent any specific product, but provides a general idea of how such chips are laid out. The processor chip in Figure 1.2 doesn\\'t represent any specific product. It is just an example of how a processor chip is laid out in a computer. Next, we zoom in on the structure of a single core, which occupies a portion of the processor chip. Next, we look at the design of a second core that sits in the middle of the chip. And finally, we take a look at a third core that occupies a small portion of the processor chip, which is the core that powers the entire chip. In general terms, the functional elements of a core are: imperative logic, instruction logic, and memory locations. This includes the tasks involved in fetching instructions, decoding each instruction to determine the instruction operation and the memory locations of any operands. The core is also called a \"core\" because it contains all the elements needed to run a computer program. Arithmetic and logic unit (ALU): Performs the operation specified by an instruction. Alu: Performs an operation specified in an instruction. AlU: Performed an operation in accordance with a given instruction. A logic unit is a unit of arithmetic and logic. Load/store logic: Manages the transfer of data to and from main memory via cache. Data is stored in a single location in the main memory. Data can then be transferred from one location to another using the same logic. The data can also be stored in the same place in both main memory and cache. The core also contains an L1 cache, split between an instruction cache and a data cache. The instruction cache is used for the transfer of instructions to and from main memory, and the data cache for transfer of operands and results. The L1 data cache is split between the instruction and data caches. Typically, today’s pro-cessor chips also include an L2 cache as part of the core. L2 is a cache that is used to cache data for the processor\\'s memory. It is also used to store data in the memory of the processor. The chip is part of a larger chip called a core. In many cases, this cache is also split between instruction and data caches, although a combined, single L2 cache is also used. This cache is used to store information about a computer\\'s state. It can also be used to cache information about the user\\'s movements. Keep in mind that this representation of the layout of the core is only intended to give a general idea of internal core structure. It is not intended to give a full picture of the internal core of the company. The layout is only meant to give an idea of how the core looks. In a given product, the functional elements may not be laid out as the three distinct elements shown in Figure\\xa01.2, especially if some or all of these functions are implemented as part of a micropro-grammed control unit. For example, the three elements in Figure 1.2 are: examples It will be instructive to look at some \\xadreal-\\u200breal-real-world examples that illustrate the hierarchical structure of computers. The examples are intended to show how computers work in a hierarchical fashion. For more information, or to download a copy of the paper, visit: http://www.cnn.com/2013/01/29/technology/top-10-computer-hierarchies-in-the-world/story.html?storylink=cpy. Figure\\xa01.3 is a photograph of the motherboard for a computer built around two Intel \\xadQuad-\\u200b \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xadCore Xeon processor chips. Figure 1.3 shows the motherboard for an Intel \\xad \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0Quad\\xa0Core\\xa0Vexil processor. Many of the elements labeled on the photograph are discussed subsequently  in this book. Many of these elements are discussed later in the book. The book is published by Simon & Schuster, a division of Simon and Schuster Inc. It is published in hardback and is available in paperback. Here, we mention the most important, in addition to the processor  grotesquesockets. Section\\xa03.6 describes PCIe. PCIe slots can be used for a high-end display adapter and for additional peripher-als (Section\\xa03\\xa06 describes\\xa0PCI). Ethernet controller and Ethernet ports for network connections. Ethernet ports are used to make network connections between computers, phones, and other devices. The controller can also be used to connect to the Internet via the Internet. The Ethernet ports can be used for making network connections to other computers and devices. 1.2 / Structure and Function\\u2002 \\u20099.2x Quad-Core Intel® Xeon® Processors with Integrated Memory Controllers.1.3x Six Channel DDR3-1333 MemoryInterfaces Up to 48GB. 1.4x Serial ATA/300 (SATA) 2x USB 2.0Internal2x Ethernet Ports10/100/1000Base-T. 2x Serial ATA (SATA) sockets for connection to disk memory (Section\\xa07.7) 2x VGA Video Output2x Power & Backplane I/O. Interfaces for DDR (double data rate) main memory chips (Section 5.3) discussed in section 5.2 of the paper. Section 5.4 of the book discussed the use of memory chips with double data rate (DDR) chips. Section 6.3 of the report discussed the uses of DDR chips with single data rate chips. Intel 3420 is an I-O controller for direct memory access operations between peripheral devices and main memory (Section\\xa07.5 discusses DDR). It is based on the\\xa0Intel\\xa03420 processor family. It is used in the Intel\\xa0Kaby Lake processors and the Intel Kaby Lake Core processors. Following our \\xadtop-\\u200bdown strategy, as illustrated in Figures 1.1 and 1.2, we can now zoom in and look at the internal structure of a processor chip. We can see that the chip is made up of a number of different pieces, which are connected by a series of wires. For variety, we look at an IBM chip instead of the Intel processor chip. The IBM chip is the same as the Intel chip, but with a different name. The chip is called an IBM processor chip and was developed by IBM in the 1950s. The Intel chip was created in the 1960s by Intel. Figure\\xa01.4 is a photograph of the processor chip for the IBM zEnterprise EC12 mainframe computer. The processor chip is used to power the mainframe\\'s main processor. The chip is also used to control the main processor of the IBM\\xa0ZEnterprise\\xa0EC12 computer. This chip has 2.75\\xa0billion transistors. It is the largest chip in the world, with more than 100\\xa0billion\\xa0transistors. This chip is used in the iPhone 5S, 5C, 5S Plus, and 5C mini-chips. It was developed by Texas Tech University. The superimposed labels indicate how the silicon real estate of the chip is allocated. The labels are also used to show how the chips are connected to the rest of the system. The chips are then connected to other parts of the computer using a series of chips called ‘connectors’. We see that this chip has six cores, or processors. The chip has a total of six processors, or cores. It has six different types of processors, including a core-based processor. It also has a chip that can run multiple processors at the same time. In addition, there are two large areas labeled L3 cache, which are shared by all six processors. There are also two large caches labeled L2 cache and L3 Cache, shared by four of the six processors. There is also a large cache area labeled L1 cache, and two large cache areas labelled L2 and L4 cache. The L3 control logic controls traffic between the L3 cache and the cores. It also controls the traffic between L3 and the external environment. L3 is used in the Apple iOS 7 operating system. It is also used in Apple\\'s Mac OS X operating system and Apple\\'s iOS 8 software. There is stor-age control (SC) logic between the cores and the L3 cache. There is also a cache that is used to store data between cores and L3 caches. This is called SC-Storage Control (SC), which is used for data storage in the cores. The memory controller (MC) function controls access to memory external to the chip. The memory controller is used to control the chip\\'s\\xa0memory\\xa0on-chip. The MC function can also control memory on-chip, such as on a PC or Mac. The chip can be used to store data on a computer or phone. Going down one level deeper, we examine the internal structure of a single core, as shown in the photograph of Figure\\xa01.5. The GX I/O bus controls the interface to the channel adapters \\xadaccessing the I-O. The\\xa0GX\\xa0I/O\\xa0bus\\xa0controls\\xa0the\\xa0interfaces\\xa0between the\\xa0channel adapters and the\\xa0I-O\\xa0buses. Keep in mind that this is a portion of the silicon surface area making up a \\xadsingle-\\u200b \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xadprocessor chip. It is a small fraction of the surface area that makes up a chip\\'s entire surface area. It\\'s not the entire chip, but a small part of it. The core area is the instruction sequence unit (ISU) The ISU is a sequence of instructions. The main areas of instruction are the following: the ISU, ISU and ISU-related areas. These areas are: the ISU, the IISU-based areas, and the IISC-based area. Determines the sequence in which instructions are executed in what is referred to as a superscalar architecture (Chapter\\xa016). Also known as the \"superscalar sequence\" or the \"Superscalary Sequence\" The sequence determines the sequence of instructions to be carried out. IFU (instruction fetch unit): Logic for fetching instructions. IFU is a unit that is used to retrieve instructions from a computer. It can also be used to send instructions to a mobile phone or tablet. IBU is used for sending instructions to mobile phones and tablets. 10\\u2002 \\u2002 Chapter 1 / Basic Concepts and Computer Evolution  \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0■■■IDU (instruction decode unit): \\xa0“Basic Concepts’” “Computer Evolution’s” Basic Concepts. ““” Computer Evolution\\'s’ “Instruction decoding unit” (IDU) “This is the instruction decode unit for this chapter.”  “What is this unit for?” The IDU is fed from the IFU buffers, and is responsible for the parsing and decoding of all z/Architecture operation codes. The IDU can also be used to send messages to other parts of the system. It can be used in a number of ways, such as to send a message to another part of the network. LSU is a unit of the U.S. military. It stands for the Lusitaries of the United States. The U.N. uses the term to refer to the military\\'s most senior personnel. It is the most senior unit in the military and has the highest status. The LSU contains the 96-kB L1 data cache,1 and man-ages data traffic between the L2 data cache and the functional execution units. The LSU also contains the L1 and L2 caches, as well as the L3 and L4 caches, and the L4 cache. It is responsible for handling all types of operand accesses of all lengths, modes, and formats as defined in the z/Architecture. It is also responsible for handle all of the operand formats defined by the z-architecture language. XU (translation unit):    XU ( translation unit) is the language unit used in this article. The English version of this article uses the XU translation unit. For more information on the translation unit, go to: http://www.globeandmail.com/globe-and-mail/2013/01/30/3030/language/xu.html#storylink=cpy. This article was translated from the German version of the same name. This unit translates logical addresses from instructions into physical addresses in main memory. It is based at the University of California, San Diego. The unit has a total capacity of 1,000,000. It has been in existence since the 1970s. It was first used in the 1980s and has since been expanded. The XU also contains a translation lookaside buffer (TLB) used to speed up memory access. The TLB is used to translate between different languages in the XU. The XU is also used to transfer data from one language to another. The TLB can be used for translation between languages. TLBs are discussed  in Chapter\\xa08 of the book. TLBs are described in terms of a fixed-point unit (FXU) and a fixed fixed point unit (FU) FU stands for fixed, fixed, adjustable, fixed and fixed. The FU is a fixed unit. The FXU executes \\xadfixed-\\u200bfixed-\\xadpoint arithmetic operations. The FXU is a computer program that executes arithmetic operations on a computer. It is based on the Fermi code, which is used in the UNIX computer programming language. It was created in the 1950s by the Soviet Union. BFU (binary \\xad774) is a floating-point unit. \"BFU\" means \"floating point\" or \"point of reference\" in English. It can be used to refer to any of a number of different things. For example, it can mean \"the point of view of a person\" or simply \"the state of the world\" The BFU handles all binary and hexadeci-mal operations. It also handles multiplication and floating point operations. The BFU was created in the 1960s. It is based on the C programming language. It was originally developed in the 1950s by the University of California, Berkeley. D Unit (decimal) is a unit of weight. D Unit is a number of decimal points. DU is the number of feet in a square meter. D unit is a fraction of a million feet in length. DUnit is a decimal number of one million feet. The DFU handles both \\xad \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0fixed-\\u200b˚repertoire-point and  \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0floating-\\u200bphthalmological-point operations on numbers that are stored as decimal digits. The DFU is based in London and has a staff of more than 1,000. RU (recovery unit): The RU keeps a copy of the complete state of the sys-tem that includes all registers, collects hardware fault signals, and manages the hardware recovery actions. The RU manages the recovery actions for the computer\\'s software and hardware. Figure\\xa01.4 is an IBM zEnterprise EC12 Processor Unit (PU) chip diagram. Figure 1.5 is a chip diagram of the zEnter Enterprise EC12 processor unit. Figure 2 is an image of the processor unit on a computer screen. Figure 3 is a picture of the chip on a laptop screen. IBM zEnterprise EC12 Core layout. Figure 1.5 is an example of the core layout.Figure 1.6 is the core design. Figure 2 is a view of the Core layout from the left. Figure 3 is the Core design from the right. Figure 4 is the layout of theCore from the top. 1.1kB = kilobyte = 2048 bytes. IBM, Reprinted by Permission to use this material. For more information, visit IBM.com/IBM-Reprints. For confidential support, call the Samaritans on 08457 90 90 90, visit a local Samaritans branch, or see www.samaritans.org. Numerical prefixes are explained in a document under the “Other Useful’ tab at ComputerScienceStudent.com. The document is titled “Numerational prefixes.” It is available in English, Spanish, French, and Arabic. It is also available in German, Italian, and Spanish. 1.3.3 / A Brief History of Computers’ 11th century computer. ‘COP’ stands for ‘dedicated’ or ‘proprietary’ computer. It was the first computer to use the word ‘processor’ as a programming language. The COP is responsible for data compression and encryption functions for each core. The COP is also responsible for the data compression and encryption functions of each core, and is called the \"COP\" It is based at the University of California, Los Angeles. It was founded in 1989 and has been in existence since. This is a 64-kB L1 instruction cache, allowing the IFU to prefetch instructions before they are needed. The IFU can also use this cache to cache data for later use. The cache can also be used to cache information for future use. L2 control: This is the control logic that manages the traffic through the two L2 caches. L2 control is the logic used to manage the traffic between the two caches. The two caches are located in the same place on the network. The L2 cache is located in a different location on the same network. This is where the traffic is routed. L2: L1: L3: L4: L5: L6: L7: L8: L9: L10: L11: L12: L13: L14: L15: L16: L17: L18: L19: L20: L21: L22: L23: L24: L25: L26: L27: L28: L29: L30: L31: L32: L33: L34: L35: L36: L37: L38: L39: L40: L41: L42: L43: L44: L45: L46: L47: L48: L49: L52: L55: L56: L57: L58: L59: L60: L61: L62: L63: L64: L65: L66: L67: L68: L69: A 1-MB L2 data cache for all memory traffic other than instructions. A 1- MB L2 cache for any data that is not used for instructions. An L2 Cache for all data that isn\\'t used for instruction traffic. A L2Cache for data that\\'s not needed for instruction-based data. L2: L1: L2: l1. L3: L4: L5: L6: L7: L8: L9: L10: L11: L12: L13: L14: L15: L16: L17: L18: L19: L20: L21: L22: L23: L24: L25: L26: L27: L28: L29: L30: L31: L32: L33: L34: L35: L36: L37: L38: L39: L40: L41: L42: L43: L44: L45: L46: L47: L48: L49: L52: L55: L56: L57: L58: L59: L60: L61: L62: L63: L64: L65: L66: L67: A 1-MB L2 instruction cache. A 1- MB L2 memory cache. An 8-GB hard drive. An 18-inch hard drive with an 8-inch screen. An 11-inch Hard Drive with an 18- inch screen. A 12-inch HD display with a 16-inch display. An 16-hour battery life. As we progress through the book, the concepts introduced in this section will become clearer. The concepts introduced will become clearer as we move through the rest of the book. We hope that this guide will help you understand some of the concepts we have been introduced to so far. In this section, we provide a brief overview of the history of the development of computers.1.3 A Brief History of Computers2. The History of Computer Technology: A Concise History of the Computer Industry, published by Oxford University Press, is available online for free. This history is interesting in itself, but more importantly, provides a basic introduction to many important concepts that we deal with throughout the book. This history provides an important introduction to many of the concepts that are dealt with in this book. It is also a good starting point for understanding some of the key concepts in the book, such as the concept of \\'theory of mind\\' The first generation of computers used vacuum tubes for digital logic elements and memory. Vacuum tubes were used in the computer\\'s memory, logic, and data storage. The first computers were built in the 1950s and 1960s. They were the first computers to use vacuum tubes. A number of research and then commercial computers were built using vacuum tubes. The tubes were used in the development of the first computers in the 1950s and 1960s. They are still used in some of the world\\'s most successful computers today. They can also be used to make computer parts such as keyboards and mice. For our purposes, it will be instructive to examine perhaps the most \\xa0famous \\xadfirst-\\u200b \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xadgeneration computer, known as the IAS computer. The IAS was the world\\'s first computer, and was developed in the 1950s and 1960s. It was the first computer to have a colour screen and a colour display. A fundamental design approach first implemented in the IAS computer is  known as the \\xadstored-\\u200b \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xadprogram concept. It is based on the idea of storing information in a database. The concept was first used to create the first computer programs in the 1970s and 1980s. This idea is usually attributed to the mathem-atician John von Neumann. It is also credited to the mathematician and inventor of the first computer, Karl-Heinz Schrödinger. The first computer was developed in the 1950s in the German city of Berlin. Alan Turing developed the idea at about the same time. He was working on a prototype of the machine at the time. It is thought to be the world\\'s first computer programme to be written in code. It was developed in the 1950s and 1960s by Alan Turing and others. The first publication of the idea was in a 1945 proposal by von Neumann for a new computer, the EDVAC (Electronic Discrete Variable Computer) The IAS computer was designed at the Princeton Institute for Advanced Studies in 1946. It was the first computer of its kind. The IAS computer, although not completed until 1952, is the prototype of all subsequent \\xad \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0general-\\u200b \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xadpurpose computers. Figure\\xa01.6 shows the structure of the Ias computer (compare with Figure 1.1). The IAS was the first of a series of computers to be named after a single person. This book’s Companion Web site (WilliamStallings.com/ComputerOrganization) contains several links  to sites that provide photographs of many of the devices and components discussed in this section. It consists of a main memory, which stores both data and instructions. An arithmetic and logic unit (ALU) capable of operating on binary data. 4A 1954 report [GOLD54] describes the implemented IAS machine and lists the final instruction set. 4.4 was the first machine to use the IAS\\xa0programming\\xa0language. It was developed in the 1950s and used in the Korean War. It is the only IAS-based machine to have ever been built. It is available at box.com/COA10e. For more information on the film, visit www.cnn.com. It is available on DVD and Blu-Ray. The film is released on September 14. For further information, visit box.co.uk/coa10e and for more information about the film click here. The 1945 report on EDVAC is available at box.com/COA10e.3. The report is available to download for free on the Internet. For confidential support call the Samaritans on 08457 90 90 90 or visit a local Samaritans branch, see www.samaritans.org for details. The term instruction refers to a machine instruction that is directly interpreted and executed by the processor. This is in contrast to a statement in a high-level language, such as Ada or C++, which must first be compiled into a series of machine instructions before being executed. The structure was outlined in von Neumann’s earlier proposal, which is worth quoting in part. These are addition, subtraction, multiplication, and divi-sion. They are also known as addition-subtraction-multiplication-division. These are also called addition- subtraction-multiplying-divi-Sion. For more information, see addition and subtraction. It is therefore reasonable that it should contain specialized organs for just these operations. It should also contain specialized software to help with other types of surgery. It will also have specialised software to assist with other kinds of surgery, such as heart surgery, for example. And it should have special software to allow it to detect and respond to emergencies. Control encompassescircuits and addresses. AC: Accumulator registerMQ: multiply-quotient register; MBR: memory buffer register;IBR: instruction buffer register. PC: program counter; MAR: memory address register;IR: insruction register. It must be observed, however, that while this principle as such  is probably sound, the specific way in which it is realized requires close scrutiny. Figure\\xa01.6 is an example of an IAS structure. The IAS Structure is based on the IAS\\xa0Program control unit (CC) At any rate a central arithmetical part of the device will probably have to exist, and this constitutes the first specific part:\\xa0CA.CA. CA. CA, CA.CA, CA,\\xa0CA,\\xa0C,\\xa0A,\\xa0B,\\xa0D,\\xa0E,\\xa0F,\\xa0G,\\xa0H,\\xa0I,\\xa0J,\\xa0K,\\xa0L,\\xa0M,\\xa0N,\\xa0O,\\xa0P,\\xa0Q,\\xa0R,\\xa0S,\\xa0T,\\xa0U,\\xa0V,\\xa0W,\\xa0Z,\\xa0X,\\xa0Y, Y, Z, U, Z. 2.3\\xa0 Second: The logical control of the device, that is, the proper sequencing of its operations, can be most efficiently car-ried out by a central control organ.2.4\\xa0 Third: The control of a device can be best achieved by a single person. If the device is to be elastic, that is, as nearly as possible all purpose, then a distinction must be made between the specific instructions given for and defining a particular problem, and the general control organs that see to it that these instructions are carried out. No matter what the instructions are, they must be carried out no matter what they are. The former must be stored in some way; the latter are represented by definite operating parts of the device. For example, a phone can be used to send and receive text messages. A computer can also be used as a computer to send or receive messages. For more information, see the Wikipedia page for the topic. By the central control we mean this latter function only, and the organs that perform it form  the second specific part: CC.CC.CC is the part of the brain that controls the heart and lungs. CC. is also the part that controls a person\\'s breathing, heartbeat, and respiration. 2.4 Third: Any device that is to carry out long and compli-cated sequences of operations (specifically of calculations) must have a considerable memory. 2.4.4 Fourth: Devices that are to carry on long and Compli-Called operations must have a considerablememory. The instructions which govern a complicated problem may constitute considerable material. This is particularly so if the code is cir-cumstantial (which it is in most arrangements) In most arrangements, the instructions are written in code, rather than in code. The instructions are then translated into code, which is then translated back into code. This material must  be remembered. This material must be remembered by all of us. We must remember that this material was created for a purpose. The purpose of this material is to make a difference in the lives of those who read it. To read the rest of this article click here. At any rate, the total memory constitutes the third specific part of the device:\\xa0M.2.6. The three specific parts CA, CC (together C), and M cor-respond to the associative neurons in the human nervous system. The brain\\'s hippocampus is the most important part of the brain. It remains to discuss the equivalents of the sensory or afferent and the motor or efferent neurons. It is hoped that this will lead to a more complete understanding of the brain\\'s motor neurons. For more information, visit: http://www.cnn.com/2013/01/29/science/science-nervous-nausea-and-breathalysers/index.html. These are the input and output organs of the device. The device can be used to send and receive data from a variety of sources. It can also be used as a way to send data back to the device\\'s\\xa0receiver\\xa0or\\xa0receive\\xa0it from the device. The device must be endowed with the ability to maintain input and output (sensory and motor) contact with some specific medium of this type. The device must also be able to maintain contact between the device and the medium it is in contact with. For more information on this type of device click here. The medium will be called the outside record- \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0ing medium of the device:\\xa0R. The device must have organs to transfer informa- \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0tion from R into its specific parts C and M. These organs form its input, the fourth specific part:\\xa0I. It will be seen that it is best to make all transfers from R (by I) into M and never directly from\\xa0C. The device must have organs to transfer from its specific parts C and M into\\xa0R. These organs form its output, the fifth specific part:\\xa0O. It will be seen that it is again best to make all transfers from M (by O) into R, and never directly from\\xa0C. With rare exceptions, all of today’s computers have this same general structure and function and are thus referred to as von Neumann machines. Thus, it is worth-while at this point to describe briefly the operation of the IAS computer. The IAS computers are operated by a team of four people. The team operates the computer by using a series of commands. These commands are carried out by a computer known as the \"IAS computer\" [BURK46, GOLD54]..[BURk46,\\xa0GOLD 54].. [BURK47,  Gold54]. [BURk48,  GOLD54].\\xa0[GOLD51, BURK52,  GOLF54]. Following [HAYE98], the terminology and notation of von Neumann’s A Brief History of Computers is changed to conform more closely to modern usage. The exam-ples accompanying this discussion are based on that latter text. For more information on the history of computers, see the Wikipedia page. The memory of the IAS consists of 4,096 storage locations, called words, of 40\\xa0binary digits (bits) each. Both data and instructions are stored there. The IAS has a total of 4.6\\xa0billion words in its memory. Numbers are represented in binary form, and each instruction is a binary code. Numbers are  represented in Binary form, with each instruction being a Binary code. Each instruction is\\xa0represented\\xa0in binary form and each code is a Binary Code. The instruction set is as follows: Figure\\xa01.7 illustrates these formats. Figure 1.7 shows how different formats are used for different types of photos. Figure\\xa01\\xa01 \\xa07 illustrates the different formats used for the different photos.Figure 1.8 shows the different ways the photos are displayed in different formats. Each number is represented by a sign bit and a 39-bit value. Each number is also known as a \"sign bit\" and a \"39-bit\" value. For more information, visit www.numbers.org.uk and www.cnn.com/news/2013/01/28/17/numbers-in-pictures. A word may alternatively contain two 20-bit instructions, with each instruction consisting of an 8-bit operation code (opcode) specifying the operation to be performed. A word may also contain a 12-bit address designating one of the words in memory (numbered from 0 to 999) The control unit operates the IAS by fetching instructions from memory and executing them one at a time. The IAS can also be used to carry out other functions such as controlling a car\\'s engine. The control unit can also carry out tasks such as driving a car or driving a truck. We explain these operations with reference to \\xadFigure\\xa01.6. Figure 1.6 is an example of an operation that is performed on a computer. The operation is carried out by a computer with a computer chip inside. The chip is then used to perform a series of operations on the chip. This figure reveals that both the control unit and the ALU contain stor-age locations, called registers. The registers are defined as follows:■Memory buffer register (MBR): Contains a word to be stored in memory or sent to the I/O unit, or is used to receive a word from memory. Memory address register (MAR): Specifies the address in memory of the word to be written from or read into the\\xa0MBR. It is used to store the address of a word that is to be written or read from or written into the MBR. For more information on the memory address register, visit\\xa0www.memoryaddressregister.org. Instruction register (IR):  Contains the 8-bit opcode instruction being executed. The instruction register is used to indicate the type of code being used. For more information on the IR, visit the instruction register website. For the rest of the story, go to CNN.com/Heroes. Instruction buffer register (IBR): Employed to hold temporarily the \\xad \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0right-\\u200b \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xad \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0hand instruction from a word in memory. The IBR is used to hold temporary instructions in a word\\'s memory. For more information on the IBR, visit: www.ibr.org. Program counter (PC): Contains the address of the next instruction pair to be fetched from memory. Program counter is used to indicate that an instruction is about to be carried out. The program counter can also be used to show the next program to be executed. The PC counter can be used as a program counter to indicate the nextprogram to be used. Accumulator (AC) and multiplier quotient (MQ) are used in the equation. The equation is based on the ratio of the additive and multiplicative coefficients. The formula is written in terms of the ratio: 1:1, 1:2, 1/2, 2/2. Employed to hold tem-porarily operands and results of ALU operations. Held to hold information about ALU\\'s operations and operations. Holds information about operations, results, and operations of the company. Held information about the company\\'s operations, operations, and finances. There is no universal definition of the term word.For example, the result 6 is a word that means \"to say\" or \"to refer to\" A word can be used to refer to a person or a group of people. For example, a word can refer to people who have been involved in a crime. In general, a word is an ordered set of bytes or bits that is the normal unit in which information may be stored, transmitted, or operated on within a given computer. A word can be used to describe any piece of information that is stored on a computer. Typically, if a processor has a \\xadfixed-\\u200b˚fixed\\xa0length instruction set, then the instruction length equals the word length. The word length is the length of the instruction set. For example, a processor could have an instruction set that has an instruction length that equals a word. (a) Number word \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0sign bit 20 (8 bits) \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0opcode (8bits) \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0address (12bits)\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0left instruction (20bits)\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0right instruction ( 20 bits)\\xa0\\xa0\\xa0\\xa0\\xa0instructions (20 bits)\\xa0\\xa0\\xa0\\xa0number\\xa0word 20 bits. The most significant 40-bits are stored in the AC and the least significant in the MQ. The result of multiplying two 40-bit numbers is an 80-bit number. Figure\\xa01.7\\u2003 IAS Memory Formats for 40-bits. The IAS operates by repetitively performing an instruction cycle, as shown in Figure\\xa01.8. The IAS can also be used to carry out other tasks, such as performing a series of jumps. The most common task is to perform a sequence of jumps, followed by another sequence of hops. Each instruction cycle consists of two subcycles. Each subcycle is made up of two parts: the first part is the instruction cycle and the second is the subcycle. The instruction cycle is broken up into two parts, the first and second parts are the instruction cycles and the third is the second. During the fetch cycle, \\xa0the opcode of the next instruction is loaded into the IR and the address portion is \\xa0loaded into the\\xa0MAR. The address portion of the opcode is then loaded into  the IR. The opcode and address are then combined to create a single instruction. This instruction may be taken from the IBR, or it can be obtained from memory by loading a word into the MBR, and then down to the I BR, IR, and\\xa0MAR. It can be used to teach students how to read and write. Why the indirection? Why not just go straight to the front door? Why the back door? I don\\'t know. Why not go to the other side of the street? I\\'m not sure why. I\\'m just looking for a way to get through the door. These operations are controlled by electronic circuitry and result in the use of data paths. These operations can be used to send and receive data. For more information on how to use data paths in your computer, visit www.cnn.com/2013/01/29/technology/how-to-use-data-paths-in-your-computer. To simplify the electronics, there is only one reg-ister that is used to specify the address in memory for a read or write. Only one register is used for the source or destination. There is also only one  register used for a write or read to and from memory. 1.1.3 / A Brief History of Computers. 1.2.1 / A brief history of computers. 2.3.2 / Computers in general. 3.4.1  A Brief history of computer technology. 4.5.3  A short history of computing. 5.6.1 The history of the computer industry. M(X) is an acronym for \"mixed race\" or \"multicultural\" M(M) is a word for multiple races or ethnicities. \"M\" is the abbreviation for \"Mixed Race or Multicultural\". \"X\" is a term used to refer to any race or ethnic group. = contents of memory location whose address is X; X(i:j) = bits i through j. Decode instruction in IRAC: Go to M(X, 0:19) Go to memory location X:j; X:i: j:i; X X:x:j:x. Once the opcode is in the IR, the execute cycle is performed. If AC > 0 then go to M(X, 0:19) If AC is not 0, go to AC + MBR. Figure 1.8 is a partial flowchart of IAS Operation. Control circuitry \\xa0interprets the opcode and executes the instruction by sending out the appropri-ate control signals to cause data to be moved or an operation to be performed by \\xa0the ALU. The ALU sends out the signals to the control circuitry to cause a data move or operation to take place. The IAS computer had a total of 21 instructions, which are listed in Table\\xa01.1.  The IAS had a\\xa0total\\xa0of 21 instructions. The computer was designed to run on the British Army\\'s\\xa0IAS\\xa0computer. It was designed for use in World War II, and was based on the IAS-1. Data transfer: Move data between memory and ALU registers or between two  \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0ALU registers. These can be grouped as follows: \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0■ \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0“Data transfer” is a way to transfer data between two registers. “Data Transfer” can also be used to move data from one register to another. Unconditional branch: Normally, the control unit executes instructions in sequence from memory. Unconditional\\xa0branch\\xa0is a special branch that can only be used in certain circumstances. The control unit can only execute instructions in a sequence from memory if it is in a certain state. This sequence can be changed by a branch instruc-tion, which facilitates repetitive operations. The sequence can also be changed with the use of a \\'branches\\' command. This command can be used to change the order in which a sequence of operations is carried out. Table\\xa01.1: The IAS Instruction Set for the IAS. Table 1.1 is a list of all the instructions used in the instruction set. The instruction set is written in the C programming language. The code is written as follows: AC00001010LOAD MQ,M(X) MQ. AC00001001LOAD M(X),MQ,X,MQ. Transfer contents of memory location X to MQ00100001STOR M(X) X. X is the location of the memory location in the MQ. MQ is the name of the file system in which the contents of X are stored. X can be any location in a file system. Transfer contents of accumulator to memory location X. Transfer M(X) to the accumulator. Transfer contents of memory location M to accumulator, then M to the memory location. transfer contents of the memory to M, then transfer M to M. Transfer the contents of M to X, then X, and then M. Transfer –M(X) to the accumulator. Transfer absolute value of M(X), M(M) to accumulator, M(m) to M(x) Transfer absolute number of M, M, X to M (x) transfer –M, X, M to M. Transfer –M to M; M to X; X to X (m) If number in the accumulator is nonnegative, take next instruction from left half of M(X) Take next instruction if number in accumulator isn\\'t negative. If number is negative, go to the next section of the instruction. Take next step if number is not negative, then take the next instruction. If number in the accumulator is nonnegative, take next instruction from right half of M(X) If number is negative, take the next instruction. If number in accumulator isn\\'t negative, then take the previous instruction. For example, if number is\\xa0negative, then number\\xa0in\\xa0accumulator\\xa0is\\xa0negative. Arithmetic \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa000000101ADD M(X) + M(Y) = M(V) + X (M(V), X (V), Y (V, Y), Z (Z, Z), M(Z), Z(Z) = X (X, Y, Z, Z) Add M(X) to AC; put the result in AC. Put most significant bits of result  in AC, put least significant bits in MQ. Put the remainder in AC00001011 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0MUL M( X) \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0Multiply M(x) by MQ; put most significant bit of result in\\xa0MQ00001100 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0DIV M (X) \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 Multiply\\xa0M(X),\\xa0DIV,\\xa0DIV. Divide AC by M(X); put the quotient in MQ and the  \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0remainder in AC.Multiply accumulator by 2; that is, shift left one bit position position position 1 position position 2 position 3 position 4 position 5 position 6 position 7 position. Replace left address field at M(X) by 12 rightmost bits  of AC. AC.00010011 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0STOR M( X,28:39) \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 AC.0000010101\\xa0AC.0000000101 AC.0000101001 AC.0010101101 AC.010109 AC.000109AC.0011AC.0001 AC.0211 AC.0030109.00AC.010909.0002 AC.03010908.0003.0004 AC.040109090.0005 AC.050109088.0006 AC.06110909090, AC.071109090,.0005AC.0811090,AC.0911090.00,AC,0001,0002,0003,000,0004,0005,0006,0007,0008,0009,00010,00011,000 Replace right address field at M(X) by 12 rightmost bits of AC. Conditional branch: The branch can be made dependent on a condition, thus  allowing decision points. For example, a conditional branch could look like the following: AC: A conditional branch looks like this:AC: A Conditional Branch looks like the below: Arithmetic: Operations performed by the\\xa0ALU. Statistics: Operations carried out by the ALU. The ALU is a unit of computing power. It can perform arithmetic, statistics, and other operations. It is the largest computing unit in the world. It has a capacity of more than 1,000,000. Address modify: Permits addresses to be computed in the ALU and then inserted into instructions stored in memory. The ALU is used to store addresses in memory and to modify them. The address modify function can be used to modify memory addresses in a variety of ways. This allows a program consider-able addressing flexibility. This allows the program to be more flexible in its approach to addressing issues. The program can be used to address issues in a variety of ways. For more information on the program, visit: http://www.cnn.com/2013/01/29/technology/top-10-most-popular-programs-of-the-year/story.html?story=true. Table\\xa01.1 presents instructions (excluding I/O instructions) in a symbolic, ˚Ś˚˙˚\\xa0˚‚\\u2009’’, ‘˚°’\\xa0‘’Ś’˙\\u2009, “˙°”, ’”˚%’%, ‘\\u2009˙%”’;. ‘œ˚&’:\\xa0“˚#”;\\xa0’#’+’%3D’%;\\xa0\\u2009#‘%3d’%:\\xa0‚&%2D”: “’&\\u2009%2E’,. ‘#‚%2F’%. ’#: ‘%2C’): ‘+”%3E”,. � Each instruction must conform to the format of Figure\\xa01.7b. In binary form, each instruction must be in the form of a dollar sign. In decimal form, the instruction is in the shape of a zig-zag line. In hexadecimal form, they are in theform of a uppercase letter. The opcode portion (first 8 bits) specifies which of the 21 instructions is to be executed. The opcode is the first 8 bits of a computer\\'s instruction set. It is used to determine the order in which an instruction is to be carried out. For more information on the opcode, visit opcode.org. The address portion (remaining 12 bits) specifies which of the 4,096 memory locations is to be involved in the execution of the instruction. The address portion is the same as the instruction\\'s instruction body. The instruction body is the body of the program\\'s instruction. Figure\\xa01.8 shows several examples of instruction execution by the control unit. The control unit can be used to control a variety of different types of computers. It can also be used as a control unit for a computer that can control a computer with a control system. For more information on the control system, visit: http://www.cnn.com/2013/01/28/technology/control-systems/index.html. Note that each operation requires several steps, some of which are quite elaborate. Some of the steps can take several hours or more to complete. For more information on how to play the game, visit the official website. The game is available in English, Spanish, and French. The multiplication operation requires 39 suboperations, one for each bit position except that of the sign bit. The multiplication operation\\xa0 requires 39\\xa0suboperations\\xa0one for each\\xa0bit position\\xa0except\\xa0that\\xa0of the sign\\xa0bit. The\\xa0 multiplication\\xa0operation\\xa0requires\\xa039 suboperation\\xa0one\\xa0for each bit\\xa0position. The first major change in the electronic computer came with the replacement of the vacuum tube by the transistor. The Second Generation: Transistors. The Third and Fourth Generation: Topsons. The Fifth and Sixth Generation: Disposers. The Seventh and Eighth Generation: Diodes. The Ninth and Tenth Generation: Trons. The transistor can be used in the same way as a vacuum tube to construct computers. The transistor is smaller, cheaper, and gener- \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0ates less heat than a vacuum tubes. It can also be used as a source of power for a variety of electronic devices such as calculators. Unlike the vacuum tube, which requires wires, metal plates, a glass capsule, and a vacuum, the transistor is a \\xadsolid-\\u200b\\xad\\xadstate device, made from silicon. The\\xa0transistor\\xa0is a solid-\\u200bstate device that can be used to make electronic devices. The transistor was invented at Bell Labs in 1947 and by the 1950s had launched an electronic revolution. The first transistor was made in 1947. It was the first electronic device to have a built-in microchip. The\\xa0transistor\\xa0was\\xa0invented\\xa0at Bell Labs. It was not until the late 1950s, however, that fully transis-torized computers were commercially available. The first commercially available computer was a Sinclair ZX-1 in the 1950s. It was the first computer to have a built-in integrated circuit. It had a speed of 1,000 words per minute. The use of the transistor defines the second generation of computers. The transistor was used in the development of the first computer in the 1950s. The first computer was based on a modified version of the Apple II. It was developed in the late 1950s and early 1960s in the United States. It has become widely accepted to classify com-puters into generations based on the fundamental hardware technology employed.(Table\\xa01.2) It is also widely accepted that com- computers can be categorised into different generations depending on the type of computer they are built on. Each new generation is characterized by greater processing perfor-mance, larger memory capacity, and smaller size than the previous one. Each new generation also has a smaller size and more processing power than its predecessor. Each generation is also characterized by a smaller screen size and greater processing power. But there are other changes as well. There will be more changes to come in the next few months. There are also plans to make changes to the way the U.S. military is funded in the future. There is also a plan to change the way that the military will be funded. The second generation saw the use of more complex arithmetic and logic units and control units. The use of high-level programming languages and the provision of system software with the \\xa0A Brief History of Computers was also introduced. The first computer was built in 1946 and was called a ‘Vacuum tube’. In broad terms, system software provided the ability to load programs, move data to peripherals, and libraries to perform common computations. This is similar to what modern operating systems, such as Windows and Linux, do. For more information, see System Software. It will be useful to examine an important member of the second generation: the IBM 7094. It will also be helpful to examine how it was designed. It was designed to compete with the likes of Hewlett-Packard and Hewlett Packard Enterprise. [BELL71].[ BELL71] The first half of this article was published on November 1, 2011. It is the first of a two-part series on the subject of the \"World of the World\" series. The second half of the series will be published on December 1, 2012. It will be called \"The World Of The World\" From the introduction of the 700 series in 1952 to the introduc-tion of the last member of the 7000 series in 1964. This IBM product line underwent an evolution that is typical of computer products. The IBM 7000 series was introduced in 1964 and was the last in the series. Successive members of the product line showed increased performance, increased capacity, and/or lower cost. The product line is designed to be used in a variety of ways. It can also be used as a source of revenue for a number of other products. For more information on the product line, visit\\xa0the\\xa0Product\\xa0Line website. The size of main memory, in multiples of 210 36-bit words, grew from \\xa02k (1k = 210) to 32k words,7. The time to access one word of memory, the mem-ory cycle time, fell from 30 ms to 1.4 ms. The number of opcodes grew from a modest 24 to 185. The most popular opcodes were 24, followed by 24 and then 24 again. The number of users was also up by more than 10 per cent, from 1.2 million to 1.3 million. Also, over the lifetime of this series of computers, the relative speed of the CPU increased by a factor of 50. Also, the\\xa0relative\\xa0speed of the CPU increased by  50 times over the life of the computer. This is due to the fact that the CPU was designed to be faster than the main processor. Speed improvements are achieved by improved electronics (e.g., a transistor implementation is faster than a vacuum tube imple-mentation) and more complex circuitry. For more information on how to improve your speed, visit\\xa0http://www.cnn.com/2013/01/30/technology/speed-improvement/top-10.html. IBM 7094 includes an Instruction Backup Register, used to buffer the next instruction. For example, the IBM 7093 includes an\\xa0Instruction\\xa0 Backup Register. For more information, visit IBM.com/IBM7094 and the IBM\\xa07093\\xa0backup\\xa0register. The control unit fetches two adjacent words from memory for an instruction fetch. The control unit fetches the words  from memory by using the word \\'fetch\\' for \\'to fetch\\' The command is then translated into English by the control unit. The instruction fetch is then carried out by the unit\\'s control system. Except for the occurrence of a branching instruction, which is relatively infrequent (perhaps 10 to 15%), this means that the control unit has to access memory for an instruction on only half the instruction cycles. The most common type of instruction is the \"A\" instruction, followed by the \"B\" instruction. This prefetching significantly reduces the average instruction cycle time. For more information on how to use this technology, visit:\\xa0http://www.cnn.com/2013/01/29/tech/prefetching/index.html. For other ways to use the technology, see: http://www\\xa0cnn\\xa0com/ 2013/02/30/technology/preferring-to-prepare-for-a-new-build.html\\xa0and\\xa0http:\\xa0www.nspawn.org/2013-02-29/technology-prefering.html/. Figure\\xa01.9 shows a large (many peripherals) configuration for an IBM 7094. The configuration is representative of \\xadsecond-\\u200b\\xadgeneration computers. Figure 1.9 is based on an IBM\\xa07094 running on an Intel Core 2 Duo processor. Several differences from the IAS computer are worth noting. The IAS is a computer with a built-in GPS system. It can be used to track the movements of people and objects in a large area. It is also used to calculate the speed of a person or vehicle. The most important of these is the use of data channels. For more information on how to use data channels, visit the Data Channels website. For further information, visit\\xa0the\\xa0Data\\xa0Channels\\xa0 website. for more information about the data\\xa0channels, see\\xa0www.data\\xa0channel.org. A data channel is an independent I-O module with its own processor and instruction set. A data channel can be used to communicate with other data channels. Data channels can also be used as a way to display data to other channels. For more information on data channels, see the Data Channel Wiki. In a computer system with such devices, the CPU does not execute detailed I/O instructions. In a computer systems with such Devices, theCPU does not executed  Detailed I-O instructions in the same way as a normal CPU. In this case the CPU would not be able to execute detailed I-o instructions. Such instructions are stored in a main memory to be executed by a \\xadspecial-\\u200b \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xadpurpose processor in the data channel itself. The data channel is then used to send and receive data from the main memory. The instructions are then stored in the mainmemory to be\\xa0executed\\xa0by a processor in a data channel. The CPU initiates an I/O transfer by sending a control signal to the data channel. The data channel instructs the CPU to execute a sequence of instructions in memory. The process is called a \\'control signal\\' or \\'I/O control signal\\' in the software. The data channel performs its task independently of the CPU and signals the CPU when the operation is complete.  The data channel is used to communicate between the processor and the data channel. It is also used to send data to and from the storage device. The data channels are used to share data between processors and devices. This arrangement relieves the CPU of a considerable processing burden. It is the first time a computer has been able to do this. It also allows the CPU to focus on other tasks, such as writing to the internet. This is a first in the history of the computer industry. Another new feature is the multiplexor, which is the central termination point for data channels, the CPU, and memory. The multiplexer is a central termination point for all data channels and the CPU. It is also a central point for memory, CPU and data channels. It can also be used to connect multiplexers to other devices. The multiplexor schedules access to the memory from the CPU and data channels, allowing these devices to act independently. The multiplexer also schedules access to the data channels to allow data to be transferred between devices. It is the first of its kind to be used in a mobile phone. The Third Generation: Integrated Circuits. A discrete component is a single, self-contained transistor. A single, \\xadself-\\u200bcontained transistor is called a discrete component. An integrated circuit is a collection of discrete components. It is the most complex form of computer technology ever created. Throughout \\xa0the 1950s and early 1960s, electronic equipment was composed largely of discrete components. These components included transistors, resistors, capacitors, and so on. In the 1960s and 1970s, discrete components were replaced by larger, more complex components such as transistors. Discrete components were manufactured separately, packaged in their own containers, and soldered or wired. A discussion of the uses of numerical prefixes, such as kilo and giga, is contained in a supporting docu-ment at the Computer Science Student Resource Site at ComputerScienceStudent.com. together onto \\xadMasonite-\\u200blike circuit boards, which were then installed in computers, oscilloscopes, and other electronic equipment. The circuit boards were then used to power computers, computers, monitors and other equipment. They were also used in the development of computers and other computer equipment. Whenever an electronic device called  \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0for a transistor, a little tube of metal containing a \\xad \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0pinhead-\\u200b \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xadsized piece of silicon had to be soldered to a circuit board. For example, a transistor is a tiny piece of metal with a tiny head and a tiny body. The entire manufacturing process, from transistor to circuit board, was expensive and cumbersome. The entire process was cumbersome and expensive. The process was costly and cumbersome, and the process was inefficient. The manufacturing process was expensive, cumbersome and costly. It was also inefficient and expensive, and so was the manufacturing process. These facts of life were beginning to create problems in the computer indus-try. The computer was unable to understand what was going on in the world around it. This was causing problems for the computer in the form of problems with the computer code. It was also creating problems with its ability to understand the human mind. Early computers contained about 10,000 transistors. Early computers also contained about 1,000\\xa0transistors. The first computers were built in the 1970s and 1980s. They were designed to run in the 80s and 90s. The current computers were designed for the 1990s and 2000s. This figure grew to the hundreds of thousands, making the manufacture of newer, more powerful machines increasingly difficult. This made it difficult to make new, more powerful machines. It also made it harder to find people to work on the new machines. This led to a shortage of people working on new machines in the UK. In 1958 came the achievement that revolutionized electronics and started the era of microelectronics: the invention of the integrated circuit. Inventor of the\\xa0integrated circuit is credited with the development of the first microprocessor. The\\xa0integration\\xa0circuit was the first of its kind in the world. It is the integrated circuit that defines the third generation of computers. It is the first time a computer has been able to run on all three major operating systems at the same time. It was developed in the 1990s and 2000s by Hewlett-Packard and IBM. In this section, we provide a brief introduction to the technology of integrated circuits. We provide a brief explanation of how integrated circuits work. We also provide an overview of some of the technology used to make the chips. We then move on to the next section of the article. The IBM System/360 and the DEC \\xad \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0PDP-\\u200b \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xad8. Microelectronics means, literally, “small electronics.” Both were intro-duced at the beginning of the era. The IBM System-360 was the first computer to run on the IBM PC. The DEC PC was the last. Since the beginnings of digital electronics and the computer industry, there has been a persistent and consistent trend toward the reduction in size of digital electronic circuits. The size of a digital electronic circuit can be reduced by as much as 50 per cent. This has led to a dramatic reduction in the size of electronic devices. Before examining the implications and benefits of this trend, we need to say something about the nature of digital electronics. We need to talk about the implications of digital technology and its role in our lives. We must talk about digital electronics and their role in the digital world. A more detailed discussion is found in Chapter\\xa011 of the book. The book also includes a list of some of the most common applications of the IBM 7094 computer. For more information on the IBM\\xa07094, visit\\xa0the\\xa0IBM\\xa0Web\\xa0site. The basic elements of a digital computer, as we know, must perform data stor- \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0age, movement, processing, and control functions. Figure 1.9 is an IBM 7094 Configuration. Figure\\xa01.3 is a Brief History of Computers. Only two fundamental types of components are required (Figure\\xa01.10): gates and memory cells. Only two types of\\xa0components\\xa0are required: gates andMemory cells. The gates are used to control the memory cells in a memory cell. The memory cells are used in memory cells to store information about the past. A gate is a device that implements a simple Boolean or logical function. A gate can be used to control the flow of information in a computer system. For example, a gate can control the movement of a computer\\'s data in the form of a stream of data. For example, an AND gate with inputs A and B and output C implements the expression IF A AND B ARE TRUE THEN C IS\\xa0TRUE. For example, \\xa0if \\xa0A and B are both true, then \\xa0C is also true. Such devices are called gates because they control data flow in much the same way that canal gates control the flow of water. They are used to control data and data flow in a similar way to a canal gate. They can also be used to store data such as a person\\'s mobile phone number. The memory cell is a device that can store 1 bit of data. The device can be in one of two stable states at any time. The memory cell can be used to store data on a computer or a mobile phone. It can also be used as a storage device. By interconnecting large numbers of these fundamental devices, we can construct a computer. We can use this technology to build a computer that can do anything. We call it a computer because we can use it to build anything. It\\'s a computer with a mind of its own. We can relate this to our four basic functions as follows: data storage, data retrieval, data storage and data storage. Data storage:  Provided by memory cells. Data retrieval: Provided by the memory cells of the memory cell.Data storage: provided by memory cell of the Memory Cell. Data Storage: Provided to memory cells by memory Cell. Data processing:  Provided by gates.com. For confidential support call the Samaritans in the UK on 08457 90 90 90 or visit a local Samaritans branch, see www.samaritans.org for details. In the U.S. call the National Suicide Prevention Line on 1-800-273-8255. Data movement:    Data movement is a movement of people moving in a certain direction. Data movement: People moving in the same direction as the movement of the data. Data Movement: People are moving in same direction, but in different directions.Data Movement: Data Movement is a Movement of People. Data Move: People move in a particular direction. The paths among components are used to move data from memory to memory and from memory through gates to memory. The paths are used in a number of different ways to transfer data from one part of the computer to the next. For more information on how to use these paths, visit the Computer History Museum\\'s website. The paths among components can carry control signals. The paths can also be used to send signals to other parts of the system. The control signals can be sent to different parts of a system at the same time. For more information, visit www.electronic-control.org. For example,  a gate will have one or two data inputs plus a control signal input that activates the gate. For example, a gate could have one data input plus one control input. A gate could also have two data input and two control input and one control output. When the control signal is ON, the gate performs its function on the data inputs and produces a data output. When the gate is OFF, the data input and data output are\\xa0separated\\xa0from each other. For more information on the gate, visit: http://www.cnn.com/2013/01/29/technology/gate-control-signal-control.html. Conversely, when the control signal is OFF, the output line is null, such as the one produced by a high impedance state. The control signal can also be turned off to produce a null output line. The effect is similar to that produced by an off-the-shelf, low-impedance device. The memory cell will store the bit that is on its input lead when the WRITE control signal is ON. It will place the bit in the cell on its output lead when the READ control signals are ON. The bit is placed on the cell\\'s output lead when it is read. A computer consists of gates, memory cells, and interconnections among these elements.Thus, a computer is a computer that is made up of gates and memory cells. A computer is also a computer which is composed of memory cells and other elements. For more information on a computer, visit the Computer World website. The gates and memory cells are, in turn, constructed of simple elec-tronic components, such as transistors and capacitors. The gates are made up of a series of simple gates, each of which is made of a different material. These materials are then used to build the memory cells and gates. The integrated circuit exploits the fact that such components as transistors, resistors, and conductors can be fabricated from a semiconductor such as silicon.  The integrated circuit can be used to build computers and other devices. It can also be used in the development of new technologies. It is merely an extension of the \\xad774 solid-\\u200bsolid-\\u200bstate art to fabricate an entire circuit in a tiny piece of silicon. Rather than assemble discrete components made from separate pieces of silicon into the same circuit, it is possible to combine them into a single circuit. Many transistors can be produced at the same time on a single wafer of silicon. Many transistors\\xa0can be produced\\xa0at the same time\\xa0on a single silicon wafer. Many different types of transistors are used to produce different kinds of devices. Equally important, these transistors can be con-nected with a process of metallization to form circuits. This process can be used to create new types of transistors. For more information on this process, visit: http://www.nano.org/metallization. Figure 1.11 depicts the key concepts in an integrated circuit. Figure 1.10\\u2003 Fundamental Computer Elements. Figure\\xa01.11\\u2003 Key Concepts in an Integrated Circuit. Figure 2.1’ Fundamental Computer elements.Figure 2.2’ Key Concepts In an integrated Circuit. Figures 2.3 and 2.4. A thin wafer of silicon is divided into a matrix of small areas, each a few millimeters square. A thin wafers of  silicon are divided into small areas. A matrix of  small areas is divided by a thin layer of silicon. The wafer is then placed on a thin sheet of glass. The identical circuit pattern is fabricated in each area, and the wafer is broken up into chips. The wafer has a different pattern for each area of the circuit. The circuit is then fabricated in a different way for each part of the pattern. The pattern is called a wafer. Each chip consists of many gates and/or memory cells plus a number of input and output attachment points. Each chip is made up of a large number of gates and or memory cells. The chip is then connected to a processor using a series of gates or cells. This chip is then packaged in housing that protects it and provides pins for attachment to devices beyond the chip. It is then used to create chips that can be used in a variety of ways. For more information on the chip, visit the company\\'s website or go to: www.sensorychip.com. A number of these packages can then be interconnected on a printed circuit board to produce larger and more complex circuits. A number of  packages can be interconnected to make larger and more complex\\xa0circuits. These packages can also be combined to create larger and\\xa0more\\xa0complex\\xa0circuit. Initially, only a few gates or memory cells could be reliably manufactured and packaged together. The first memory cells were made in the 1970s and 1980s. In the 1990s, memory cells began to be made in a variety of ways. They include memory cells, memory gates, and memory cells with memory cells. These early integrated circuits are referred to as \\xadsmall-\\u200b \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xadscale integration (SSI) SSI is a type of integrated circuit. SSI was developed in the 1950s and 1960s by the German semiconductor company Riemann. It was used in early computers, among other things, to make early computers. As time went on, it became possible to pack more and more com-ponents on the same chip. As a result, the chip was able to hold more than one component at a time. The chip is now being used in a range of products, including Apple\\'s iPhone 5 and Samsung\\'s Galaxy S5. This growth in density is illustrated in Figure\\xa01.12; it is one of the most remarkable technological trends ever recorded. Figure 1.12: Growth in density\\xa0is illustrated in figure 1.8. Figures 1.9 and 1.10: Growth\\xa0in\\xa0density\\xa0is\\xa0exhibited\\xa0in figures 1.11 and 1\\xa012. This figure reflects  the famous Moore’s law, which was propounded by Gordon Moore, cofounder of Intel, in 1965. It is based on the fact that the number of pixels on a computer screen increases with the size of the chip. The figure reflects the figure of 1,000,000 by 1,200,000. [MOOR65] is an online magazine for young people. It is published by Oxford University Press in the UK and the U.S. For more information, visit www.moore65.com. For confidential support, call the Samaritans on 08457 90 90 90 or visit a local Samaritans branch, see www.samaritans.org for details. Moore observed that the number of transistors that could  be put on a single chip was doubling every year. He correctly predicted that this pace would continue into the near future. Moore predicted that the next generation of computers would be able to use as many transistors as possible. To the surprise of many, including Moore, the pace continued year after year and decade after decade. The pace continued to be the same for years to come, including in the 1990s and early 2000s. In the 2000s, the pace was even faster, and in the 2010s, it continued. The pace slowed to a doubling every 18\\xa0months in the 1970s but has sustained that rate ever since. The world\\'s population has grown at a rate of more than one per cent a year since the 1950s. The rate of growth is expected to continue for at least the next 20 years. The consequences of Moore’s law are profound:1. It is impossible to predict the future. 2. It can’t be predicted in advance. 3. It has no effect on the present. 4. It cannot be predicted after it has happened. The cost of a chip has remained virtually unchanged during this period of rapid growth in density. The chip\\'s density has increased from 1,000g to 1,500g per square centimetre over the past decade. The cost of the chip has also remained virtually same during this time. This means that the cost of computer logic and memory cir-cuitry has fallen at a dramatic rate. The cost of computing has fallen by more than 50 per cent in the last 20 years. This has led to a dramatic drop in the price of computers and memory. Wafer, Chip, and Gate is a relationship between a chip and a gate. Figure 1.11 shows the relationship between the chip and the gate. The vertical axis uses a log scale. Figure\\xa01.3 is a timeline of the history of computer technology. It includes the development of the first computers. A basic review of log scales is in the math refresher document at the Computer Science Student Resource Site at ComputerScienceStudent.com. The log scale is a way to measure the length of a line. It is also a way of measuring the distance between two points on a scale of 1 to 10. 22\\u2002 \\u2002 Chapter 1 / Basic Concepts and Computer Evolution  2.2. Because logic and memory elements are placed closer together on more ensely packed chips, the electrical path length is shortened, increasing oper-ating speed. 2.3. Because electrical path lengths are shortened, the speed of the computer is increased. 3.3.2. The 3.3 rule of thumb: Don’t be afraid to ask for help when you’re having a bad day. It’s never too late to get help when it comes to getting a good day off. It can be done. The computer becomes smaller, making it more convenient to place in a vari-ety of environments. It can be used in a variety of environments, including the home, office, and office. The computer can also be used on the road, in a car, or in a hotel. 4. There is a reduction in power requirements. 5. There are no plans to reduce the number of turbines in the fleet. 6. There will be no further changes to the power grid in the near future. 7. The power grid will not be reduced in any way in the future. 5.5. \"I\\'m a little bit of a mess,\" says one of the world\\'s best-selling authors. \"It\\'s like I\\'m in a dream,\" says another. \"There\\'s no way out of it,\" says the author of \"The Power of Love\" \"I don\\'t know how to get out of this one.\" The interconnections on the integrated circuit are much more reliable than solder connections. The interconnects on the Integrated Circuit are\\xa0much\\xa0more\\xa0reliable than  solder\\xa0connections. The\\xa0integrated\\xa0circuit is much more\\xa0stable\\xa0than the\\xa0solder circuit. The integrated circuit is much\\xa0more reliable\\xa0than\\xa0thesolder circuits. With more circuitry on each chip, there are fewer inter-chip connections. With more circuitry, there is less need for inter- chip connections. More circuitry means fewer connections between chips, making it easier to test and test chips. More chips mean fewer connections mean fewer problems with the chip. ibm system/360 By 1964, IBM had a firm grip on the computer market with its 7000 series of machines. IBM system /360: A look back at some of the most memorable moments in the history of the company\\'s computers. The system was designed to run the IBM 7800 and 7800 Plus computers. In that year, IBM announced the System/360, a new family of computer products. It was the first of a series of computers that would become known as the IBM\\xa0System\\xa0360. IBM\\xa0systems\\xa0and software\\xa0were\\xa0launched\\xa0in\\xa01989. IBM announced the 360 product line was incompatible with older IBM machines. The announcement was no surprise, but it was unpleasant news for current IBM customers. The 360 line was designed to compete with Microsoft\\'s Windows 8.1 operating system. It was the first time IBM had made such an announcement. IBM felt this was necessary to break out of some of the constraints of the 7000 architecture and to produce a system capable of evolving with the new integrated circuit technology. The transition to the 360 would be difficult for the current customer base, but IBM felt it was necessary. [PADE81, GIFF87].    \"Pade81\" is one of the most popular GIFs on the internet. \"GIFF87\" is a collection of GIFs from around the world. The most popular is \"GifF87,\" which has more than 100,000 views on YouTube. The strategy paid off both financially and technically. The company was able to increase its revenue by more than $1.5 billion over the next five years. The strategy was also successful in terms of reducing the cost of living for its employees. The business was also able to reduce the number of employees it employed. The 360 was the success of  the decade and cemented IBM as the overwhelmingly dominant computer vendor. IBM had a market share above 70% of the computer market at the end of the decade. The 360 was IBM\\'s most successful computer product to date. IBM\\'s 360 computer was the most popular computer in the world at the time. The 360 remains to this day the architecture of IBM’s mainframe9 computers. And, with some modifications and extensions, the architecture of the 360 remains the same as that of the IBM mainframe. The 360 was the first computer to run on IBM\\'s mainframe system. Examples using this architecture can be found throughout this text. For more information on this architecture, visit the Architectural Institute of America\\'s website. For information on how to use this architecture in your own home, go to the Architectual Institute of the United States\\' website. for more information about this Architecture, visit its website. The System/360 was the industry’s first planned family of computers. It was designed to compete with the Apple II, the PC, and the Commodore 64. The system was designed in the 1980s and 1990s. It is still in production today, with some versions still available. The family \\xa0covered a wide range of performance and cost. The family  covered a range of\\xa0performance\\xa0and\\xa0cost. The\\xa0family\\xa0covered\\xa0a wide range\\xa0of\\xa0performance and cost, including a wide\\xa0range of\\xa0costs\\xa0for\\xa0cars. The models were compatible in the 11947 to the 21947 model year. The models were available in black, white, and red. They were also available in blue, green, and white. The model year was 1947-1947, and the model year 1948-1949. The term mainframe is used for the larger, most powerful computers other than supercomputers. First workingtransistor was the first working mainframe. Moore\\'s law is based on the fact that the mainframe has more transistors than the mainframes. The term mainframes is also used to refer to large computers that can run multiple programs. Typical characteristics of a mainframe are that it supports a large database, has elaborate I/O hardware, and is used in a central data processing facility. For more information on IBM\\'s mainframes, visit: www.ibm.com/mainframes. sense that a program written for one model should be capable of being executed by another model in the series. With only a difference in the time it takes to execute, the program can be written for both models. The program can then be run on either of the two models at the same time. The concept of a family of compatible computers was both novel and extremely successful. The concept was to create a single computer that could run a range of programs. This was the first of a series of successful family computers. The family of computers was designed to work together in the same way. A customer with modest requirements and a budget to match could start with the relatively inexpensive Model 30. The Model 30 can be had for as little as $1,000. It is available in black or white and comes in a range of different colors. For more information on the Model 30, go to: http://www.electricvehicle.com. Later, if the customer’s needs grew, it was \\xa0possible to upgrade to a faster machine with more memory without sacrificing the \\xa0investment in \\xadalready-\\u200b \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xaddeveloped software, the company said. The company also said that it was possible to buy a machine with a faster processor and more memory. Similar or identical instruction set: In many cases, the exact same set of machine instructions is supported on all members of the family. The characteristics of a family are as follows: ■ \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0“\\xa0”“” “’” \\xa0‘”\\xa0\\u2009’\\xa0‚”, ‘’, ’’.’ ”,\\xa0’\"”. A pro-gram that executes on one machine will also execute on any other. This is because the machines are designed to work together. For more information, visit the U.S. National Security Agency\\'s website. For confidential support, call the Samaritans on 08457 90 90 90 or visit a local Samaritans branch, see www.samaritans.org for details. In some cases, the lower end of the family has an instruction set that is a subset of that of the top end. This is known as a \"lower end\" instruction set. In some cases, this instruction set is not the same as the one at the top. In other cases, it is the same or a subset. This means that programs can move up but not down. It also means programs can be moved up but not down. For more information on how this works, go to: http://www.cnn.com/2013/01/30/programs-move-up-but-not-down.html. Similar or identical operating system: The same basic operating system is available for all family members. Similar or identical: A system that is similar or identical to the one you already have in your home or office. It\\'s called a \"similar or identical\" operating system. It means the same basic system is available for all of you. In some cases, additional features are added  to the \\xadhighest-\\u200bhighest-\\xadhighest-end members. For more information on how to join, or to sign up for a free trial, visit the \\xadhigher-\\u200bhigher-Âhighest-level-membership site. The rate of instruction execution increases in going from lower to higher family members. The rate of execution increases as the family size increases. The number of family members involved in the instruction increases with the number of people in the family. This increases the speed at which instruction is carried out. The number of I/O ports increases in going from lower to higher family members. The number of family members also increases. The increase in family members is due to the increase in the number of people in the family. The higher the family\\'s income, the more I/o ports are available to them. Increasing memory size: The size of main memory increases in going from lower to higher family members. Increasing memory size also increases the number of memories a person has. This increases the amount of memories that can be retrieved from a person\\'s memory. This is called increasing memory size or memory expansion. At a given point in time, the cost of a system increases in going from lower to higher family members. The cost of the system increases as it goes from a lower to a higher family member. The system is designed to help people get along with each other. It is not intended to be a permanent solution. How could such a family concept be implemented? How could such an idea be implemented in the UK? What would be the best way to implement such a concept? What are your ideas? Share them with us in the comments below. We would love to hear from you. Differences were achieved based on three factors: basic speed, size, and degree of simultaneity [STEV64]. The results were published in the journal STEV64, published by the University of California, Los Angeles, on November 16, 2013. The book was published by Elsevier, a division of Apple Inc, and is available on Amazon.com. For example, greater speed in the execution of a given instruction could be gained by the use of more complex circuitry in the ALU. The ALU allows suboperations to be car-ried out in parallel. For  example, the use of  complex circuitry could be used to speed up the\\xa0execution\\xa0of an instruction. Another way of increasing speed was to increase the width of the data path between main memory and the\\xa0CPU. Another way was to\\xa0increase\\xa0the width of the data path from main memory to the\\xa0 CPU. This increased the speed of the processor by up to 30%. On the Model 30, only 1 byte (8 bits) could be fetched from main memory at a time. On the Model 75, 8 bytes could be fetched at a Time. The Model 75 has a much larger memory capacity than the 30 and 75. The System/360 not only dictated the future course of IBM but also had a pro-found impact on the entire industry. The system was the first of its kind in the world of computer networking. It was developed by IBM in the late 1950s and early 1960s. IBM was founded in 1947 and went on to become one of the world\\'s largest computer companies. Many of its features have become standard on other large computers. For more information on how to get the most out of your computer, visit www.computerworld.com. In the U.S. call the National Computer Center at 1-800-273-8255 or visit http://www.ncc.org/. IBM shipped its first System/360 in the same year that it shipped the first PC. The PC was a PC from Digital Equipment Corporation (DEC), which later became IBM. DEC\\'s PC was the first computer to run IBM\\'s PC operating system, the IBM PC. IBM\\'s first PC shipped in 1983. At a time when the average computer required an \\xad \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0conditioned room, PDP-8 was small enough that it could be placed on top of a lab bench or be built into other equipment. PPDP-8 (dubbed a minicomputer by the industry, after the miniskirt of the day) was developed in the 1950s. It could not do everything the mainframe could, but at $16,000, it was cheap enough for each lab technician to have one. It was the first of its kind in the U.S. and was used by the National Institutes of Health. In contrast, the System/360 series of mainframe computers introduced just a few months before cost hundreds of thousands of dollars. The system was the first of its kind in the U.S. and was designed to compete with IBM\\'s mainframe computer. It was the last of its type to be introduced in North America. 1.3 / A Brief History of Computers. 1.4 / Basic Concepts and Computer Evolution.1.5 / The Evolution of the Computer. 2.1 / The Computer’s Evolution. 3.2. The evolution of the computer. 4.3. “The Computer”. The low cost and small size enabled another manufacturer to integrate it into a total system for resale. The \\xadPGP-\\u200bGP-P-GP-GP was developed in the 1990s by a group of researchers at the University of California, San Diego. It was the first of its kind to be developed by a manufacturer. These other manu-facturers came to be known as original equipment manufacturers (OEMs), and the OEM market became and remains a major segment of the computer marketplace. The\\xa0OEM\\xa0market is now one of the largest in the world. It is estimated to be worth more than $100 billion. In contrast to the \\xadcentral-\\u200b purposefully-switched architecture (Figure\\xa01.9) used by IBM on its 700/7000 and 360 systems, later models of the \\xad \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0PDP-\\u200b\\xad8 used a structure that became vir- \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0tually universal for microcomputers. This is illustrated in Figure\\xa01.13. Figure 1.13 is an example of the different ways in which the different methods are used. The most common method is to use a mixture of techniques from different areas of the world. For more information on the different techniques, see Figure 2.1. The \\xadPDP-\\u200b774\\xad8 bus, called the Omnibus, consists of 96 separate signal paths. It is used to carry control, address, and data signals. The Omnibus is used in the U.S. and Europe. It was developed in the 1960s. Because all system components share a common set of signal paths, their use can be controlled by the\\xa0CPU. For example, the CPU can control the use of the system\\'s Wi-Fi signal to control a computer\\'s Internet connection. For more information on how to use the system, visit the\\xa0System\\xa0Cabinet\\xa0and the\\xa0Programming\\xa0Center. This architecture is highly flexible, allowing modules to be plugged into the bus to create various configurations.  The architecture is based on a modular design system.  It is designed to be able to be used in a variety of ways, including as a mobile phone system. It is only in recent years that the bus structure has given way to a structure known as \\xa0reproach.Beyond the third generation there is less general agreement on defining generations of computers. The term\\xa0reach\\xa0is used to refer to an interconnect between two computers. Table\\xa01.2 suggests that there have been a number of later generations, based on advances in integrated circuit technology. Table 1.2.1 suggests that  later generations have been\\xa0based\\xa0on advances in\\xa0integrated\\xa0circuit technology. table 1.1.1\\xa0suggests\\xa0there have been several generations. More than 1,000 components can be placed on a single inte-grated circuit chip. LSI is a form of large-\\u200bscale integration (LSI) that can be used to combine multiple components into a single chip. The technology is being trialled in the U.S. and will be rolled out in other countries. VLSI chips achieved more than 10,000 components per chip. Current ULSi chips can contain more than one billion components. VLSI chip can\\xa0contain\\xa0more than 1,000,000\\xa0components per chip, compared to 1,500,000 per chip on current chips. With the rapid pace of technology, the high rate of introduction of new prod-ucts, and the importance of software and communications as well as hardware, the classification by generation becomes less clear and less meaningful. The\\xa0classification\\xa0by generation is no longer clear or meaningful. In this section, we mention two of the most important of developments in later generations. The first is the development of the computer, and the second is the creation of the internet. We also discuss the impact of the Internet on the lives of people around the world. We end the section with a look at some of the best known examples. semiconductor memory. The first application of integrated circuit technology to computers was the construction of the processor (the control unit and the arithmetic and logic unit) out ofintegrated circuit chips. The technology was developed in the 1950s and 1960s by the semiconductor industry. But it was also found that this same technology could be used to construct memories. For example, a person could use the technology to construct a memory of a person they had never met. For more information on the project, visit: www.cnn.com/soulmatestories. In the 1950s and 1960s, most computer memory was constructed from tiny rings of ferromagnetic material, each about a sixteenth of an inch in diameter. In the 1960s and 1970s, the memory was made from much larger rings of magnetic material. These rings were strung up on grids of fine wires suspended on small screens inside the computer. These rings were hung from a small screen inside the computer. The rings were then hung on wires suspended from small screens in the computer itself. The result was a series of pictures called \"Rings of Power\" Magnetized one way, a ring (called a core) represented a one; mag-netized the other way, it stood for a zero. A ring was also called a core if it was magnetized in a certain way. A core was a core when it was magnetized in the opposite way. It took as little as a millionth of a second to read a bit stored in memory. memory was rather fast;  it took a million years to write a single word to a file. It was possible to write as much as a billion words in a single second. It could also be written in fractions of seconds. expensive and bulky, and used destructive readout: The simple act of reading a core  \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0erased the data stored in it. But it was  \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\u2009\\u2009 \\u2009‚\\u2009,\\xa0“”, “’’, \\u2009, ‘’.’\\xa0””;\\xa0‘”.“.” “. ”. ‘. ’. ‚’; “\\xa0\\u202c”,. “; ”, \\xa0‚, ”; ’\\u2009; \\u2009;”\\xa0’I’m looking at you,” she said. “What’s going on?” he asked. She replied: “I don’t know. I’ve been looking at this for a few minutes. What is it?’ It was therefore necessary to install circuits to restore the data as soon as it had been extracted. It was then possible to use the data again in the same way as before. The data was then restored to its original state. It is now possible to\\xa0use the data in a similar way to restore the original state of the data. Then, in 1970, Fairchild produced the first relatively capacious semiconductor memory. Fairchild\\'s first semiconductor memory was developed in 1970. It was the first memory chip to be able to store a large amount of data. The technology was developed by Fairchild in the 1960s and 1970s. This chip, about the size of a single core, could hold 256 bits of memory. It could be used to create a new type of chip called a nanowire. The chip is expected to be unveiled at an event in New York City in the coming weeks. It was nondestructive and much faster than core. It was used in the development of the iPhone, iPad, iPod, and other mobile devices. It could also be used as a way to communicate with other mobile phones and computers. It is now being used in a variety of ways, including the Internet. It took only 70\\xa0billionths of a second to read a bit. It took only\\xa070 billionths\\xa0of a second to read a\\xa0bit. It\\xa0took only 70 billionths of\\xa0a\\xa0second\\xa0to\\xa0read a bit\\xa0to read\\xa0a bit.\\xa0It took\\xa0only\\xa070\\xa0billionth\\xa0of\\xa0a second  to\\xa0read\\xa0abit. However, the cost per bit was higher than for that of core. The cost of each bit was also higher than the cost of a single core. This is due to the higher cost of producing each bit of the core. For more information, visit http://www.cnn.com/2013/01/30/technology/cnn-cnn\\xa0cnnbit.html. In 1974, the price per bit of semiconductor memory dropped below the price of core memory. This was a seminal event in the history of semiconducting memory. The price of a core memory bit is now more than $1,000. The cost of a bit of memory is now less than $100. Following this, there has been a con-tinuing and rapid decline in memory cost. This has been accompanied by a corresponding increase in physical memory density. There has also been a rapid increase in the number of neurons in the human brain. There have been a number of studies on the effects of memory loss on cognitive function. This has led the way to smaller, faster machines with mem-ory sizes of larger and more expensive machines from just a few years earlier. This has led to the development of smaller and faster machines that can do more with less power than ever before. It has also led to a revolution in the way computers are designed and built. Devel-opments in memory technology, together with developments in processor technology, have changed the nature of computers in less than a decade. The next step is to look at how memory technology has changed over the past 10 years, and how it will change in the future. Although bulky, expensive computers remain a part of the landscape, the computer has also been brought out to the “end user,” with office machines and personal computers. “I’m a computer geek. I like to play with computers. I’ve always been able to do that.” Since 1970, semiconductor memory has been through 13 generations: 1k, 4k, 16k, 64k, 256k, 1M, 4M, 16M, 64M, 256M, 1G, 4G, and, as of this writing, 8 Gb. Each generation has provided increased storage density, accompanied by declining cost per bit and declining access time. Each generation has also provided  increased storage density and increased storage capacity, but also increased cost per bits. Each\\xa0generation\\xa0has provided\\xa0increased\\xa0storage density,\\xa0accompanied\\xa0by\\xa0decreased\\xa0cost per bit, and declining\\xa0access\\xa0time. Densities are projected to reach 16 Gb by 2018 and 32 GB by 2023. The world\\'s population is expected to grow by 1.2 billion people by 2050. The U.S. population is projected to grow 1.7 billion people in the same period. The density of elements on memory chips has continued to rise, so has the density of element on processor chips. The research was published in the International Institute of Supercomputing (ITRS14) report. The report was published on the eve of the World Economic Forum\\'s annual meeting in Davos, Switzerland. As time went on, more and more elements were placed on each chip, so that fewer and fewer chips were needed to construct a single computer processor. The first computer processors were built in the 1950s. The current processors were created in the 1990s. They are based on a design that was developed in the 1980s. A breakthrough was achieved in 1971, when Intel developed its 4004. Intel\\'s 4004 was the world\\'s first microprocessor. It was developed by Intel\\'s co-founders at the University of California, Los Angeles. The 4004 processor was the basis for Intel\\'s next-generation microprocessors. The 4004 was the first chip to contain all of the components of a CPU on a single chip. The microprocessor was born. The chip was developed in the 1960s by Texas Instruments. It was used in the Apple II, III, IV, and V computers. The 4004 can add two 4-bit numbers and can multiply only by repeated addi-tion. The 4004 was developed in the 1970s and is still in use today. It was developed by the same team that created the 4003 computer. It is the only computer in the world that can add and multiply 4 bit numbers. By today’s standards, the 4004 is hopelessly primitive, but it marked the begin-ning of a continuing evolution of microprocessor capability and power. The 4004 was the first microprocessor to have a built-in word processor. It was developed in the 1960s and 1970s by the Massachusetts Institute of Technology. This evolution can be seen most easily in the number of bits that the processor deals with at a time. It can also be seen in the amount of data the processor is able to process at a given time. For more information on how to get the most from your computer, visit the Computerworld website. The data bus width is the number of bits of data that can be brought into or sent out of the processor at a time. There is no clear-\\u200bclear-\\u200bgoers\\xadcut measure of this, but perhaps the best meas-uve is the dataBus width. Another measure is the number of bits in the accumu-lator or in the set of \\xadgeneral-\\u200bpurpose registers. Another measure is to see how many bits there are in a set of registers, such as in a computer\\'s hard drive. For example, a computer can have as many as 1,000 bits in its hard drive at a time. Often, these measures coincide, but not always. Sometimes they coincide with each other, but sometimes they don\\'t. Sometimes the measures coincide but not always, as in the case of this case. Sometimes, the measures don\\'t coincide at all. But sometimes they do, as this case shows. For example, a number of microprocessors were developed that operate on 16-bit numbers in registers but can only read and write 8 bits at a time. This is called the 8-bit limit. It was developed to allow the use of 8 bits in a 16- bit register. The next major step in the evolution of the microprocessor was the introduc-tion in 1972 of the Intel 8008. The next step was the introduction in 1973 of theIntel 8007. The Intel 8007 was the first microprocessor to be sold in the U.S. This was the first 8-bit microprocessor and was almost twice as complex as the 4004. It was almost twice the size of the original microprocessor. It is still the most powerful microprocessor in the world today. It has been used in the iPhone, iPad, and many other gadgets. Neither of these steps was to have the impact of the next major event: the introduction in 1974 of the Intel 8080. In 1974, Intel introduced the first computer with a processor with a speed of 1.5 megabits per second. It was the first of a series of processors that would become known as the \"Intel 8080\" This was the first \\xadgeneral-\\u200bgeneral-\\xadpurpose microprocessor. It was developed in the 1990s by a group of researchers at the University of California, Los Angeles. It is now used in a range of products, including smartphones, tablets, and computers. Whereas the 4004 and the 8008 had been designed for specific applications, \\xa0the 8080 was designed to be the CPU of a \\xadgeneral-\\u200b\\xadpurpose microcomputer. The 8080 is the successor to the 804, 8008 and 4004. Like the  \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa01.3 / A Brief History of Computers’ \\u200225 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa026\\u2002 \\u2002 Chapter 1 / Basic Concepts and Computer Evolution’s  \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0Table 1.3\\u2003 Evolution of Intel Microprocessors (page 1 of 2) \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0(a) 1970s. Processors were introduced in the 1980s. Processors were designed to run at speeds of up to 80\\xa0MHz. The first processors were designed in the 1970s and 80s. They were based on the Hasselblad and Zilog Z80 processors. Processors \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa080286 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0386 TM DX \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0386TM SX \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0486TM DX CPU \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa01982 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa01985 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa01988 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa01989 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa01989\\xa0\\xa0\\xa0-\\xa0\\xa0 -\\xa0\\xa0--\\xa0---\\xa0- - -\\xa0 - - - --- ---- - -- - --- - -- --- -- - ----- ------------ -------- --- ---\\xa0 --- ------ --- --- \\xa0----\\xa0 ---- -- ---- - --- -- ---\\xa0 - --\\xa0 -- -- - --- - -- \\xa0 -- -\\xa0 --  -- -  -- -- --\\xa0-- - ---  --\\xa0 ---  ---- \\xa0 - \\xa0\\xa0 -- --- -- -- - - Â -- - Clock speeds \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa06–12.5 MHz \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa016–33 MHz\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa025–50\\xa0MHz\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0Bus width \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa016 bits \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa032 bits \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa016\\xa0bits \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa032\\xa0bits\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0Number of transistors \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0134,000 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0275,000\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa01.2\\xa0million. Feature size (\\u2009µm) \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa01.5 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa016 MB \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa04\\xa0GB \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa04 GB \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa04GB\\xa0\\xa0\\u2009\\u20091.8–1.4 GB. Processors \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0486TM SX \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0Pentium Pro\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0Pentum II\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0\"Pentanium\" is a family of processors. 1997 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0Clock speeds 16–33 MHz, 60–166 MHz, 150–200 MHz. Bus width 32 bits 32 bits, 64 bits. Number of transistors 1.185\\xa0million 3.1\\xa0million 5.5\\xa0million 7.5 million. Feature size (\\u2009µm) 1.2\\xa0million 1.3\\xa0million. 0.8 encompasses0.6 encompass0.35 encompasses 0.8 encompass 0.4 encompasses 0,6 encompass 0,8 encompass0,6 embody 0,4 encompass 0,.6 encompass 1 encompasses 0,.8 encompass 1.4 encompass 1,6 encompasses 1,8 encompasses 1.2 encompass 1,.4 encompass. , the 8080 is an 8-bit microprocessor. It is used in the Apple II, III, and IV computers. The 8080 can also be used for the Commodore 64 and Commodore\\xa0Vega\\xa0processors. It was developed in the 1970s and 1980s by Motorola. The 8080, however, is faster, has a richer instruction set, and has a large addressing capability. The 8080 is faster than the 8060, but the 8080 has a larger addressable area. It is also faster than both the 8070 and the 8075. About the same time, 16-bit microprocessors began to be developed. 16- bit microprocessers were developed in the 1980s and 1990s. They were used in computers with 16-bits of memory. They are still used today in the iPhone and other mobile devices. How-ever, it was not until the end of the 1970s that powerful 16-bit processors appeared. The first 16- bit processors were developed in the late 1960s and early 1970s in the U.S. and Europe. They were the first computers to use 16-bits. One of these was the 8086. The 8086 was an early version of the Apple IIGS. It was designed by Steve Wozniacki, who also designed the iPhone and iPad. It went on sale in 1983 and was the most successful Apple product of all time. The next step in this trend occurred in 1981, when both Bell Labs and Hewlett-Packard developed 32-bit, single-chip microprocessors. The next step was in 1983, when the first 32- bit microprocessor was developed by IBM. In 1984, the first 64-bit chip was also developed. Intel introduced its own 32-bit microprocessor, the 80386, in 1985 (Table\\xa01.3). The\\xa080386 was the first\\xa032-bit\\xa0microprocessor\\xa0to be\\xa0available\\xa0to the general public. It was used in the Intel PC, PC, Mac, and Macbook computers. 1.4 / 1.5 / 2.1.2 / 3.2.3 / 4.3.4. 5.4/ 6.5. 6.6. 7.7. 8.8. 9.9. 10.10. 11.11. The Evolution of the Intel x86 Architecture’s 27 most recent Processors. Core 2 Duo, Core i7 EE 4960X, and Core i9 are the most recent processors. The Intel X86 Architecture is based on the Intel Pentium III and Pentium 4 processors. Throughout this book, we rely on many concrete examples of computer design and implementation to illustrate concepts. The Evolution of the Intel x86 Architecture is published by Oxford University Press, priced £16.99. For more information on the book, visit www.oxfordpub.co.uk. Numeroussys-tems, both contemporary and historical, provide examples of important computer architecture design features. For more information, visit the Computer Architecture Institute\\'s website or the Computer Science Museum of New York City\\'s Computer History Museum. For further information on the Computer Architect\\'s Institute, visit its website or its Museum of Computer Science. But the book relies principally on examples from two processor families: the Intel x86 and the ARM architectures. It is written in an attempt to make the book more accessible to a wider audience. The book is available now in hardback and e-book form. For more information, visit the book\\'s website. The current x86 offer-ings represent the results of decades of design effort on complex instruction set com-puters (CISCs) The x86 architecture is based on the instruction set computer (ISC) design of the 1960s and 1970s. It is the result of years of research and development on complex instructions. The x86 incorporates the sophisticated design principles once found only on mainframes and supercomputers. It serves as an excellent example of CISC design. The x86 is the most powerful computer platform in the world. It is the world\\'s largest computer platform, with more than 100 million users. An alternative approach to processor design is the reduced instruction set computer (RISC) RISC is based on a reduced number of instructions per instruction. RISC can be used to reduce the size of a processor\\'s instruction set. For more information on RISC see RISC-RISC. The ARM architecture is used in a wide variety of embedded systems. It is one of the most powerful and \\xad \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0best-\\u200b \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xaddesigned \\xad \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0RISC-\\u200b proprietary\\xadbased systems on the market. The ARM architecture can be used to build a variety of systems, including smartphones and tablets. In this section and the next, we provide a brief overview of these two systems. In the next section, we will look at how the two systems work together. The next section will provide an overview of how they work in combination. The third and final section will focus on how they interact with each other. In terms of market share, Intel has ranked as the number one maker of micro-processors for \\xadnon-\\u200b \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xadembedded systems for decades, a position it seems unlikely to yield. Intel has been the world\\'s largest processor maker for more than a decade. The evolution of its flagship microprocessor product serves as a good indica-tor of the evolution of computer technology in general. It is a good indication of how technology is evolving. It\\'s a good indicator of how computer technology is progressing. It has been around since the 1970s and is still around today. Table\\xa01.3 shows that evolution. 3 shows that. evolution. 4 shows that the evolution. of the human brain. 4.3 is the most recent stage in the evolution of the brain. 3.4 shows that it is the first stage of the development of the nervous system. Interestingly, as microprocessors have grown faster and much more complex, Intel has actually picked up the pace. Intel says it has been able to keep pace with the growth of the microprocessor. The company says it will continue to make progress in the next few years. Intel used to develop microprocessors one after another, every four years.Intel used to develop microprocessor one after two, every 4 years. Intel used to\\xa0develop\\xa0microprocessors\\xa0one after another,. every three years. Today, Intel develops microprocessers one after three, every two years. But Intel hopes to keep rivals at bay by trimming a year or two off this development time, and has  done so with the most recent x86 generations.Intel refers to this as the \\xadipienttick-\\u200b\\xadtock model, and says it will be used in the next generation of computers. Using this model, Intel has successfully delivered \\xadnext-\\u200b \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xadgeneration silicon technology as well as new processor microarchitecture on alternating years for the past several years, according to the company\\'s website. The company has also delivered new processors in the past two years. See http://www.intel.com/content-www-us/en/\\xadsilicon-\\xadinnovations/intel-tick-tock- model-general.html for more information on Intel\\'s TICK-Tock program. For more information about Intel\\'s Tick-Tocks program, visit http:// www.IntelTickTock.com. 28\\u2002 \\u2002 Chapter 1 / Basic Concepts and Computer Evolution. It is worthwhile to list some of the highlights of the evolution of the Intel prod-uct line. Intel’s first processor, the Pentium, was developed in the 1970s. The Pentium was the first computer with a processor chip. CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery. Visit CNN.com/Travel each week for a new gallery of snapshots. Visit the gallery each week to see which photos have been chosen for next week\\'s gallery. We will feature a new photo each week until the end of the year. The world’s first \\xadgeneral-\\u200bgeneral-\\xadpurpose microprocessor. The world\\'s first microprocessor with a built-in ‘microphone’ for voice control. It was developed in the 1960s by the Massachusetts Institute of Technology in Cambridge, Massachusetts. The chip was the first of its kind in the world. This was an 8-bit machine, with an8-bit data path to memory. It was designed to run in the 1980s and 1990s. The machine was called a \"machine\" and was based on the IBM PC operating system. It ran in the 80s and 90s, and was known as the \"Pioneer\" machine. The 8080 was used in the first personal computer, the Altair. The 8080 is still the most popular computer in the world today. It is still used in some of the world\\'s most\\xa0popular\\xa0television shows. It was also used to make the first\\xa0personal\\xa0computer, the\\xa0Altair. 8086: A far more powerful, 16-bit machine. 8086: The world\\'s first computer with a 16- bit processor. 8087: The next generation of the 8086, with a much more powerful processor. The 8086 is now the world\\'s most powerful 16-bits computer. In addition to a wider data path and larger registers, the 8086 sported an instruction cache, or queue, that prefetches a few instructions before they are executed. The 8086 was the first computer to have a built-in 8086 processor. A variant of the 8088 was used in IBM’s first personal computer, securing the suc-cess of Intel. A variant of this pro-986cessor, the 8087, was also used in the Apple II. The 8087 was the first computer to have a built-in floppy drive. The 8086 is the first appearance of the x86 architecture. The 8086 was the first computer to run on x86 processors. It was developed in the 1980s and 1990s by Intel. It ran on the Intel 8086 processor, which was based on the 8085 processor. ■■80286:.■ ■ ■ 80286: A look back at some of the most memorable moments in the history of the U.S. military\\'s involvement in World War II. A look at the events that led to the creation of the United States Air Force. This extension of the 8086 enabled addressing a 16-MB memory instead of just 1\\xa0MB. The 8086 was the first computer with a 16MB address space. It was also the first machine to have a 16\\xa0MB\\xa0addressable\\xa0memory. The 8th\\xa0generation\\xa0of the\\xa0Intel\\xa0processor\\xa0was the first\\xa0to\\xa0address\\xa0a\\xa016-MB\\xa0memory\\xa0space. ■■80386: \\xa0“I’ll be in touch with you soon.” “What’s on your mind?” you ask. “I have a question for you.’’ ‘What do you want to know?’ I’m looking for answers. ‘I want to hear from you. What do you think?‘’ you say. ’I�’ve got a question. What would you like to know?\\' ‘Yes, please tell me what you think’. Intel’s first 32-bit machine, and a major overhaul of the product. The new machine will be called the Intel Xilinx. It will be the company\\'s first machine to run 32-bits. It is expected to go on sale later this year or early next year. With  a 32-bit architecture, the 80386 rivaled the complexity and power of minicom-puters and mainframes introduced just a few years earlier. The 80386 was the first computer with a 32 bit processor. It was developed in the 1980s by Hewlett-Packard, which went on to create the HP-80. This was the first Intel processor to support multitasking, meaning it could run multiple pro-grams at the same time. It was also the first processor to be able to run multiple apps at once. The Intel Core i7 processor is the most powerful processor of its kind. The 80486 introduced the use of much more sophisticated and power-ful cache technology and sophisticated instruction pipelining. The 80486 was the first computer to use the word\\'memory\\' in its name. It was developed in the 1950s and 1960s by the Motorola Corporation. The 80486 also offered a \\xad built-in math coprocessor, offloading complex math operations from the main\\xa0CPU. The 80486 was the first computer to use the\\xa0IBM\\xa0MISC\\xa0processor. It was also the first\\xa0computer\\xa0to use\\xa0the\\xa0Intel\\xa0Core\\xa0programming\\xa0system. Pentium: With the Pentium, Intel introduced the use of superscalar tech-niques, which allow multiple instructions to execute in parallel. The Pentium is the world\\'s first computer with a chip that can run multiple programs at once. It\\'s also the first computer to have a processor that can execute multiple instructions in parallel at the same time. It was introduced in the Intel Pentium II. Pentium Pro continued the move into superscalar organiza-tion begun with the Pentium, with aggressive use of register renaming, branch prediction, data flow analysis, and speculative execution. The Pentium Pro was the first computer with a processor that could run multiple processors at the same time. Pentium II incorporated Intel MMX technology, which is designed specifically to process video, audio, and graphics data efficiently. The Pentium II was the world\\'s first computer with an integrated graphics chip. It was also the first computer to have a built-in modem. Pentium III is the most powerful computer in the world. It was created by the Soviet Union in the 1980s. It has been dubbed the \"world\\'s most advanced computer\" It is the world\\'s most powerful weapon against the human race. It is also the most advanced weapon in the history of computing. The Streaming SIMD Extensions (SSE) instruction set extension added  \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa070 new instructions designed to increase performance when exactly the same operations are to be performed on multiple data objects. The Pentium III incorporates additional \\xad774 floating point instruc-tions. Typical applications are digital signal processing and graphics processing. For more information, visit www.digitalsignal.com and www.graphicsprocessing.com. For a list of other applications, visit the Graphics Processing section of the website. for more information on Graphics Processing, go to Graphics Processing.org. The Pentium 4 is the first Intel x86 microprocessor with a dual core. It includes additional floating-pointing and other features for multimedia. The chip is based on the Intel Pentium 3 processor, which was released in 2007. It is the most powerful processor of its kind. Core 2 extends the Core architecture to 64 bits. The Core 2 is a 64-bit version of the original Core architecture. Core 2 will be available in the U.S. and Europe in the next few months. For more information on the Core 2, visit Core2.com. The Core 2 Quad provides four cores on a single chip. It is the first time a chip has had four cores in the same chip. The chip is based on the Intel Core 2 Duo processor. It has a total of four cores and four threads. It was unveiled at an event in New York City on November 14. More recent Core offerings have up to 10 cores per chip. More than one chip can be used in a single chip at a time. Core chips can also have as many as 10 cores per chip, depending on the chip\\'s chip size and chip type. An important addition to the architecture was the Advanced  vector Extensions instruction set. It provided a set of 256-bit and then 512-bit, instructions for efficient processing of vector data. It was the first version of the instruction set that was available to all processors. It is still used today in many high-performance computers. With the Pentium 4, Intel switched from Roman numerals to Arabic numerals for model numbers. The Pentium 3.11 was the last Pentium model to have Arabic model number. It was the first Pentium to be sold in the U.S. with Arabic model numbers, and the last in the world. The x86 architecture continues to dominate the processor market outside of embedded systems. Almost 40 years after its introduction in 1978, the x86 Architecture continues to dominate the processors market. The x86 Architectures for Embedded Systems is published by Oxford University Press on behalf of the Intel Corporation. Although the organiza-tion and technology of the x86 machines have changed dramatically over the decades, \\xa0the instruction set architecture has evolved to remain backward compatible with ear-lier versions. The x86 architecture is backwards compatible with all versions of the operating system, including Windows and Mac OS X. Any program written on an older version of the x86 architecture can execute on newer versions. This means that programs written on x86 can be run on a newer version of x86. For example, a program written for x86-64 can be written for\\xa0x86-87. All changes to the instruction set architecture have  involved additions to the instructions. There have been no subtractions from the original instruction set. There has been no change to the\\xa0architecture\\xa0of the instruction\\xa0set. There are no\\xa0additions\\xa0to\\xa0the instruction set, with no\\xa0substitutions. The rate of change has been the addition of roughly one instruction per month added to the architecture. There are now thousands of instructions in the instruction set. The most recent version of the ANTH08 instruction set was released on October 1, 2008. It is available for download from the ANth08 website. The x86 provides an excellent illustration of the advances in computer hard-ware over the past 35 years. The x86 is the most popular operating system in the world, followed by the PC, Mac and iPad. It is the only operating system that can run both Windows and Mac OS X. The 1978 8086 was introduced with a clock speed of 5.5\\xa0MHz and had 29,000 transistors. It was the world\\'s first 8-bit computer. The 8086 is still the most popular computer in the world today. It is still used by millions of people worldwide. Core i7 EE 4960X introduced in 2013 operates at 4 GHz, a speedup of a factor of 800, and has 1.86\\xa0billion transistors, about 64,000 times as many as the 8086. Core i7 is a six-core processor. The Core i7 EE 4960X is in only a  \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0slightly larger package than the 8086 and has a comparable cost. The term embedded system refers to the use of electronics and software within a product, as opposed to a general computer such as a laptop or desktop. Millions of computers are sold every year, including laptops, personal comput-ers, workstations, servers, mainframes, and supercomputers. Millions of computers were sold in the U.S. last year. The number of computers sold in 2012 was more than 1.2 billion. In contrast, billions of computer systems are produced each year that are embedded within larger devices.   Each year billions of computer systems are created each year.  They are often embedded in larger devices such as computers, phones and televisions. Many of these systems are used to run software that is then used in larger systems. Today, many, perhaps most, devices that use electric power have an embedded com-puting system. Many of these devices have been developed in the U.S. over the past few years. The Internet of Things is the next step in the evolution of this technology. It is likely that in the near future virtually all such devices will have embedded computing systems. It is also likely that the devices will be able to communicate with each other via the internet. The technology is expected to be available within the next few years. It will also be available on smartphones and tablets. Types of devices with embedded systems are almost too numerous to list. The list is almost too long to list, but here are a few of the most popular. The most popular are the iPad, iPhone, iPad mini, iPad Pro and iPad mini Plus. The iPad Mini has a built-in embedded system, and the iPad Mini 2 has an embedded system. The iPhone 2 has a separate embedded system and the iPhone 5 has an integrated embedded system as well. Examples include cell phones, digital cameras, video cameras, calculators, micro-wave ovens, home security systems, washing machines, lighting systems, ther-mostats, printers, various automotive systems. Tennis rack-ets, toothbrushes, and numerous types of sensors and actuators in automated systems. Often, embedded systems are tightly coupled to their environment. Often, they are closely coupled to the environment they are embedded in. They are often tightly coupled with each other and with their environments. They can be easily connected to each other or to other systems in the same way. This can  give rise to \\xadreal-\\u200b˚reality-time constraints imposed by the need to interact with the environ-mentation. This can give rise to ‘’real-’reality’’, which is a term used to refer to a lack of ‘real’ time. Constraints, such as required speeds of motion, required precision of meas-urement, and required time durations, dictate the timing of software operations. Constraints such as speed of motion and precision of measurement are used by software developers to determine timing of operations. If multiple activities must be managed simultaneously, this imposes more complex \\xa0constrained\\xa0time constraints. If \\xa0multiple\\xa0activities\\xa0must\\xa0be\\xa0managed simultaneously,\\xa0this imposes\\xa0more complex\\xa0time\\xa0 constraints. This is known as multitasking. It\\xa0occurs\\xa0when\\xa0you\\xa0have\\xa0to\\xa0manage\\xa0more than one activity at once. Figure\\xa01.14 shows in general terms an embedded system organization. Figure 1.14: Embedded system organization in terms of a number of different types of systems. Figure 2: Embeddable system organization for an embedded computer system. Figure 3: Embed system organization of an embedded computing system in general. There may be a variety of interfaces that enable the system to measure, manip-ulate, and otherwise interact with the external environment. In addi-tion to the processor and memory, there are a number of elements that differ from  the typical desktop or laptop computer. Embedded systems often interact (sense, manipulate, and communicate) with external world through sensors and actuators. They are typically reactive systems; a \\xa0reactive system is in continual interaction with the environment and executes at a pace determined by that environment. For more information on embedded systems, visit: http://www.cnn.com/2013/01/30/computer-science/electronic-sensors-and- actuators/index.html. The human interface may be as simple as a flashing light or as complicated as robotic vision. The human interface could be assimple as a flash light or more complicated as a robotic vision, says the company. The company is working on a new version of its software to make the human interface more intuitive and intuitive. In many cases, there is no human interface. In some cases there is even no human interaction at all. In others, there are no human interfaces at all in some cases. In other cases, the human interface is not present at all, and there is only a computer interface. The diagnostic port may be used for diagnosing the system that is being controlled. The port may also be used to monitor the health of the system. The diagnostic port is not just used to diagnose the computer. It is also used for monitoring the system’s health. Special-\\u200bpurpose field programmable (FPGA), or even nondigital hardware may be used to increase performance or reliability. The term \"special\" is used to refer to hardware that can be used in a variety of ways to improve performance and reliability. For more information, visit:\\xa0http://www.cnn.com/2013/01/30/technology/special-purpose-field-programmable-gadget-gadset. Software often has a fixed function and is specific to the application. It can also have a different function depending on the application\\'s needs. Software often has to be customised to suit the needs of a specific application. Software can also be customized to suit different applications\\' needs. Efficiency is of paramount importance for embedded systems. Efficientness is key to the success of embedded systems, says the company. The company is focusing on improving the efficiency of its embedded systems in a variety of ways. For more information, visit the company\\'s website or go to: http://www.integratedsystems.com. They are opti-mized for energy, code size, execution time, weight and dimensions, and cost. They are also opt-in for size, size, weight, dimensions and cost, as well as code size and size. They can also be opt-out for cost, size and weight, and size and dimensions. Even with nominally fixed function software, the ability to field upgrade to fix bugs, to improve security, and to add functionality, has become very important for embedded systems. There are several noteworthy areas of similarity to general-\\u200bgeneral-\\u200bpurpose computer systems as well. One comparatively recent development has been of embedded system plat-forms that support a wide variety of apps. One of these is the Apple iPad Pro, which has a built-in touch screen. The iPad Pro has a touch screen that can be used to control a range of different apps. Good examples of this are smart-phones and audio/visual devices, such as smart TVs. Smart TVs and phones can be used to control music, video and other content. They can also be used as part of a larger entertainment system, like a smart phone or TV. It is worthwhile to separately callout  one of the major drivers in the \\xadproliferation of embedded systems. The Internet of Things is a major driver in the  proliferation of embedded systems. It is also a driver of the growth of the internet of things. The Internet of things (IoT) is a term that refers to the expanding network of smart devices. Smart devices range from appliances to tiny sensors. The term is also used to refer to connected cars and other vehicles, as well as smart phones and other mobile devices that connect to the Internet. A domi- \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0nant theme is the embedding of mobile transceivers into a wide array of gadgets and everyday items. It enables new forms of communication between people and things, and between things themselves. The project is being led by the University of California, San Diego. The Internet now supports the intercon-nection of billions of industrial and personal objects, usually through cloud systems.   The Internet is now used to connect billions of objects.  It is also used to communicate with each other via the cloud. It is used to share information between people and businesses. The objects deliver sensor information, act on their environment, and, in some cases, modify themselves. The objects can be used to create overall management of a larger system, like a factory or city. The technology is being developed by a group of researchers at the University of Cambridge. The IoT is primarily driven by deeply embedded devices (defined below)  The Internet of Things (IoT) is a network of embedded devices that connect to the Internet. The IoT is driven by devices that are deeply embedded in the internet of things (IOT) These devices are. appliances that communicate with each other and provide data via user.interfaces. These devices are \\xad \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0low-\\u200b worrisome\\xadbandwidth, \\xad \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 low-\\u200b grotesque\\xadrepetition, and \\xad \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 high-definition data-\\u200bcapture. Embedded appliances, such as security cameras and VoIP phones, require bandwidth streaming capabilities. These appliances can be used to stream video to a TV or computer. For more information on embedded appliances, go to: http://www.cnn.com/2013/01/16/technology/embedded-appliance-video-cameras-video. Yet countless products simply require packets of data to be intermit-tently delivered. This is the case with the iPhone, iPad, and many other mobile devices. The iPhone is the first of its kind to be able to deliver data in real-time to the user\\'s mobile device. The Internet has gone through roughly four generations of deployment culminating in the IoT. Information technology (IT): PCs, servers, routers, firewalls, and so on, bought as IT devices by enterprise IT people and primarily using wired connectivity. The Internet of Things (IoT) is the next generation of this technology. 2. Operational technology (OT): Machines/appliances with embedded IT built by medical machinery, SCADA, process control, and kiosks, bought as appliances by enterprise OT people and primarily using wired connectivity. 2.2. Embedded IT: Software and services that are embedded into the machine, such as medical machinery. 3. Personal technology: Smartphones, tablets, and eBook readers bought as IT devices by consumers (employees) exclusively using wireless connectivity and often multiple forms of wireless connectivity. 4. Business technology: Mobile phones, tablets and e-readers used for business purposes. Single-\\u200bpurpose devices bought by consumers, IT, and OT people exclusively using wireless connectivity. They are generally of a single form, as part of larger systems. 4.4.5: Sensor/actuator technology: \\xadsingle-use devices with a single function. It is the fourth generation that is usually thought of as the IoT, and it is marked by the use of billions of embedded devices. It is also the first time that the Internet of Things has been used in this way. It will be used to connect people, businesses and even the environment. There are two general approaches to developing an embedded operating system (OS) There are two ways to develop an embedded system. There are also two types of embedded operating systems that can be used in different ways. For more information on embedded systems, see Embedded Operating Systems and Embedded Software. The first approach is to take an existing OS and adapt it for the embedded application. The second is to adapt an existing operating system to an embedded application. The third is to create a new operating system for an embedded device. The fourth is to develop a new OS for an embeddable device. For example, there are embedded versions of Linux, Windows, and Mac, as well as other commercial and proprietary operating systems specialized for embedded systems. For example, for embedded systems there are versions of Windows, Linux, and Mac OS X. For embedded systems, there is an embedded version of Linux. The other approach is to design and implement an OS intended solely for embedded use. For example, an operating system could be designed to run on an embedded device. For more information, visit the OS for embedded devices website. For information on the OS to embedded devices, visit\\xa0the OS for embeddable devices\\xa0 website. An example of the latter is TinyOS, widely used in wireless sensor networks. TinyOS is an open source, free software that can be downloaded from the Internet. It can be used to build, test, and share data with other devices. It is also used in the development of mobile phones and other mobile devices. This topic is explored in depth in [STAL15]. We hope to see you in the next edition of the journal. Back to the page you came from. Follow us on Twitter @stal15 and @jenniferjensen. We hope you\\'ll join us for the next issue. Application Processors versus Dedicated Processors in this subsection. In this subsection, and the next two, we discuss the difference between application processors and dedicated processors. We also discuss the differences between application processes and dedicated processes. In the next section, we look at the differences in the two types of processes. , we briefly introduce some terms commonly found in the literature on embedded systems. We also briefly discuss some of the most commonly used terms in embedded systems literature. We conclude with a list of some of our favorite examples of embedded systems terms. We hope this will help you understand some of these terms better. Application processors are defined  by the processor’s ability to execute complex operating systems, such as Linux, Android, and Chrome. Application processors are used to run applications on Linux, Android, and other operating systems. They are also used to execute applications on Windows, Mac OS X, and iOS. Thus, the application processor is \\xadgeneral-\\u200bpurpose in nature. It is not meant to be used for anything other than its intended purpose. It can only be used in certain situations, such as in the application of an application to an existing application. It cannot be used to create an entirely new application. A good example of the use of an embedded application processor is the smartphone. The smartphone can be used to run applications on the go. It can also be used as a way to send data to other devices. It is possible to use an embedded processor to run apps on the phone. The embedded system is designed to support numerous apps and perform a wide variety of functions. The embedded system was designed to be able to run a wide variety of apps. It was developed by the same team that developed the iPhone and iPad. It has been designed to work with a wide range of software and hardware. Most embedded systems employ a dedicated processor, which, as the name implies, is dedicated to one or a small number of specific tasks required by the host device. Most embedded systems use processors that are dedicated to a single task or a set of tasks. For more information on embedded systems, visit the embedded systems section of this website. Because such an embedded system is dedicated to a specific task or tasks, the processor and associated components can be engineered to reduce size and cost. For example, a computer could be programmed to run a specific program. For more information on how to build an embedded computer, visit: www.techcrunch.com/embedded-computer. Microprocessors versus Microcontrollers. Early microprocessor chips included registers, an ALU, and some sort of control unit or instruction processing logic. Later microprocessors included registers and ALU. Later, microcontrollers included register, ALU and other logic. As transistor density increased, it became possible to increase the complexity of the instruction set architecture. It was also possible to add memory and more than one processor to a computer. The first computers were built in the 1950s and 1960s. They were powered by a variety of semiconductor devices, including transistors. Contemporary micropro-cessor chips, as shown in Figure\\xa01.2, include multiple cores and a substantial amount of cache memory. Figure 1.2: Micropro-centric chips with multiple cores. Figure 2: Microprocessor chips with cache memory with a large number of cores. A microcontroller chip makes a substantially different use of the logic space available. A microcontrollerchip makes a significantly different Use of the Logic Space to a conventional chip. A chip can be used in a variety of ways, depending on how it is programmed. For more information on microcontrollers, visit\\xa0www.microcontrollers.co.uk. Figure\\xa01.15 shows in general terms the elements typically found on a microcontroller chip. Figure 1.15 is a list of the elements that make up a microcontroller. The elements are:\\xa0C,\\xa0D,\\xa0E,\\xa0F,\\xa0G,\\xa0H,\\xa0I,\\xa0J,\\xa0K,\\xa0L,\\xa0M,\\xa0N,\\xa0P,\\xa0Q,\\xa0R,\\xa0S,\\xa0T,\\xa0U,\\xa0V,\\xa0W,\\xa0Z,\\xa0Y, Z, Z. As shown, a microcontroller is a single chip that contains the processor, volatile memory for the program, a clock, and an I/O control unit. A microcontroller can also be used to control a computer’s graphics or other features. For more information on microcontrollers, see the Microcontrollers section of this article. The processor portion of the microcontroller has a much lower silicon area than other microprocessors. The processor has much higher energy efficiency. The chip is used in a variety of applications, including the Internet of Things. It is also used to power smart phones and other mobile devices. Also called a “computer on a chip,” billions of microcontroller units are embedded each year in myriad products from toys to appliances to automobiles. We examine microcontroller organization in more detail in Section\\xa01.6 of this report. We\\xa0examine\\xa0the role of microcontrollers in the development of the Internet of Things. For example, a single vehicle can use 70 or more microcontrollers. For example, a car can have up to 70 microcont controllers in the engine compartment. For more information, visit www.motorola.com. For additional information on how to use a microcontroller in a car, visit\\xa0www.mobile.org. Typically, especially for the smaller, less expensive microcontrollers, they are used as dedicated proces-sors for specific tasks. For example, a microcontroller can be used to control a computer\\'s display. For more information, visit the Microchip Institute\\'s website. Microcontrollers are heavily utilized in automa-tion processes. For example, microcontrollers can be used to control a car\\'s engine. Microcontrollers also play a key role in the development of the Internet of Things (IoT) and cloud computing. By providing simple reactions to input, they can control machinery, turn fans on and off, open and close valves, and so forth. They can also control machinery by using simple reactions, such as turning fans on or off, or opening and closing valves, for example. They are integral parts of modern industrial technology. They are among the most inexpensive ways to produce machinery that can handle extremely complex functionalities. They can also be used to test the performance of new technology. For more information on how to use them, visit the manufacturer’s website. Microcontrollers come in a range of physical sizes and processing power. Microcontrollers can be used to control computers, phones, and other devices. They can also be used for personal computers, tablets, and smartwatches. For more information on microcontrollers, visit microcontroller.com. Pro-cessors range from 4-bit to 32-bit architectures. They can be used on a variety of platforms, from PCs to servers. They are designed to run on a wide range of processors and processors. They range in size from 2GB to 4GB to 6GB. Microcontrollers tend to be much slower than microprocessors. They are typically operating in the MHz range rather than the GHz speeds of microprocessers. Microcontrollers are often used in embedded systems, such as medical devices. They can also be used in the home to control computers. Another typical feature of a microcontroller is that it does not provide for human interaction. Another typical feature is that it does not provides for human\\xa0interaction\\xa0with the microcontroller. It does not\\xa0provide\\xa0for human interaction with other microcontrollers. The microcontroller is programmed for a specific task, embedded in its device, and executes as and when required. The microcontroller can be programmed for any task at any time. It can also be programmed to run on any of a number of different\\xa0programs\\xa0at any given time. We have, in this section, defined the concept of an embedded system. Embedded versus Deeply Embedded Systems is the difference between an embedded and a deeply embedded systems. The difference between the two can be seen by looking at the following diagram. The diagram below shows the differences between embedded and deeply embedding systems. A subset of embedded systems, and a quite numerous subset, is referred to as deeply embed-ded systems. A subset of deeply embedded systems is known as deeply embedded systems. The term is used to refer to systems that are deeply embedded in other systems. Although this term is widely used in the technical and commercial 1.6 / ARM Architecture’s literature, you will search the Internet in vain (or at least I did) for a straightfor-ward definition. The term is used to refer to a set of software tools that can be used to build computers. Generally, we can say that a deeply embedded system has a proces-sor whose behavior is difficult to observe both by the programmer and the user.  A deeply embedded systems has a Proces-Sor that is hard to observe by the user and the programmer. A deeply embedded system uses a microcontroller rather than a microprocessor. It is not programmable once the program logic for the device has been burned into ROM. It has no interaction with a user and is not\\xa0programmable\\xa0in the traditional sense. It can be used to run programs on a computer without the need for a computer. Deeply embedded systems are dedicated, \\xadsingle-\\u200bpurpose devices that detect something in the environment, perform a basic level of processing, and then do some- \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0thing with the results. These systems are often used in the military, government, and other industries. Deeply embedded systems often have wireless capability and appear in networked configurations. Networks of sensors deployed over a large area (e.g., factory, agricultural field) can be used to communicate with each other. For more information on embedded systems, visit:\\xa0www.cnn.com/2013/01/26/technology/deeply-embedded-systems-wireless. The Internet of things depends heavily on deeply embedded systems. The Internet of Things is a network of connected devices that communicate with each other. It\\'s a network that needs to be deeply embedded to work properly. It needs to work in tandem with other systems to be fully functional. The ARM architecture refers to a processor architecture that has evolved from RISC design principles. It is used in embedded systems and has extreme resource con-straints in terms of memory, processor size, time, and power consumption. The architecture is based on the RISC-V instruction set processor.  chapter 15 examines RISC design principles in detail. Chapter 15 examines RISC design Principles in detail, including the design of the RISC-V instruction set. The design principles are described in more detail in the next section of this article. For more information on RISC processors, visit RISC.org. In this section, we give a brief overview of the ARM architecture. In the next section we will look at some of the features that make up the architecture. We will then go on to look at the design and development of the software. The next section will be about the software and its features. ARM is a family of microprocessors and microcontrollers designed by Cambridge-based CambridgeARM Holdings, England.ARM is based on RISC-RISC-based microprocessers and micro Controllers.ARM was developed by CambridgeARM in the 1980s and 1990s. The company doesn’t make processors but  designs microprocessor and multicore architectures and licenses them to man-ufacturers. The company doesn\\'t make processors, but instead designs microprocessors and multicores. It also designs and licenses these microprocesses to other man-manufacturers.  ARM Holdings has two types of licensable products: proces-sors and processor architectures. The company\\'s products include processors, processors, and chips that can be used in smartphones, tablets, and other devices. ARM\\'s products can also be used to develop software and services. For processors, the customer buys the rights to use the design in their own chips. For chip makers, the rights are sold to other chip makers to use in their products. For more information, visit the chip maker\\'s website or go to www.chipmakers.com. For a processor architecture, the customer buys the rights to design their own processor compliant with ARM’s architecture. For a processor\\xa0architecture, the\\xa0customer\\xa0buys\\xa0the rights\\xa0to design their\\xa0own processor compliant\\xa0with ARM\\'s architecture. The\\xa0customers\\xa0can\\xa0design\\xa0their\\xa0own\\xa0processor compliant with\\xa0ARM’S architecture. ARM chips are \\xadhigh-\\u200bhighest-speed processors that are known for their small die size and low power requirements. ARM chips are used in smartphones, tablets, PCs and servers. They are designed to run on low-power, low-voltage sources of electricity. They can also be used to power computers. They are widely used in smartphones and other hand-held devices, including game systems, as well as a large variety of consumer prod-ucts. They are also used in a large number of consumer products, such as game systems and consumer electronics. They can also be used to play video games on a PC or Mac. ARM chips are the processors in Apple’s popular iPod and iPhone devices, and are used in virtually all Android smartphones as well. ARM chips are also used in almost all Android phones and tablets as well as the iPad and iPhone 4S, 4C, 5 and 6. ARM is probably the most widely used embedded processor architecture. It is the most widely used processor architecture of any kind in the world. It has been used in everything from mobile phones to the Internet of Things (IoT) to the iPhone and iPad. It\\'s also used in Apple\\'s iOS and Android mobile operating systems. [VANC14]. VANC14.com will feature iReporter photos in a weekly Travel Snapshots gallery. Please submit your best shots of Vancouver for next week. Visit Vancouver.com/Travel next week for a new gallery of snapshots. For more information, visit Vancouver.gov. The origins of ARM technology can be traced back to the \\xadBritish-\\u200b \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xadbased Acorn Computers company. The technology is now used in a range of products, including smartphones, tablets and computers. It has also been used in the iPhone, iPad and Mac computers. In the early 1980s, Acorn was awarded a contract by the Brit-ish Broadcasting Corporation (BBC) to develop a new microcomputer architecture. Acorn\\'s architecture was used for the BBC Computer Literacy Project. It was the first microcomputer of its kind. The success of this contract enabled Acorn to go on to develop the first commercial RISC processor, the Acorn RISC Machine (ARM) The ARM was the first processor to run on the\\xa0RISC\\xa0Chipset. Acorn also developed the RISC-V processor, which was used in the BBC iPlayer. The first version, ARM1, became operational in 1985. It was used for  internal research and development as well as being used as a coprocessor in the  BBC machine. ARM1 was the first version of the ARM microprocessor to be\\xa0operational\\xa0in the UK. In this early stage, Acorn used the company VLSI Technology to do the actual fabrication of the processor chips. Acorn\\'s processor chips were made using a process known as chip-in-die (BIG) technology. The processor chips are used to power the computer\\'s processors. VLSI was licensed to market the chip on its own  and had some success in getting other companies to use the ARM in their products, particularly as an embedded processor. VLSI has also had success with other chip makers, such as Texas Instruments and Texas Instruments. The ARM design matched a growing commercial need for a processor for embedded appli- \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0cations. It was designed to be small, small-sized, and low-cost for embedded applications. The ARM design was developed to meet growing demand for a low-power, small size processor. But further development was beyond the scope of Acorn’s capabilities. ‘We’ve got a lot of work to do. We’re not going to give up.’ ‘I’m not giving up. I’ll keep working on it,’ he said. Accordingly, a new company was organized, with Acorn, VLSI, and Apple Com-puter as founding partners. The new company is known as ARM Ltd. and is based in Cambridge, Massachusetts. ARM was founded by Steve Wozniacki, the former CEO of Apple. The ARM instruction set is highly regular, designed for efficient implementation of the processor and efficient execution. The Acorn RISC Machine became Advanced RISC Machines. 12.2 is the most\\xa0advanced\\xa0RISC Machine to date. It has an instruction set architecture that is based on the instruction set of the ARM processor. All instructions are 32 bits long and follow a  regular format. Use these instructions to help you with your reading comprehension and vocabulary. Use the weekly Newsquiz to test your knowledge of stories you saw on CNN.com or the CNN iReport app for a chance to win a prize. This makes the ARM ISA suitable for implementation over a wide range of products. The ISA is designed to be used in a variety of products, including smartphones, tablets, and computers. It is intended to be\\xa0implementable\\xa0over a wide\\xa0range\\xa0of products. Augmenting the basic ARM ISA is the Thumb instruction set, which is a \\xadre-\\u200b \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xadencoded subset of the ARM instruction set. The Thumb instructions are intended to be more flexible than the ARM instructions. They can be used in conjunction with other instructions to create new programs. Thumb is designed to increase the per-formance of ARM implementations that use a 16-bit or narrower version of the ARM architecture. Thumb is based on an open-source version of ARM that uses 16-bits or narrower versions of the processor. The software is available now for download from the Apple App Store. memory data bus, 12. The company dropped the designation Advanced RISC Machines in the late 1990s. It was replaced by the name RISC-V in the mid-2000s. The name was changed to RISC V in the early 2000s, and then RISC\\xa0V in 2007. It is now simply known as the ARM architecture. It is based on the\\xa0architecture\\xa0of the ARM microprocessor. The ARM architecture was designed in the 1990s. It was designed to be able to run multiple processors at the same time. It can now run up to four processors at once. 1.6 / ARM Architecture’s ‘35’ and ‘36’ instruction sets. Designed to allow better code density than provided by the ARM instruction set. 1.6 ’35 and ’36 ’s’ ‘44’ instructions are the same as the ARM instructions. The Thumb instruction set contains a subset of the ARM 32-bit instruction set recoded into 16-bit instructions. It is used in the Apple II, III, IV, V, VII, VIII and X-ray processors. It was created to replace the 32- bit ARM instruction set. The current defined version of the ARM is \\xad \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0Thumb-\\u200b \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xad\\xa0\\xa0\\xa0\\xa02. The ARM and \\xad\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa02 ISAs are discussed in Chapters\\xa012 and 13. The current definition of the ISAs is \\xad\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa01,\\xa02, and\\xa03. ARM Holdings licenses a number of specialized microprocessors and related tech-nologies. The bulk of their product line is the Cortex family of microprocessor architectures.ARM products include the Cortex-M, Cortex-R, and Cortex-A7 processors. There are three Cortex architectures, conveniently labeled with the \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0initials A, R, and\\xa0M. The Cortex architectures are intended for mobile devices such as smartphones and eBook readers, as well as consumer devices like digital TV and home gateways (e.g., DSL and Internet modems) These processors run at higher clock frequency (over 1 GHz), and support a memory management unit (MMU) This is required for full feature OSs such as Linux, Android, MS Windows, and mobile OSs. These processors run  over  1 GHz. An MMU is a hardware module that supports virtual memory and paging by translating virtual addresses into physical addresses. This topic is explored in Chapter\\xa08. of the book. An MMU can also be used as a way to connect to a web browser. For more information on MMU\\'s, see the Wikipedia page. The two architectures use both the ARM and \\xad \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0Thumb-\\u200b encompasses-2 instruction sets. The \\xadCortex-\\u200b \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0R is designed to support real-time applications, in which the timing of events needs to be controlled with rapid response to events. It is a 32-bit machine, and the \\xadC Vortex-\\u200b\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0A50 is a 64- bit machine. They can run at a fairly high clock frequency (e.g., 200MHz to 800MHz) and have very low response latency. They can also be used to send and receive data from a mobile phone. They have been used in the film \"The Godfather\" and the movie \"The Hobbit\" Cortex-R includes enhancements both to the instruction set  and to the processor organization to support deeply embedded devices. It is intended to be used in the development of the next generation of the Internet of Things (IoT) system. It has been dubbed the ‘Internet of Things’ system. Most of these processors do not have MMU. The limited data requirements and  limited number of simultaneous processes eliminates the need for elaborate hardware and software support for virtual memory. Most of these processor families are based on the Haswell processor family of processors. These processors have no MMU and do not need to be equipped with virtual memory support. The \\xadCortex-\\u200b \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0R does have a Memory Protection Unit (MPU), cache, and other memory features designed for industrial applications. It is designed to be used in a variety of industrial applications, such as computers, cars, trucks, and aircrafts. An MPU is a hardware module that prohibits one program  from accidentally accessing memory assigned to another active program. An MPU can also be used to prevent a program from accessing another program\\'s memory. MPU\\'s can be used in computers, phones, tablets, and other devices. Using various methods, a protective boundary is created around the program, and instructions within the program are prohibited from referencing data outside of that boundary. For example, a program can only access data that is within that boundary. For more information on how to create a program like this, go to: http://www.cnn.com/2013/01/30/programs/how-to-create-a-program-like-this. The \\xadCortex-M series processors have been developed primarily for the microcontroller domain. They are designed for the need for fast, highly deterministic interrupt management and low power consumption. Examples of embedded systems that would use them are automotive braking systems, mass storage controllers and networking and printing devices. As with the \\xad \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0Cortex-\\u200b \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0R series, the \\xad\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0Cortex\\xa0M series has an MPU but no\\xa0MMU. As with the Cortex-M series, there is no MPU orMMU in the \\xad\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0M\\xa0architecture\\xa0series. The \\xad\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0Cortex-\\u200b \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xad \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0M uses only the \\xad \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0Thumb-\\u200b\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa02 instruction set. The \\xad \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0Thumb\\xa0 is a set of two fingers and a thumb. The fingers are connected by a ring. The thumb is connected by two fingers. There are currently four versions of the \\xad \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0Cortex-\\u200b \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0M series. The market includes IoT devices, wireless sensor/actuator networks used in factories and other enterprises, automotive  body electronics, and so on. There are four versions: 1, 2, 3, and 4. It is optimized for small silicon die size (starting from 12k gates) and use in the lowest cost chips. It is designed to be used in the next generation of microprocessors. It can also be used to make low-cost chips in the future. M0+: An enhanced version of the M0 that is more energy efficient. M0: A new version of M0 designed to be more energy-efficient. M1: An upgraded version of an M0 with a more powerful processor. M2: An M0-like version with a new processor designed to make it more powerful. Cortex-M3 is designed for 16- and 32-bit applications. It emphasizes performance and energy efficiency. It is available in two versions: M2 and M3. For more information on the M3, go to:\\xa0http://www.m3.com/. It also has comprehensive debug and trace features to enable software developers to develop their applications quickly. It is available in a range of sizes, from 32GB to the 64GB version of the software. The software can be downloaded from the company\\'s website. It can also be pre-ordered on the company’s website. M4: M4: \"M4\" is an acronym for theortex. M4 is a type of human brain. It is made up of a number of different types of nerve cells. The M4 brain is made of four types of cells: neurons. This model provides all the features of the \\xad \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0Cortex-\\u200b \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xadM3, with addi-tional instructions to support digital signal processing tasks. This model is available now in the U.S. and Europe. For more information on this model, visit the manufacturer\\'s website. In this text, we will primarily use the ARM \\xadCortex-\\u200b \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xadM3 as our example embed-ded system processor. We will use this processor for our example embedded system processor, the ARM Cortex-M3. In this text we will use the Cortex-A3 processor as an example embedded processor. It is the best suited of all ARM models for \\xadgeneral-\\u200bpurpose microcontroller use. It is also the most powerful of the ARM-powered chips. It can be used to build smartphones, tablets, computers, and even cars. It has a range of ARM chips that can be programmed in different ways. The \\xadCortex-\\u200b \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xadM3 is used by a variety of manufacturers of micro-controller products. It can be used to control a number of different types of microcontroller products, such as a PC, phone, tablet, and even a car. It is also used to test the performance of different chips. Initial microcontroller devices from lead partners already exist. They combine the \\xadCortex-\\u200bM3 processor with flash, SRAM, and multiple peripherals to provide a competitive offering at the price of just $1. For more information, visit the company\\'s website or go to: http://www.cortex-m3.com. Figure\\xa01.16 provides a block diagram of the EFM32 microcontroller from Sil-icon Labs. The microcontroller was developed by Sil-Icon Labs and is based on the NXP SIX microcontroller. It has been used in a number of high-end smartphones and tablets. The figure also shows detail of the \\xad \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0Cortex-\\u200b \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xadM3 processor and core com- \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0ponents. The figure also includes detail on the processor\\'s\\xa0\\xa0core\\xa0com- \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0Ponents and\\xa0memory\\xa0systems. For more information on the iWatch, visit iWatch\\'s website. We examine each level in turn. We begin with the lowest level, then move up to the highest level. We then examine the next level, which is the highest. We conclude with the last level, the highest one, and then the lowest one. We examine the levels in turn, starting at the bottom. The \\xadCortex-\\u200b \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xad\\xadM3 core makes use of separate buses for instructions and data. The core is designed to be able to run on a range of processors. The Cortex-M3 uses a number of different processors to run the core. It can also run on different processors for different types of data. This arrangement is sometimes referred to as a Harvard architecture, in contrast with the von Neumann architecture, which uses the same signal buses and mem-ory for both instructions and data. The Harvard architecture is also known as the \"Harvard\" architecture, after Harvard University. By being able to read both an instruction and  data from memory at the same time, the \\xadCortex-\\u200b \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xadM3 processor can perform many operations in parallel, speeding application execution. The processor can also perform many operations in parallel to speed up execution. The core contains a decoder for Thumb instructions, an advanced ALU with support for hardware multiply and divide, control logic, and interfaces to the other components of the processor. The core also contains a\\xa0control\\xa0logic\\xa0and a control logic\\xa0interfaces\\xa0to other components. In particular, there is an interface to the nested vector interrupt controller (NVIC) and the embedded trace macrocell (ETM) module. There is also an interface  to the embedded trace microcell module. The software is available on the Google Play Store and the Apple App Store. The core is part of a module called the \\xadCortex-\\u200b \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xadM3 processor. The processor is used to power the Apple iPhone 5 and 5S. The iPhone 5 is expected to go on sale later this year in the U.S. and in the UK. This term is somewhat misleading, because typically in the literature, the terms core and pro-cessor are viewed as equivalent. This term is used to refer to a group of people who work together on a single project. For more information, visit\\xa0www.cnn.com/2013/01/30/science/science-and-technology/core-processor.html. In addition to the core, the processor includes the following elements:NVIC: Provides configurable interrupt handling abilities to the processor. The processor also includes the processor’s\\xa0“core” and “die” elements, as well as its\\xa0”die’ and\\xa0\\u2009”’“” components, which include the “core,” ”die,’ ””, \\u2009’, and �\\xa0‚” processors. It facilitates low-low-low exception and interrupt handling, and controls power management. It also controls power\\xa0management\\xa0and\\xa0power\\xa0efficiency. It\\xa0facilitated\\xa0low-Low-Low Exception and Interrupt Handling. It controls power management and power\\xa0efficiency\\xa0and power\\xa0performance. ETM: An optional debug component that enables reconstruction of program execution. EMT can be used to help developers understand the state of the program. ETM can also be used as a tool to help programmers understand program\\xa0execution\\xa0after a bug has been detected. The ETM is designed to be a high-speed, low-power debug tool. It only supports instruction trace. The ETM can be used as a debug tool or as a command-line tool. For more information on the ETM, visit:\\xa0www.etm.org. Debug access port (DAP) is used to access ports on a computer. DAP can be used to connect to a number of different types of ports. The port can be accessed by any port on the computer. For more information on DAP, visit: http://www.dap.org/. This provides an interface for external debug access to the processor. It is used to test and troubleshoot software on the PC. It can also be used as a way to test software on a PC\\'s graphics card. It has been used by Apple for its Mac OS X software for years. Basic debug functionality includes processor halt, single-step step, core register access, unlimited software breakpoints, and full system memory access. Basic debug logic includes processor. halt,single-step,processor core. register access and unlimited software. breakpoints. Basic. debug logic: processor halt. Single-step. Step. Core register access. Cortex-M3 Core is a typical Microcontroller Chip. It is based on the Cortex-M2 microcontroller chip. The chip has an interface called the \"ICode\" interface. It can be used to communicate with other microcontrollers. It also has the \"Bus\" interface, which is used to control the bus. Fetches instructions from the code memory space. Fetching information from the memory space is a key part of the code. The code is written in the form of a series of letters and numbers. The letters are then translated into code using the code language of the program. SRAM & peripheral interface: Read/write interface to data memory and peripheral devices. SRAM and peripheral interface is used to communicate with data memory. It is used for data memory, peripheral devices, and other data storage. It can also be used to transfer data from one device to another. Bus matrix: Connects the core and debug interfaces to external buses on the microcontroller. Connects to the microcontroller\\'s\\xa0core\\xa0and\\xa0 debug\\xa0interfaces to external\\xa0buses. Connected to the\\xa0microcontroller\\xa0\\'s core\\xa0and debug interfaces\\xa0to external buses. Memory protection unit: Protects critical data used by the operating system. Protects data from user applications by disallowing access to each other’s data. Detects unexpected memory accesses that could potentially break the system. Disables access to memory regions, allowing memoryregions to be defined as \\xad read-only. The upper part of Figure\\xa01.16 shows the block diagram of a typical micro-controller built with the \\xadCortex-\\u200b \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xadM3. In this case the EFM32 microcontroller is used. The lower part of the diagram shows the basic design of a microcontroller. This microcontroller is marketed for use in a wide variety of devices, including energy, gas, and water metering; alarm and security systems; industrial automation devices; home automation devices. It is also used in smart accessories; and health and fitness devices. The sil- \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0icon chip consists of 10 main areas:Core and memory: This region includes the \\xadCortex-\\u200b\\xad \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0M3 processor, static RAM  \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0(SRAM) data memory,14 and flash memory15 for storing program instructions and nonvarying application data. Flash memory is nonvolatile (data is not lost when power is shut off) and so is ideal for this purpose. Flash memory can be used to store data in a variety of ways. It can also be used as a backup for data stored on a hard drive. The SRAM stores variable data. The SRAM is used to store data on the state of the device. It can also store historical data, such as the date and time of an event. It is also used to record historical data such as time, date, and time. This area also includes a debug interface, which makes it easy to reprogram and update the system in the field. The system also includes an easy-to-use web interface, making it easy for users to access the system\\'s settings. The software can also be used to test and test the software in real time. Parallel I/O ports: Configurable for a variety of parallel I-O schemes. I-Pads can be used to connect multiple computers at the same time. The system can also connect up to four computers at a time. It can also be used as a hub to connect to other computers at once. Serial interfaces: Supports various serial I-O schemes. Supports variousserial I-o schemes. Can be used to connect to the Internet via the Internet Protocol (IP) network. Can also be used as a way to communicate with other computers. Can connect to other computers via the internet using the internet protocol (IP). Analog interfaces: \\xad˚Analog-\\u200b\\xad\\xad \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0to-\\u200b \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0digital and \\xad \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0digital-\\u200b˚analog logic to support ˚sensors and actuators. Analog\\xa0logic\\xa0to support \\xa0“analog\\xa0interfaces’:\\xa0‚’Analog logic\\xa0to\\xa0support\\xa0‘analog’ logic to\\xa0support “digital” logic. Timers and triggers: Keeps track of timing and counts events, generates out-put waveforms, and triggers timed actions in other peripherals. Out-put Waveforms: Generates waveforms that can be used to create new waveforms. Waveforms can also be used in other ways to generate new waveform data. Clock management: Controls the clocks and oscillators on the chip. Clock management: controls the clocks, oscillators and other features of the chips. Clock control: Controls the clock, oscillator, and other parts of the chip\\'s clock. Clock Management: controls clock, clock, and oscillator controls. Multiple clocks and oscillators are used to minimize power consumption and provide short startup times. Multiple clock and oscillator systems are used in a variety of ways to reduce\\xa0power\\xa0consumption\\xa0and\\xa0startup\\xa0times. Multiple clocks and\\xa0oscillators\\xa0are used to\\xa0minimize\\xa0power consumption and\\xa0provide\\xa0short\\xa0startups. Energy management: Manages the various modes of operation of the processor and peripherals to provide \\xadreal-\\u200breal-“real”\\xa0time management of the energy needs so as to minimize energy consumption. It manages the various low- and high-low-energy modes of operations. The chip includes a hardware implementation of the Advanced Encryption Standard (AES) The chip also includes a software version of the OpenSSL encryption standard. The chip is expected to go on sale in the U.S. in the first half of 2015. It will be available for pre-order on the company\\'s website. 13This discussion does not go into details about all of the individual modules. For the interested reader, an EFM32G200.pdf discussion is provided in the document EFM 32G200, available at box.com/COA10e. \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa014Static RAM (SRAM) is a form of cache memory used for cache memory. 32-bit bus: Connects all of the components on the chip. 32-bit chip: Connecting all the components of the chip on one chip. 64-bit processor: Allows for 64-bits of data to be stored on the same chip. 128-bit processors allow for 128-bits on a single chip. Peripheral bus is a network which lets the different peripheral module commu-nicate directly with each other without involving the processor. It is used by Apple\\'s Macbook Pro and Macbook Air. It lets users control their Macbook with a range of different operating systems. This supports \\xadtiming-\\u200bcritical operation and reduces software overhead. This supports  \\xa0“timing’-\\u200b”critical operation’s\\xa0\\u2009“” and ””\\xa0”’”, ‚’, \\u2009\\u2009, “‚‚, ’’ and \\u2009,’ ‚,” “, ”,\\xa0‘’.’\\xa0’It supports  “Timing”-”timing.”. “This supports\\xa0‚\\'”: “The ability to “time” a computer to run a program in a certain time frame.“. ”: The ability to time a program to run in a particular time period. \\u2009: The capability to time it in a specific way. ‚: The possibility to time Comparing Figure\\xa01.16 with Figure 1.2, you will see many similarities and the same general hierarchical structure. Comparing Figure 1.16 and Figure\\xa02.1, you can see the same general structure and many similarities between the two. Note, however, that the top level of a microcontroller computer system is a single chip, whereas for a multicore com-puter, the toplevel is a motherboard containing a number of chips. For example, a\\xa0microcontroller\\xa0com-puter may have a single processor chip, but a\\xa0multicore computer system may have multiple chips. Another note-worthy difference is that there is no cache, neither in the \\xadCortex-\\u200b \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xad \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0M3 processor nor in the microcontroller as a whole. This plays an important role if the code or  data resides in external memory, for example. Though the number of cycles to read the instruc-tion or data varies depending on cache hit or miss, the cache greatly improves the performance when external memory is used. The number of\\xa0cycles\\xa0to read the\\xa0instruction\\xa0or\\xa0data varies depending\\xa0on\\xa0the\\xa0cache hit or\\xa0miss. Such overhead is not needed for a microcontroller. Microcontrollers can be used to control a number of different types of computers. Microprocessors can also be used for other types of computer systems, such as smartphones and tablets. The microcontroller can also control a variety of computers, including smartphones, tablets and computers. Cloud computing services first became available in the early 2000s, particularly targeted at large enterprises. The general concepts for cloud computing go back to the 1950s, although the term was coined in the 1960s. Cloud computing is a form of cloud-based data storage and management. The term was first used in the 1990s. Since then, cloud computing has spread to small and medium size businesses, and most recently to consumers. It is now available to businesses and consumers, and is becoming more widely available in the U.S. and Europe. It\\'s also available in more than 100 countries around the world, including the UK, Australia and New Zealand. Apple’s iCloud was launched in 2012 and had 20\\xa0million users within a week of launch. It is now estimated to have more than 100 million users worldwide. Apple has not revealed how much money it has made from iCloud so far. The company has also not revealed the price of its iCloud service. Evernote, the \\xadcloud-\\u200bbased notetaking and archiving service, launched in 2008, approached 100\\xa0million users in less than 6 years. Evernote is a cloud-based notetaking service that allows users to take and store notes. In this section, we provide a brief overview. In the first part of the article, we give you an overview of the main features of the game. The second part of this section provides a brief description of the gameplay. The third and final section gives you a brief look at the game\\'s scoring system. Cloud computing is examined in more detail in Chapter\\xa017. This is the first in a series of articles on the subject. The next two chapters will focus on the evolution of cloud computing. The third and final chapter will look at the development of cloud-based services. There is an increasingly prominent trend in many organizations to move a substantial portion or even all information technology (IT) operations to an \\xadInternet-\\u200breprehensible-connected infrastructure known as enterprise cloud computing. There is a growing trend in organizations to shift a substantial part of their IT operations to a cloud-based environment. At the same time, individual users of PCs and mobile devices are relying more and more on cloud computing services to backup data, synch devices, and share, using personal cloud computing.  Users are using cloud computing services such as Dropbox, Google Drive, and Microsoft Office 365. NIST defines cloud computing as a model for enabling ubiquitous, convenient, and convenient access to computing resources. Cloud computing can be rapidly provisioned and released with minimal management effort or service provider interaction. The NIST definition of cloud computing is published in NIST \\xadphthalSP-\\u200bphthalmological-SP-800-145 (The NIST Definition of Cloud  CarbuncleComputing) Basically, with cloud computing, you get economies of scale, professional network management, and professional security management. It\\'s a big step up from the web, which has been around for a long time, but is still a step down from the Internet of Things, which was invented in the 1990s. These features can be attractive to companies large and small, government agencies, and individual PC and mobile users. These features can be attractive to large companies, small companies and individual PCs and mobile devices. They are also attractive to government agencies and individual PC and phone users. The individual or company only needs to pay for the storage and services they need. For more information on how to get started with a computer, visit Computerworld.com. For information on getting started with computers, visit the ComputerWorld.com website. for more information about how to start with a Computer, visit\\xa0ComputerWorld.org. The user, be it company or individual, doesn’t have  the hassle of setting up a database system, acquiring the hardware they need, and doing maintenance. All these are part of the cloud service. The cloud service is available in the U.S., Canada, Australia, and New Zealand. In theory, another big advantage of using cloud computing to store your data and share it with others is that the cloud provider takes care of security. The cloud provider will take care of data security for you and your data. For more information on how to use cloud computing, visit cloud computing.org. Alas, the customer is not always protected. It\\'s not always easy to find out who you are talking to. It can be hard to tell if you\\'re talking to the right person or not. It could be difficult to tell who you\\'re speaking to. There have been a number of security failures among cloud providers. There have been \\xa0a number of  failures among cloud providers in the past. There has also been an increase in the number of people using cloud services. This has led to a rise in the cost of cloud services in the UK. Evernote made headlines in early 2013 when it told all of its users to reset their passwords after an intrusion was discovered. Evernote is a cloud-storage company that allows users to store and share documents. It was founded in 2007 and is based in San Francisco. Cloud networking refers to the networks and network management function-ality that must be in place to enable cloud computing. Cloud networking is a term used to refer to networks and networks that are used to connect to the cloud. It can also refer to the network that is used to communicate with the cloud, such as a phone network. Most cloud computing solu-tions rely on the Internet, but that is only a piece of the networking infrastructure. The Internet is just one part of the cloud computing infrastructure, but it\\'s a big part of it, too. Cloud computing is the future of cloud computing, says IBM. One example of cloud networking is the provisioning of \\xad grotesquely high-performance networking between the provider and subscriber. Another example is cloud-based video conferencing between a provider and a video-conferencing service provider, such as Skype or Vimeo. The cloud can also be used to provide cloud services. In this case, some or all of the traffic between an enterprise and the cloud bypasses the Internet and uses dedicated private network facilities owned or leased by the cloud service pro-vider. Some \\xa0pro-viders\\xa0own\\xa0private\\xa0networking\\xa0facilities\\xa0and\\xa0 lease\\xa0them\\xa0to\\xa0their\\xa0employees. More generally, cloud networking refers to the collection of network capa-bilities required to access a cloud. This includes making use of specialized services over the Internet, linking enterprise data centers to a cloud, and using firewalls and other network security devices at critical points to enforce access security policies. We can think of cloud storage as a subset of cloud computing. We can also think of it as a way of storing data in the cloud. It\\'s a form of cloud-based storage that can be used to store data in a variety of ways. For more information, visit cloudstorage.com. In essence, cloud storage consists of database storage and database applications hosted remotely on cloud servers. Cloud storage is a form of cloud computing that allows users to store and access data on a remote server. In essence, it is a way of storing and accessing data without having to be physically present. Cloud storage enables small businesses and individual users to take advantage of data storage that scales with their needs. Users can take advantage of a variety of database applications without having to buy, maintain, and manage the storage assets. For more information on how to use cloud storage, visit cloudstorage.com. Cloud computing is the convenient rental of computing resources. The essential purpose of cloud computing is to provide for the convenient renting of resources. Cloud computing can be used to provide a range of services, including cloud-based storage and storage. For more information on cloud computing, visit cloudcomputing.org. A cloud service provider (CSP) maintains computing and data storage resources that are available over the Internet or private networks. A CSP is a company that provides cloud computing services to customers. CSPs are usually based in the U.S. and provide cloud services across the world. Customers can rent a portion of these resources as needed. For more information on how to rent these resources, go to: http://www.cnn.com/2013/01/27/cnn-resources-rent-a-part-of-my-life-in-pictures. Virtually all cloud ser-vice is provided using one of three models (Figure\\xa01.17): SaaS, PAAS, and IaaS. As the name implies, a SAAS cloud provides service to customers in the form of software. SaaS follows the familiar model of Web services, in this case applied to cloud resources. SaaS is a form of cloud-based software that can be used to run web services. It can also be used as a way to provide cloud services to other users. SaaS enables the customer to use the cloud provider’s applications running on the provider\\'s cloud infrastructure. SaaS is a form of cloud-based software that runs on top of the cloud provider\\'s infrastructure. For more information, visit cloud-saaS.com. The applications are accessible from various client devices through a simple interface such as a Web browser. The app is available for iOS, Android and Windows devices. It is available to download from the Apple App Store and Google Play Store. It also offers a free trial version of the app for iOS. Instead of obtaining desktop and server licenses for software products it uses, an enterprise obtains the same functions from the cloud. The cloud  service is available in the U.S. for $99 a year. For more information, visit cloud.service.com or go to cloud.com/service. SaaS saves the complexity of software installation, maintenance, upgrades, and patches. SaaS is a cloud-based service that allows users to manage their software remotely. It can be used to save money, time, and money on software maintenance and upgrades. For more information, go to: www.saaas.com. Examples of services at this level are Gmail, Google’s \\xadreprehensible\\xad\\xadmail service, and Salesforce.com, which help firms keep track of their customers. For more information on how to sign up for the service click here. For confidential support call the Samaritans on 08457 90 90 90 or visit a local Samaritans branch, see www.samaritans.org for details. Common subscribers to SaaS are organizations that want to provide their employees with access to typical office productivity software, such as document 1.7 / Cloud Computing’s ‘41 Management’ and ‘management and email’ services. The average cost of a SAAS subscription is around $100,000 per year. Individuals also commonly use the SaaS model to acquire cloud resources. For example, a company could use a cloud service to provide a service to a customer. For more information, visit cloud.org.uk or go to the cloud.gov website. For information on how to use the cloud, go to cloud.co.uk. Typically, subscribers use specific applications on demand. Subscribers can use apps on demand to stream content to their phones and computers. The service is available in the U.S. and Europe, with plans to roll out to other countries later this year. For more, visit CNN.com/sales. The cloud provider also usually offers data-related features such as automatic backup and data sharing between subscribers. The cloud provider usually offers \\xad \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0data-\\u200breprehensive\\xadrelated features like automatic backup  \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0and data sharing. The company also offers a cloud service called Platform as a Service (PaaS) A PaaS cloud provides service to customers in  the form of a platform on which the customer’s applications can run. A PaaC cloud is a cloud-based service that provides a platform for applications to run on. PaaCs are often used by software companies to provide cloud services to customers. PaaS enables the customer to deploy onto the cloud infrastructure. PaaS can be used to build, manage, and deploy applications. It can also be used as a way to test and test new products and services. For more information, visit paaS.com. A PaaS cloud provides useful software building blocks, plus a number of development tools. These tools include programming languages, \\xadrun-\\u200b\\xadtime environments, and other tools that assist in deploying new applications. For more information, visit the cloud-as-a-service website. In effect, PaaS is an operating system in the cloud. In effect, it is a cloud-based operating system. PaaS can be used to build, manage, test, and run software on a variety of platforms. It can also be used as a way to test and test applications. PaaS is useful for an organization that wants to develop new or tailored applications while paying for the needed computing resources only as needed and only for as long as needed. PaaS can be used by companies that want to\\xa0develop\\xa0new or tailored\\xa0applications. Google App Engine and Salesforce1 Platform from Salesforce.com are examples of PaaS. With IaaS, the customer has access to the  \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0underlying cloud infrastructure. CSP is a cloud service provider that manages cloud applications for its customers. It also manages software as a service for cloud applications. IaaS provides virtual machines and other abstracted hardware and operating systems. The virtual machines may be controlled through a service application programming interface (API) IaaS is a cloud-based service that provides cloud services to businesses and government agencies. It is available in a number of languages, including English. IaaS offers the customer processing, storage, networks, and other fundamental computing resources so that the customer is able to deploy and run arbitrary software, which can include operating systems and applications. IaaS can also be used to build cloud-based applications and services. IaaS enables customers to combine basic computing services,  such as number crunching and data storage, to build highly adaptable computer systems. IaaS is a cloud-based version of cloud computing, and can be used to build a range of applications and services. Examples of IaaS are Amazon Elastic Compute Cloud (Amazon EC2) and Windows Azure. Amazon EC2 and Windows Azure are both cloud computing platforms. Amazon\\'s EC2 is the world\\'s largest cloud computing platform. Microsoft\\'s Azure is the second largest cloud provider. Review Questions: What is the distinction between computer structure and computer function? What are the four main functions of a computer? How do we define the main structural components of acomputer? What is a computer\\'s architecture? How does a computer work? And what are the main problems it faces? 1.5: The main structural components of a processor. 1.5.1: List and briefly define the main structural parts of the processor.  1. 5.2: The components of the processors that make up the processor\\'s power supply. 2. The main components that make the processor operate.  The processor\\'s\\xa0power supply. 1.1.6: What is a stored program computer? 1.2.7: Explain Moore’s law. 1.3.8: List and explain the key characteristics of a computer family. 2.0.1: Explain the key features of a program computer. 1.9 million people voted in the U.S. for President Barack Obama. Obama won the White House in a landslide victory in 2008. Obama\\'s approval rating hit a record high of 90.9 percent in 2010. His approval rating has since dropped to 80.1 percent. You are to write an IAS program to compute the results of the following equation. What is the key distinguishing feature of a microprocessor? 1.8 / Key Terms, Review Questions, and Problems. 1.1 / Key terms, review questions, and problems. Assume that the computation does not result in an arithmetic overflow and that X, Y, and N are positive integers with N ≥ 1. Y = a \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0N \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0X=1˚. X = a˚N˚X = 1˚Y. Note: The IAS did not have assembly language, only machine language. It was the first time the IAS was written in machine language, not assembly language. The first IAS to be written in assembly language was the\\xa0IAS\\xa01, which was written\\xa0in\\xa0machine\\xa0language\\xa0in the 1950s. a. Use the equation Sum(Y) = N(N + 1)2 when writing the IAS program. b. Use Sum(N) = Y + 1 to get the same result. c. Use this equation when writing your own IAS programs. b. Do it the “hard way,” without using the equation from part (a) c. Use the ‘hard way’ instead of ‘easy’ to get the same result.d. Use “the hard way” instead of the � “easy” way. On the IAS, describe in English the process that the CPU must undertake to read a value from memory and to write a value to memory in terms of what is put into the MAR, MBR, address bus, data bus, and control bus. How many trips to memory does the CPU need to make to complete this instruc-tion during the instruction cycle? In Figure\\xa01.6, indicate the width, in bits, of each data path (e.g., between AC and ALU). In Figure 1.1, show the assembly language code for the program, starting at address 08A. In Figure 2, explain what this program does. In the IBM 360 Models 65 and 75, addresses are staggered in two separate main mem-agicallyory units (e.g., all \\xadeven-\\u200b worrisome-numbered words in one unit and all \\xadodd-\\u200b fearful-numbered Words in another unit) The address system is based on IBM\\'s IBM 360 software, which was released in 1989. The relative performance of the IBM 360 Model 75 is 50 times that of the 360 Model 30. Yet the instruction cycle time is only 5 times as fast. What might be the purpose of this technique? We\\'ll have to wait and see to find out. We\\'ll keep you posted on the progress. How do you account for this  discrepancy? Do you have a theory? Share it with us in the comments below. We would love to hear from you. Please share your story of how you\\'ve dealt with this issue. We\\'d like to hear about your experiences in this way. Billy Bob\\'s computer store has the fastest computer in the store. A customer asks Billy Bob what is the fastestComputer in the Store that he can buy. The answer is a 1.8-inch desktop computer. Billy Bob’s computer store is located in New York City. Billy Bob replies, “You’re looking at our Macintoshes.” “Yes, I am,” says Billy Bob. “And I’m looking at your face.’’ “No, you aren’t,’ he says. ‘You are looking at my face’ The fastest Mac we have runs at a clock speed of 1.2 GHz. The Macbook Pro runs at speeds of up to 1.4GHz. It\\'s the most powerful Mac ever made, with a top speed of 2.5Ghz. It can also run at speeds up to 3Ghz, and has a battery life of 18 months. If you really want the fastest machine, you should buy our 2.4-GHz Intel Pentium IV instead. It\\'s the fastest processor on the market at the time of this writing. It has a top speed of 2.8GHz. It is the most powerful processor available on a PC at the moment. Is Billy Bob correct? Billy Bob says he is. Billy Bob: I\\'m not sure I\\'m correct. Is Billy Bob right? BillyBob: I don\\'t know. I just don\\'t think I\\'m right. Is that what you want to hear? Is that the truth? Is it? The ENIAC, a precursor to the ISA machine, was a decimal machine, in which each register was represented by a ring of 10 vacuum tubes. The machine was used to test the accuracy of the computer\\'s calculations. It was used by the U.S. military to test its computer systems. At any time, only one vacuum tube was in the ON state, representing one of the 10 decimal digits. Only one vacuum tube could be in the OFF state at any one time. The vacuum tube was in an ON state to represent the 10th decimal digit of the alphabet. For each of the following examples, determine whether this is an embedded system, and explain why or why not. For example, suppose ENIAC has multiple vacuum tubes in the ON and OFF state. Why is this representation “wasteful” and what range of integer values could we represent using the 10 vacuum tubes? For example, is the internal microprocessor controlling a disk drive an example of an embedded system? Is a PDA (Personal Digital Assistant) an embedded system? Are the computers in a big \\xadphthalmologist’s radar radar considered embedded? Do I/O drivers control hardware, so does the presence of an I-O driver imply that the computer executing the driver is embedded? These radars are 10-story buildings with one to three 100-foot diameter radiating patches on the sloped sides of the building. They are used in the U.S. military to test new weapons and equipment. They can also be used to test the effectiveness of anti-aircraft weapons. Is a traditional flight management system (FMS) built into an airplane cockpit considered embedded? Is the computer controlling a pacemaker in a person’s chest an embedded computer? Are the computers in a \\xad \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0hardware-\\u200b\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0in a HIL (HIL) simulator embedded? j.j.o.o: Is the computer controlling fuel injection in an automobile engine embedded?j. j.o.: How do we know if a computer is embedded in an engine?j j.jao: What are the key terms, review questions, and problems to be looked at?'"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_concat = \" \".join(summaries)\n",
    "summary_concat"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-30T10:46:12.503837050Z",
     "start_time": "2024-04-30T10:46:12.439979683Z"
    }
   },
   "id": "bc5a4978b49692c8",
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "2    Chapter 1 / Basic Concepts and Computer Evolution \n\t 1.1\t Organization and Architecture\nIn describing computers, a distinction is often made between computer architec-\nture and computer organization. Although it is difficult to give precise definitions \nfor these terms, a consensus exists about the general areas covered by each. For \nexample, see [VRAN80], [SIEW82], and [BELL78a]; an interesting alternative view \nis presented in [REDD76].\nComputer architecture refers to those attributes of a system visible to a pro-\ngrammer or, put another way, those attributes that have a direct impact on the \nlogical execution of a program. A term that is often used interchangeably with com-\nputer architecture is instruction set architecture (ISA). The ISA defines instruction \nformats, instruction opcodes, registers, instruction and data memory; the effect of \nexecuted instructions on the registers and memory; and an algorithm for control-\nling instruction execution. Computer organization refers to the operational units \nand their interconnections that realize the architectural specifications. Examples of \narchitectural attributes include the instruction set, the number of bits used to repre-\nsent various data types (e.g., numbers, characters), I/O mechanisms, and techniques \nfor addressing memory. Organizational attributes include those hardware details \ntransparent to the programmer, such as control signals; interfaces between the com-\nputer and peripherals; and the memory technology used.\nFor example, it is an architectural design issue whether a computer will have \na multiply instruction. It is an organizational issue whether that instruction will be \nimplemented by a special multiply unit or by a mechanism that makes repeated \nuse of the add unit of the system. The organizational decision may be based on the \nanticipated frequency of use of the multiply instruction, the relative speed of the \ntwo approaches, and the cost and physical size of a special multiply unit.\nHistorically, and still today, the distinction between architecture and organ-\nization has been an important one. Many computer manufacturers offer a family of \ncomputer models, all with the same architecture but with differences in organization. \nConsequently, the different models in the family have different price and perform-\nance characteristics. Furthermore, a particular architecture may span many years \nand encompass a number of different computer models, its organization changing \nwith changing technology. A prominent example of both these phenomena is the \nIBM System/370 architecture. This architecture was first introduced in 1970 and \nLearning Objectives\nAfter studying this chapter, you should be able to:\nr\nr Explain the general functions and structure of a digital computer.\nr\nr Present an overview of the evolution of computer technology from early \ndigital computers to the latest microprocessors.\nr\nr Present an overview of the evolution of the x86 architecture.\nr\nr Define embedded systems and list some of the requirements and constraints \nthat various embedded systems must meet.\n1.2 / Structure and Function   3\nincluded a number of models. The customer with modest requirements could buy a \ncheaper, slower model and, if demand increased, later upgrade to a more expensive, \nfaster model without having to abandon software that had already been developed. \nOver the years, IBM has introduced many new models with improved technology \nto replace older models, offering the customer greater speed, lower cost, or both. \nThese newer models retained the same architecture so that the customer’s soft-\nware investment was protected. Remarkably, the System/370 architecture, with a \nfew enhancements, has survived to this day as the architecture of IBM’s mainframe \nproduct line.\nIn a class of computers called microcomputers, the relationship between archi-\ntecture and organization is very close. Changes in technology not only influence \norganization but also result in the introduction of more powerful and more complex \narchitectures. Generally, there is less of a requirement for ­\ngeneration-​\n­\nto-​\n­\ngeneration \ncompatibility for these smaller machines. Thus, there is more interplay between \norganizational and architectural design decisions. An intriguing example of this is \nthe reduced instruction set computer (RISC), which we examine in Chapter 15.\nThis book examines both computer organization and computer architecture. \nThe emphasis is perhaps more on the side of organization. However, because a \ncomputer organization must be designed to implement a particular architectural \nspecification, a thorough treatment of organization requires a detailed examination \nof architecture as well.\n\t 1.2\t Structure and Function\nA computer is a complex system; contemporary computers contain millions of \nelementary electronic components. How, then, can one clearly describe them? The \nkey is to recognize the hierarchical nature of most complex systems, including the \ncomputer [SIMO96]. A hierarchical system is a set of interrelated subsystems, each \nof the latter, in turn, hierarchical in structure until we reach some lowest level of \nelementary subsystem.\nThe hierarchical nature of complex systems is essential to both their design \nand their description. The designer need only deal with a particular level of the \nsystem at a time. At each level, the system consists of a set of components and \ntheir interrelationships. The behavior at each level depends only on a simplified, \nabstracted characterization of the system at the next lower level. At each level, the \ndesigner is concerned with structure and function:\n■\n■Structure: The way in which the components are interrelated.\n■\n■Function: The operation of each individual component as part of the structure.\nIn terms of description, we have two choices: starting at the bottom and build-\ning up to a complete description, or beginning with a top view and decomposing the \nsystem into its subparts. Evidence from a number of fields suggests that the ­\ntop-​\n­\ndown approach is the clearest and most effective [WEIN75].\nThe approach taken in this book follows from this viewpoint. The computer \nsystem will be described from the top down. We begin with the major components \nof a computer, describing their structure and function, and proceed to successively \n4    Chapter 1 / Basic Concepts and Computer Evolution \nlower layers of the hierarchy. The remainder of this section provides a very brief \noverview of this plan of attack.\nFunction\nBoth the structure and functioning of a computer are, in essence, simple. In general \nterms, there are only four basic functions that a computer can perform:\n■\n■Data processing: Data may take a wide variety of forms, and the range of pro-\ncessing requirements is broad. However, we shall see that there are only a few \nfundamental methods or types of data processing.\n■\n■Data storage: Even if the computer is processing data on the fly (i.e., data \ncome in and get processed, and the results go out immediately), the computer \nmust temporarily store at least those pieces of data that are being worked on \nat any given moment. Thus, there is at least a ­\nshort-​\n­\nterm data storage function. \nEqually important, the computer performs a ­\nlong-​\n­\nterm data storage function. \nFiles of data are stored on the computer for subsequent retrieval and update.\n■\n■Data movement: The computer’s operating environment consists of devices \nthat serve as either sources or destinations of data. When data are received \nfrom or delivered to a device that is directly connected to the computer, the \nprocess is known as ­\ninput–​\n­\noutput (I/O), and the device is referred to as a \nperipheral. When data are moved over longer distances, to or from a remote \ndevice, the process is known as data communications.\n■\n■Control: Within the computer, a control unit manages the computer’s \nresources and orchestrates the performance of its functional parts in response \nto instructions.\nThe preceding discussion may seem absurdly generalized. It is certainly \npossible, even at a top level of computer structure, to differentiate a variety of func-\ntions, but to quote [SIEW82]:\nThere is remarkably little shaping of computer structure to fit the \nfunction to be performed. At the root of this lies the ­\ngeneral-​\n­\npurpose \nnature of computers, in which all the functional specialization occurs \nat the time of programming and not at the time of design.\nStructure\nWe now look in a general way at the internal structure of a computer. We begin with \na traditional computer with a single processor that employs a microprogrammed \ncontrol unit, then examine a typical multicore structure.\nsimple ­\nsingle-​\n­\nprocessor computer Figure 1.1 provides a hierarchical view \nof the internal structure of a traditional ­\nsingle-​\n­\nprocessor computer. There are four \nmain structural components:\n■\n■Central processing unit (CPU): Controls the operation of the computer and \nperforms its data processing functions; often simply referred to as processor.\n■\n■Main memory: Stores data.\n1.2 / Structure and Function   5\n■\n■I/O: Moves data between the computer and its external environment.\n■\n■System interconnection: Some mechanism that provides for communication \namong CPU, main memory, and I/O. A common example of system intercon-\nnection is by means of a system bus, consisting of a number of conducting \nwires to which all the other components attach.\nThere may be one or more of each of the aforementioned components. Tra-\nditionally, there has been just a single processor. In recent years, there has been \nincreasing use of multiple processors in a single computer. Some design issues relat-\ning to multiple processors crop up and are discussed as the text proceeds; Part Five \nfocuses on such computers.\nMain\nmemory\nI/O\nCPU\nCOMPUTER\nSystem\nbus\nALU\nRegisters\nControl\nunit\nCPU\nInternal\nbus\nControl unit\nregisters and\ndecoders\nCONTROL\nUNIT\nSequencing\nlogic\nControl\nmemory\nFigure 1.1  The Computer: ­\nTop-​\n­\nLevel Structure\n6    Chapter 1 / Basic Concepts and Computer Evolution \nEach of these components will be examined in some detail in Part Two. How-\never, for our purposes, the most interesting and in some ways the most complex \ncomponent is the CPU. Its major structural components are as follows:\n■\n■Control unit: Controls the operation of the CPU and hence the computer.\n■\n■Arithmetic and logic unit (ALU): Performs the computer’s data processing \nfunctions.\n■\n■Registers: Provides storage internal to the CPU.\n■\n■CPU interconnection: Some mechanism that provides for communication \namong the control unit, ALU, and registers.\nPart Three covers these components, where we will see that complexity is added by \nthe use of parallel and pipelined organizational techniques. Finally, there are sev-\neral approaches to the implementation of the control unit; one common approach is \na microprogrammed implementation. In essence, a microprogrammed control unit \noperates by executing microinstructions that define the functionality of the control \nunit. With this approach, the structure of the control unit can be depicted, as in \n­\nFigure 1.1. This structure is examined in Part Four.\nmulticore computer structure As was mentioned, contemporary \ncomputers generally have multiple processors. When these processors all reside \non a single chip, the term multicore computer is used, and each processing unit \n(consisting of a control unit, ALU, registers, and perhaps cache) is called a core. To \nclarify the terminology, this text will use the following definitions.\n■\n■Central processing unit (CPU): That portion of a computer that fetches and \nexecutes instructions. It consists of an ALU, a control unit, and registers. \nIn a system with a single processing unit, it is often simply referred to as a \nprocessor.\n■\n■Core: An individual processing unit on a processor chip. A core may be equiv-\nalent in functionality to a CPU on a ­\nsingle-​\n­\nCPU system. Other specialized pro-\ncessing units, such as one optimized for vector and matrix operations, are also \nreferred to as cores.\n■\n■Processor: A physical piece of silicon containing one or more cores. The \nprocessor is the computer component that interprets and executes instruc-\ntions. If a processor contains multiple cores, it is referred to as a multicore \nprocessor.\nAfter about a decade of discussion, there is broad industry consensus on this usage.\nAnother prominent feature of contemporary computers is the use of multiple \nlayers of memory, called cache memory, between the processor and main memory. \nChapter 4 is devoted to the topic of cache memory. For our purposes in this section, \nwe simply note that a cache memory is smaller and faster than main memory and is \nused to speed up memory access, by placing in the cache data from main memory, \nthat is likely to be used in the near future. A greater performance improvement may \nbe obtained by using multiple levels of cache, with level 1 (L1) closest to the core \nand additional levels (L2, L3, and so on) progressively farther from the core. In this \nscheme, level n is smaller and faster than level n + 1.\n1.2 / Structure and Function   7\nFigure 1.2 is a simplified view of the principal components of a typical mul-\nticore computer. Most computers, including embedded computers in smartphones \nand tablets, plus personal computers, laptops, and workstations, are housed on a \nmotherboard. Before describing this arrangement, we need to define some terms. \nA printed circuit board (PCB) is a rigid, flat board that holds and interconnects \nchips and other electronic components. The board is made of layers, typically two \nto ten, that interconnect components via copper pathways that are etched into \nthe board. The main printed circuit board in a computer is called a system board \nor motherboard, while smaller ones that plug into the slots in the main board are \ncalled expansion boards.\nThe most prominent elements on the motherboard are the chips. A chip is \na single piece of semiconducting material, typically silicon, upon which electronic \ncircuits and logic gates are fabricated. The resulting product is referred to as an \nintegrated circuit.\nMOTHERBOARD\nPROCESSOR CHIP\nCORE\nProcessor\nchip\nMain memory chips\nI/O chips\nCore\nL3 cache\nInstruction\nlogic\nL1 I-cache\nL2 instruction\ncache\nL2 data\ncache\nL1 data cache\nArithmetic\nand logic\nunit (ALU)\nLoad/\nstore logic\nL3 cache\nCore\nCore\nCore\nCore\nCore\nCore\nCore\nFigure 1.2  Simplified View of Major Elements of a Multicore Computer\n8    Chapter 1 / Basic Concepts and Computer Evolution \nThe motherboard contains a slot or socket for the processor chip, which typ-\nically contains multiple individual cores, in what is known as a multicore processor. \nThere are also slots for memory chips, I/O controller chips, and other key computer \ncomponents. For desktop computers, expansion slots enable the inclusion of more \ncomponents on expansion boards. Thus, a modern motherboard connects only a \nfew individual chip components, with each chip containing from a few thousand up \nto hundreds of millions of transistors.\nFigure 1.2 shows a processor chip that contains eight cores and an L3 cache. \nNot shown is the logic required to control operations between the cores and the \ncache and between the cores and the external circuitry on the motherboard. The \nfigure indicates that the L3 cache occupies two distinct portions of the chip surface. \nHowever, typically, all cores have access to the entire L3 cache via the aforemen-\ntioned control circuits. The processor chip shown in Figure 1.2 does not represent \nany specific product, but provides a general idea of how such chips are laid out.\nNext, we zoom in on the structure of a single core, which occupies a portion of \nthe processor chip. In general terms, the functional elements of a core are:\n■\n■Instruction logic: This includes the tasks involved in fetching instructions, \nand decoding each instruction to determine the instruction operation and the \nmemory locations of any operands.\n■\n■Arithmetic and logic unit (ALU): Performs the operation specified by an \ninstruction.\n■\n■Load/store logic: Manages the transfer of data to and from main memory via \ncache.\nThe core also contains an L1 cache, split between an instruction cache \n \n(­\nI-​\n­\ncache) that is used for the transfer of instructions to and from main memory, and \nan L1 data cache, for the transfer of operands and results. Typically, today’s pro-\ncessor chips also include an L2 cache as part of the core. In many cases, this cache \nis also split between instruction and data caches, although a combined, single L2 \ncache is also used.\nKeep in mind that this representation of the layout of the core is only intended \nto give a general idea of internal core structure. In a given product, the functional \nelements may not be laid out as the three distinct elements shown in Figure 1.2, \nespecially if some or all of these functions are implemented as part of a micropro-\ngrammed control unit.\nexamples It will be instructive to look at some ­\nreal-​\n­\nworld examples that \nillustrate the hierarchical structure of computers. Figure 1.3 is a photograph of the \nmotherboard for a computer built around two Intel ­\nQuad-​\n­\nCore Xeon processor \nchips. Many of the elements labeled on the photograph are discussed subsequently \nin this book. Here, we mention the most important, in addition to the processor \nsockets:\n■\n■­\nPCI-​\n­\nExpress slots for a ­\nhigh-​\n­\nend display adapter and for additional peripher-\nals (Section 3.6 describes PCIe).\n■\n■Ethernet controller and Ethernet ports for network connections.\n■\n■USB sockets for peripheral devices.\n1.2 / Structure and Function   9\n2x Quad-Core Intel® Xeon® Processors\nwith Integrated Memory Controllers\nSix Channel DDR3-1333 Memory\nInterfaces Up to 48GB\nIntel® 3420\nChipset\nSerial ATA/300 (SATA)\nInterfaces\n2x USB 2.0\nInternal\n2x Ethernet Ports\n10/100/1000Base-T\nEthernet Controller\nClock\nPCI Express®\nConnector A\nPCI Express®\nConnector B\nPower & Backplane I/O\nConnector C\nVGA Video Output\nBIOS\n2x USB 2.0\nExternal\nFigure 1.3  Motherboard with Two Intel ­\nQuad-​\n­\nCore Xeon Processors\nSource: Chassis Plans, www.chassis-plans.com\n■\n■Serial ATA (SATA) sockets for connection to disk memory (Section 7.7 \n­\ndiscusses Ethernet, USB, and SATA).\n■\n■Interfaces for DDR (double data rate) main memory chips (Section  5.3 \n­\ndiscusses DDR).\n■\n■Intel 3420 chipset is an I/O controller for direct memory access operations \nbetween peripheral devices and main memory (Section 7.5 discusses DDR).\nFollowing our ­\ntop-​\n­\ndown strategy, as illustrated in Figures 1.1 and 1.2, we can \nnow zoom in and look at the internal structure of a processor chip. For variety, we \nlook at an IBM chip instead of the Intel processor chip. Figure 1.4 is a photograph \nof the processor chip for the IBM zEnterprise EC12 mainframe computer. This chip \nhas 2.75 billion transistors. The superimposed labels indicate how the silicon real \nestate of the chip is allocated. We see that this chip has six cores, or processors. \nIn addition, there are two large areas labeled L3 cache, which are shared by all six \nprocessors. The L3 control logic controls traffic between the L3 cache and the cores \nand between the L3 cache and the external environment. Additionally, there is stor-\nage control (SC) logic between the cores and the L3 cache. The memory controller \n(MC) function controls access to memory external to the chip. The GX I/O bus \ncontrols the interface to the channel adapters ­\naccessing the I/O.\nGoing down one level deeper, we examine the internal structure of a single \ncore, as shown in the photograph of Figure 1.5. Keep in mind that this is a portion \nof the silicon surface area making up a ­\nsingle-​\n­\nprocessor chip. The main ­\nsub-​\n­\nareas \nwithin this core area are the following:\n■\n■ISU (instruction sequence unit): Determines the sequence in which instructions \nare executed in what is referred to as a superscalar architecture (Chapter 16).\n■\n■IFU (instruction fetch unit): Logic for fetching instructions.\n10    Chapter 1 / Basic Concepts and Computer Evolution \n■\n■IDU (instruction decode unit): The IDU is fed from the IFU buffers, and is \nresponsible for the parsing and decoding of all z/Architecture operation codes.\n■\n■LSU (­\nload-​\n­\nstore unit): The LSU contains the 96-kB L1 data cache,1 and man-\nages data traffic between the L2 data cache and the functional execution \nunits. It is responsible for handling all types of operand accesses of all lengths, \nmodes, and formats as defined in the z/Architecture.\n■\n■XU (translation unit):  This unit translates logical addresses from instructions \ninto physical addresses in main memory. The XU also contains a translation \nlookaside buffer (TLB) used to speed up memory access. TLBs are discussed \nin Chapter 8.\n■\n■FXU (­\nfixed-​\n­\npoint unit): The FXU executes ­\nfixed-​\n­\npoint arithmetic operations.\n■\n■BFU (binary ­\nfloating-​\n­\npoint unit): The BFU handles all binary and hexadeci-\nmal ­\nfloating-​\n­\npoint operations, as well as ­\nfixed-​\n­\npoint multiplication operations.\n■\n■DFU (decimal ­\nfloating-​\n­\npoint unit): The DFU handles both ­\nfixed-​\n­\npoint and \n­\nfloating-​\n­\npoint operations on numbers that are stored as decimal digits.\n■\n■RU (recovery unit): The RU keeps a copy of the complete state of the sys-\ntem that includes all registers, collects hardware fault signals, and manages the \nhardware recovery actions.\nFigure 1.4  zEnterprise EC12 Processor Unit \n(PU) chip diagram\nSource: IBM zEnterprise EC12 Technical Guide, \nDecember 2013, SG24-8049-01. IBM, Reprinted by \nPermission\nFigure 1.5  zEnterprise EC12 Core layout\nSource: IBM zEnterprise EC12 Technical Guide, \nDecember 2013, SG24-8049-01. IBM, Reprinted by \nPermission\n1kB = kilobyte = 2048 bytes. Numerical prefixes are explained in a document under the “Other Useful” \ntab at ComputerScienceStudent.com.\n1.3 / A Brief History of Computers   11\n■\n■COP (dedicated ­\nco-​\n­\nprocessor): The COP is responsible for data compression \nand encryption functions for each core.\n■\n■­\nI-​\n­\ncache: This is a 64-kB L1 instruction cache, allowing the IFU to prefetch \ninstructions before they are needed.\n■\n■L2 control: This is the control logic that manages the traffic through the two \nL2 caches.\n■\n■­\nData-​\n­\nL2: A 1-MB L2 data cache for all memory traffic other than instructions.\n■\n■­\nInstr-​\n­\nL2: A 1-MB L2 instruction cache.\nAs we progress through the book, the concepts introduced in this section will \nbecome clearer.\n\t 1.3\t A Brief History of Computers2\nIn this section, we provide a brief overview of the history of the development of \ncomputers. This history is interesting in itself, but more importantly, provides a basic \nintroduction to many important concepts that we deal with throughout the book.\nThe First Generation: Vacuum Tubes\nThe first generation of computers used vacuum tubes for digital logic elements and \nmemory. A number of research and then commercial computers were built using \nvacuum tubes. For our purposes, it will be instructive to examine perhaps the most \nfamous ­\nfirst-​\n­\ngeneration computer, known as the IAS computer.\nA fundamental design approach first implemented in the IAS computer is \nknown as the ­\nstored-​\n­\nprogram concept. This idea is usually attributed to the mathem-\natician John von Neumann. Alan Turing developed the idea at about the same time. \nThe first publication of the idea was in a 1945 proposal by von Neumann for a new \ncomputer, the EDVAC (Electronic Discrete Variable Computer).3\nIn 1946, von Neumann and his colleagues began the design of a new ­\nstored-​\n­\nprogram computer, referred to as the IAS computer, at the Princeton Institute for \nAdvanced Studies. The IAS computer, although not completed until 1952, is the \nprototype of all subsequent ­\ngeneral-​\n­\npurpose computers.4\nFigure 1.6 shows the structure of the IAS computer (compare with Figure 1.1). \nIt consists of\n■\n■A main memory, which stores both data and instructions5\n■\n■An arithmetic and logic unit (ALU) capable of operating on binary data\n2 \nThis book’s Companion Web site (WilliamStallings.com/ComputerOrganization) contains several links \nto sites that provide photographs of many of the devices and components discussed in this section.\n4A 1954 report [GOLD54] describes the implemented IAS machine and lists the final instruction set. It \nis available at box.com/COA10e.\n3The 1945 report on EDVAC is available at box.com/COA10e.\n5In this book, unless otherwise noted, the term instruction refers to a machine instruction that is directly \ninterpreted and executed by the processor, in contrast to a statement in a ­\nhigh-​\n­\nlevel language, such as Ada \nor C++, which must first be compiled into a series of machine instructions before being executed.\n12    Chapter 1 / Basic Concepts and Computer Evolution \n■\n■A control unit, which interprets the instructions in memory and causes them \nto be executed\n■\n■­\nInput–​\n­\noutput (I/O) equipment operated by the control unit\nThis structure was outlined in von Neumann’s earlier proposal, which is worth \nquoting in part at this point [VONN45]:\n2.2 First: Since the device is primarily a computer, it will \nhave to perform the elementary operations of arithmetic most fre-\nquently. These are addition, subtraction, multiplication, and divi-\nsion. It is therefore reasonable that it should contain specialized \norgans for just these operations.\nControl\ncircuits\nAddresses\nControl\nsignals\nInstructions\nand data\nAC: Accumulator register\nMQ: multiply-quotient register\nMBR: memory buffer register\nIBR: instruction buffer register\nPC: program counter\nMAR: memory address register\nIR: insruction register\nInstructions\nand data\nM(0)\nM(1)\nM(2)\nM(3)\nM(4)\nM(4095)\nM(4093)\nM(4092)\nMBR\nArithmetic-logic unit (CA)\nCentral processing unit (CPU)\nProgram control unit (CC)\nInput-\noutput\nequipment\n(I, O)\nMain\nmemory\n(M)\nAC\nMQ\nArithmetic-logic\ncircuits\nIBR\nPC\nIR\nMAR\nFigure 1.6  IAS Structure\nIt must be observed, however, that while this principle as such \nis probably sound, the specific way in which it is realized requires \nclose scrutiny. At any rate a central arithmetical part of the device will \nprobably have to exist, and this constitutes the first specific part: CA.\n2.3  Second: The logical control of the device, that is, the \nproper sequencing of its operations, can be most efficiently car-\nried out by a central control organ. If the device is to be elastic, \nthat is, as nearly as possible all purpose, then a distinction must \nbe made between the specific instructions given for and defining \na particular problem, and the general control organs that see to it \nthat these ­\ninstructions—​\n­\nno matter what they ­\nare—​\n­\nare carried out. \nThe former must be stored in some way; the latter are represented \nby definite operating parts of the device. By the central control we \nmean this latter function only, and the organs that perform it form \nthe second specific part: CC.\n2.4 Third: Any device that is to carry out long and compli-\ncated sequences of operations (specifically of calculations) must \nhave a considerable memory . . .\nThe instructions which govern a complicated problem may \nconstitute considerable material, particularly so if the code is cir-\ncumstantial (which it is in most arrangements). This material must \nbe remembered.\nAt any rate, the total memory constitutes the third specific \npart of the device: M.\n2.6 The three specific parts CA, CC (together C), and M cor-\nrespond to the associative neurons in the human nervous system. It \nremains to discuss the equivalents of the sensory or afferent and the \nmotor or efferent neurons. These are the input and output organs of \nthe device.\nThe device must be endowed with the ability to maintain \ninput and output (sensory and motor) contact with some specific \nmedium of this type. The medium will be called the outside record-\ning medium of the device: R.\n2.7 Fourth: The device must have organs to transfer informa-\ntion from R into its specific parts C and M. These organs form its \ninput, the fourth specific part: I. It will be seen that it is best to make \nall transfers from R (by I) into M and never directly from C.\n2.8 Fifth: The device must have organs to transfer from its \nspecific parts C and M into R. These organs form its output, the \nfifth specific part: O. It will be seen that it is again best to make all \ntransfers from M (by O) into R, and never directly from C.\nWith rare exceptions, all of today’s computers have this same general structure \nand function and are thus referred to as von Neumann machines. Thus, it is worth-\nwhile at this point to describe briefly the operation of the IAS computer [BURK46, \nGOLD54]. Following [HAYE98], the terminology and notation of von Neumann \n1.3 / A Brief History of Computers   13\n14    Chapter 1 / Basic Concepts and Computer Evolution \nare changed in the following to conform more closely to modern usage; the exam-\nples accompanying this discussion are based on that latter text.\nThe memory of the IAS consists of 4,096 storage locations, called words, of \n40 binary digits (bits) each.6 Both data and instructions are stored there. Numbers are \nrepresented in binary form, and each instruction is a binary code. Figure 1.7 illustrates \nthese formats. Each number is represented by a sign bit and a 39-bit value. A word \nmay alternatively contain two 20-bit instructions, with each instruction consisting \nof an 8-bit operation code (opcode) specifying the operation to be performed and \na 12-bit address designating one of the words in memory (numbered from 0 to 999).\nThe control unit operates the IAS by fetching instructions from memory \nand executing them one at a time. We explain these operations with reference to \n­\nFigure 1.6. This figure reveals that both the control unit and the ALU contain stor-\nage locations, called registers, defined as follows:\n■\n■Memory buffer register (MBR):  Contains a word to be stored in memory or sent \nto the I/O unit, or is used to receive a word from memory or from the I/O unit.\n■\n■Memory address register (MAR): Specifies the address in memory of the word \nto be written from or read into the MBR.\n■\n■Instruction register (IR):  Contains the 8-bit opcode instruction being executed.\n■\n■Instruction buffer register (IBR): Employed to hold temporarily the ­\nright-​\n­\nhand instruction from a word in memory.\n■\n■Program counter (PC): Contains the address of the next instruction pair to be \nfetched from memory.\n■\n■Accumulator (AC) and multiplier quotient (MQ): Employed to hold tem-\nporarily operands and results of ALU operations. For example, the result \n6There is no universal definition of the term word. In general, a word is an ordered set of bytes or bits \nthat is the normal unit in which information may be stored, transmitted, or operated on within a given \ncomputer. Typically, if a processor has a ­\nfixed-​\n­\nlength instruction set, then the instruction length equals \nthe word length.\n(a) Number word\nsign bit\n0\n39\n(b) Instruction word\nopcode (8 bits)\naddress (12 bits)\nleft instruction (20 bits)\n0\n8\n20\n28\n39\n1\nright instruction (20 bits)\nopcode (8 bits)\naddress (12 bits)\nFigure 1.7  IAS Memory Formats\nof multiplying two 40-bit numbers is an 80-bit number; the most significant \n40 bits are stored in the AC and the least significant in the MQ.\nThe IAS operates by repetitively performing an instruction cycle, as shown in \nFigure 1.8. Each instruction cycle consists of two subcycles. During the fetch cycle, \nthe opcode of the next instruction is loaded into the IR and the address portion is \nloaded into the MAR. This instruction may be taken from the IBR, or it can be \nobtained from memory by loading a word into the MBR, and then down to the IBR, \nIR, and MAR.\nWhy the indirection? These operations are controlled by electronic circuitry \nand result in the use of data paths. To simplify the electronics, there is only one reg-\nister that is used to specify the address in memory for a read or write and only one \nregister used for the source or destination.\n1.3 / A Brief History of Computers   15\nStart\nIs next\ninstruction\nin IBR?\nMAR \n   \n    PC\nMBR \n   \n    M(MAR)\nIR    \n    IBR (0:7)\nMAR \n  \n     IBR (8:19)\nIR    \n     MBR (20:27)\nMAR    \n    MBR (28:39)\nLeft\ninstruction\nrequired?\nIBR \n    \n   MBR (20:39)\nIR     \n   MBR (0:7)\nMAR \n   \n    MBR (8:19)\nPC    \n    PC + 1\nYes\nYes\nYes\nNo\nNo\nNo\nM(X) = contents of memory location whose address is X\n(i:j) = bits i through j\nNo memory\naccess\nrequired\nDecode instruction in IR\nAC   \n     M(X)\nGo to M(X, 0:19)\nIf AC > 0 then\ngo to M(X, 0:19)\nAC \n   \n    AC + M(X)\nIs AC > 0?\nMBR   \n    M(MAR)\nMBR \n  \n    M(MAR)\nPC    \n    MAR\nAC    \n    MBR\nAC \n   \n    AC + MBR\nFetch\ncycle\nExecution\ncycle\nFigure 1.8  Partial Flowchart of IAS Operation\n16    Chapter 1 / Basic Concepts and Computer Evolution \nOnce the opcode is in the IR, the execute cycle is performed. Control circuitry \ninterprets the opcode and executes the instruction by sending out the appropri-\nate control signals to cause data to be moved or an operation to be performed by \nthe ALU.\nThe IAS computer had a total of 21 instructions, which are listed in Table 1.1. \nThese can be grouped as follows:\n■\n■Data transfer: Move data between memory and ALU registers or between two \nALU registers.\n■\n■Unconditional branch: Normally, the control unit executes instructions in \nsequence from memory. This sequence can be changed by a branch instruc-\ntion, which facilitates repetitive operations.\nTable 1.1  The IAS Instruction Set\nInstruction \nType\nOpcode\nSymbolic \nRepresentation\nDescription\nData transfer\n00001010\nLOAD MQ\nTransfer contents of register MQ to the accumulator AC\n00001001\nLOAD MQ,M(X)\nTransfer contents of memory location X to MQ\n00100001\nSTOR M(X)\nTransfer contents of accumulator to memory location X\n00000001\nLOAD M(X)\nTransfer M(X) to the accumulator\n00000010\nLOAD –M(X)\nTransfer –M(X) to the accumulator\n00000011\nLOAD |M(X)|\nTransfer absolute value of M(X) to the accumulator\n00000100\nLOAD –|M(X)|\nTransfer –|M(X)| to the accumulator\nUnconditional  \nbranch\n00001101\nJUMP M(X,0:19)\nTake next instruction from left half of M(X)\n00001110\nJUMP M(X,20:39)\nTake next instruction from right half of M(X)\nConditional \nbranch\n00001111\nJUMP + M(X,0:19)\nIf number in the accumulator is nonnegative, take next \ninstruction from left half of M(X)\n00010000\nJUMP + M(X,20:39)\nIf number in the accumulator is nonnegative, take next \ninstruction from right half of M(X)\nArithmetic\n00000101\nADD M(X)\nAdd M(X) to AC; put the result in AC\n00000111\nADD |M(X)|\nAdd |M(X)| to AC; put the result in AC\n00000110\nSUB M(X)\nSubtract M(X) from AC; put the result in AC\n00001000\nSUB |M(X)|\nSubtract |M(X)| from AC; put the remainder in AC\n00001011\nMUL M(X)\nMultiply M(X) by MQ; put most significant bits of result \nin AC, put least significant bits in MQ\n00001100\nDIV M(X)\nDivide AC by M(X); put the quotient in MQ and the \nremainder in AC\n00010100\nLSH\nMultiply accumulator by 2; that is, shift left one bit position\n00010101\nRSH\nDivide accumulator by 2; that is, shift right one position\nAddress \nmodify\n00010010\nSTOR M(X,8:19)\nReplace left address field at M(X) by 12 rightmost bits \nof AC\n00010011\nSTOR M(X,28:39)\nReplace right address field at M(X) by 12 rightmost bits \nof AC\n■\n■Conditional branch: The branch can be made dependent on a condition, thus \nallowing decision points.\n■\n■Arithmetic: Operations performed by the ALU.\n■\n■Address modify: Permits addresses to be computed in the ALU and then \ninserted into instructions stored in memory. This allows a program consider-\nable addressing flexibility.\nTable 1.1 presents instructions (excluding I/O instructions) in a symbolic, \n­\neasy-​\n­\nto-​\n­\nread form. In binary form, each instruction must conform to the format of \nFigure 1.7b. The opcode portion (first 8 bits) specifies which of the 21 instructions is \nto be executed. The address portion (remaining 12 bits) specifies which of the 4,096 \nmemory locations is to be involved in the execution of the instruction.\nFigure 1.8 shows several examples of instruction execution by the control unit. \nNote that each operation requires several steps, some of which are quite elaborate. \nThe multiplication operation requires 39 suboperations, one for each bit position \nexcept that of the sign bit.\nThe Second Generation: Transistors\nThe first major change in the electronic computer came with the replacement of the \nvacuum tube by the transistor. The transistor, which is smaller, cheaper, and gener-\nates less heat than a vacuum tube, can be used in the same way as a vacuum tube to \nconstruct computers. Unlike the vacuum tube, which requires wires, metal plates, a \nglass capsule, and a vacuum, the transistor is a ­\nsolid-​\n­\nstate device, made from silicon.\nThe transistor was invented at Bell Labs in 1947 and by the 1950s had launched \nan electronic revolution. It was not until the late 1950s, however, that fully transis-\ntorized computers were commercially available. The use of the transistor defines \nthe second generation of computers. It has become widely accepted to classify com-\nputers into generations based on the fundamental hardware technology employed \n(Table 1.2). Each new generation is characterized by greater processing perfor-\nmance, larger memory capacity, and smaller size than the previous one.\nBut there are other changes as well. The second generation saw the intro-\nduction of more complex arithmetic and logic units and control units, the use of \n­\nhigh-​\n­\nlevel programming languages, and the provision of system software with the \n1.3 / A Brief History of Computers   17\nTable 1.2  Computer Generations\nGeneration\nApproximate \nDates\nTechnology\nTypical Speed  \n(operations per second)\n1\n1946–1957\nVacuum tube\n40,000\n2\n1957–1964\nTransistor\n200,000\n3\n1965–1971\n­\nSmall-​\n­\n and ­\nmedium-​\n­\nscale \nintegration\n1,000,000\n4\n1972–1977\nLarge scale integration\n10,000,000\n5\n1978–1991\nVery large scale integration\n100,000,000\n6\n1991–\nUltra large scale integration\n>1,000,000,000\n18    Chapter 1 / Basic Concepts and Computer Evolution \ncomputer. In broad terms, system software provided the ability to load programs, \nmove data to peripherals, and libraries to perform common computations, similar \nto what modern operating systems, such as Windows and Linux, do.\nIt will be useful to examine an important member of the second generation: the \nIBM 7094 [BELL71]. From the introduction of the 700 series in 1952 to the introduc-\ntion of the last member of the 7000 series in 1964, this IBM product line underwent \nan evolution that is typical of computer products. Successive members of the product \nline showed increased performance, increased capacity, and/or lower cost.\nThe size of main memory, in multiples of 210 36-bit words, grew from \n \n2k (1k = 210) to 32k words,7 while the time to access one word of memory, the mem-\nory cycle time, fell from 30 ms to 1.4 ms. The number of opcodes grew from a modest \n24 to 185.\nAlso, over the lifetime of this series of computers, the relative speed of the \nCPU increased by a factor of 50. Speed improvements are achieved by improved \nelectronics (e.g., a transistor implementation is faster than a vacuum tube imple-\nmentation) and more complex circuitry. For example, the IBM 7094 includes an \nInstruction Backup Register, used to buffer the next instruction. The control unit \nfetches two adjacent words from memory for an instruction fetch. Except for the \noccurrence of a branching instruction, which is relatively infrequent (perhaps 10 to \n15%), this means that the control unit has to access memory for an instruction on \nonly half the instruction cycles. This prefetching significantly reduces the average \ninstruction cycle time.\nFigure 1.9 shows a large (many peripherals) configuration for an IBM 7094, \nwhich is representative of ­\nsecond-​\n­\ngeneration computers. Several differences from \nthe IAS computer are worth noting. The most important of these is the use of data \nchannels. A data channel is an independent I/O module with its own processor and \ninstruction set. In a computer system with such devices, the CPU does not execute \ndetailed I/O instructions. Such instructions are stored in a main memory to be \nexecuted by a ­\nspecial-​\n­\npurpose processor in the data channel itself. The CPU initi-\nates an I/O transfer by sending a control signal to the data channel, instructing it to \nexecute a sequence of instructions in memory. The data channel performs its task \nindependently of the CPU and signals the CPU when the operation is complete. \nThis arrangement relieves the CPU of a considerable processing burden.\nAnother new feature is the multiplexor, which is the central termination \npoint for data channels, the CPU, and memory. The multiplexor schedules access \nto the memory from the CPU and data channels, allowing these devices to act \nindependently.\nThe Third Generation: Integrated Circuits\nA single, ­\nself-​\n­\ncontained transistor is called a discrete component. Throughout \n \nthe 1950s and early 1960s, electronic equipment was composed largely of discrete \n­\ncomponents—​\n­\ntransistors, resistors, capacitors, and so on. Discrete components were \nmanufactured separately, packaged in their own containers, and soldered or wired \n7A discussion of the uses of numerical prefixes, such as kilo and giga, is contained in a supporting docu-\nment at the Computer Science Student Resource Site at ComputerScienceStudent.com.\ntogether onto ­\nMasonite-​\n­\nlike circuit boards, which were then installed in computers, \noscilloscopes, and other electronic equipment. Whenever an electronic device called \nfor a transistor, a little tube of metal containing a ­\npinhead-​\n­\nsized piece of silicon had \nto be soldered to a circuit board. The entire manufacturing process, from transistor \nto circuit board, was expensive and cumbersome.\nThese facts of life were beginning to create problems in the computer indus-\ntry. Early ­\nsecond-​\n­\ngeneration computers contained about 10,000 transistors. This \nfigure grew to the hundreds of thousands, making the manufacture of newer, more \npowerful machines increasingly difficult.\nIn 1958 came the achievement that revolutionized electronics and started the \nera of microelectronics: the invention of the integrated circuit. It is the integrated \ncircuit that defines the third generation of computers. In this section, we provide a \nbrief introduction to the technology of integrated circuits. Then we look at perhaps \nthe two most important members of the third generation, both of which were intro-\nduced at the beginning of that era: the IBM System/360 and the DEC ­\nPDP-​\n­\n8.\nmicroelectronics Microelectronics means, literally, “small electronics.” Since the \nbeginnings of digital electronics and the computer industry, there has been a persistent \nand consistent trend toward the reduction in size of digital electronic circuits. Before \nexamining the implications and benefits of this trend, we need to say something about \nthe nature of digital electronics. A more detailed discussion is found in Chapter 11.\nCPU\nMemory\nIBM 7094 computer\nPeripheral devices\nData\nchannel\nMag tape\nunits\nCard\npunch\nLine\nprinter\nCard\nreader\nDrum\nDisk\nDisk\nHyper-\ntapes\nTeleprocessing\nequipment\nData\nchannel\nData\nchannel\nData\nchannel\nMulti-\nplexor\nFigure 1.9  An IBM 7094 Configuration\n1.3 / A Brief History of Computers   19\n20    Chapter 1 / Basic Concepts and Computer Evolution \nThe basic elements of a digital computer, as we know, must perform data stor-\nage, movement, processing, and control functions. Only two fundamental types of \ncomponents are required (Figure 1.10): gates and memory cells. A gate is a device \nthat implements a simple Boolean or logical function. For example, an AND gate \nwith inputs A and B and output C implements the expression IF A AND B ARE \nTRUE THEN C IS TRUE. Such devices are called gates because they control data \nflow in much the same way that canal gates control the flow of water. The memory \ncell is a device that can store 1 bit of data; that is, the device can be in one of two \nstable states at any time. By interconnecting large numbers of these fundamental \ndevices, we can construct a computer. We can relate this to our four basic functions \nas follows:\n■\n■Data storage:  Provided by memory cells.\n■\n■Data processing:  Provided by gates.\n■\n■Data movement:  The paths among components are used to move data from \nmemory to memory and from memory through gates to memory.\n■\n■Control:  The paths among components can carry control signals. For example, \na gate will have one or two data inputs plus a control signal input that activates \nthe gate. When the control signal is ON, the gate performs its function on the \ndata inputs and produces a data output. Conversely, when the control signal \nis OFF, the output line is null, such as the one produced by a high impedance \nstate. Similarly, the memory cell will store the bit that is on its input lead when \nthe WRITE control signal is ON and will place the bit that is in the cell on its \noutput lead when the READ control signal is ON.\nThus, a computer consists of gates, memory cells, and interconnections among \nthese elements. The gates and memory cells are, in turn, constructed of simple elec-\ntronic components, such as transistors and capacitors.\nThe integrated circuit exploits the fact that such components as transistors, \nresistors, and conductors can be fabricated from a semiconductor such as silicon. \nIt is merely an extension of the ­\nsolid-​\n­\nstate art to fabricate an entire circuit in a tiny \npiece of silicon rather than assemble discrete components made from separate \npieces of silicon into the same circuit. Many transistors can be produced at the same \ntime on a single wafer of silicon. Equally important, these transistors can be con-\nnected with a process of metallization to form circuits.\nBoolean\nlogic\nfunction\nInput\nActivate\nsignal\n(a) Gate\nOutput\n•\n•\n•\nBinary\nstorage\ncell\nInput\nRead\nWrite\n(b) Memory cell\nOutput\nFigure 1.10  Fundamental Computer Elements\nFigure 1.11 depicts the key concepts in an integrated circuit. A thin wafer of \nsilicon is divided into a matrix of small areas, each a few millimeters square. The \nidentical circuit pattern is fabricated in each area, and the wafer is broken up into \nchips. Each chip consists of many gates and/or memory cells plus a number of input \nand output attachment points. This chip is then packaged in housing that protects \nit and provides pins for attachment to devices beyond the chip. A number of these \npackages can then be interconnected on a printed circuit board to produce larger \nand more complex circuits.\nInitially, only a few gates or memory cells could be reliably manufactured and \npackaged together. These early integrated circuits are referred to as ­\nsmall-​\n­\nscale \nintegration (SSI). As time went on, it became possible to pack more and more com-\nponents on the same chip. This growth in density is illustrated in Figure 1.12; it is \none of the most remarkable technological trends ever recorded.8 This figure reflects \nthe famous Moore’s law, which was propounded by Gordon Moore, cofounder of \nIntel, in 1965 [MOOR65]. Moore observed that the number of transistors that could \nbe put on a single chip was doubling every year, and correctly predicted that this \npace would continue into the near future. To the surprise of many, including Moore, \nthe pace continued year after year and decade after decade. The pace slowed to a \ndoubling every 18 months in the 1970s but has sustained that rate ever since.\nThe consequences of Moore’s law are profound:\n1.\t The cost of a chip has remained virtually unchanged during this period of rapid \ngrowth in density. This means that the cost of computer logic and memory cir-\ncuitry has fallen at a dramatic rate.\nWafer\nChip\nGate\nPackaged\nchip\nFigure 1.11  Relationship among \nWafer, Chip, and Gate\n1.3 / A Brief History of Computers   21\n8Note that the vertical axis uses a log scale. A basic review of log scales is in the math refresher document \nat the Computer Science Student Resource Site at ComputerScienceStudent.com.\n22    Chapter 1 / Basic Concepts and Computer Evolution \n2.\t Because logic and memory elements are placed closer together on more \ndensely packed chips, the electrical path length is shortened, increasing oper-\nating speed.\n3.\t The computer becomes smaller, making it more convenient to place in a vari-\nety of environments.\n4.\t There is a reduction in power requirements.\n5.\t The interconnections on the integrated circuit are much more reliable than \nsolder connections. With more circuitry on each chip, there are fewer inter-\nchip connections.\nibm system/360 By 1964, IBM had a firm grip on the computer market with \nits 7000 series of machines. In that year, IBM announced the System/360, a new \nfamily of computer products. Although the announcement itself was no surprise, it \ncontained some unpleasant news for current IBM customers: the 360 product line \nwas incompatible with older IBM machines. Thus, the transition to the 360 would \nbe difficult for the current customer base, but IBM felt this was necessary to break \nout of some of the constraints of the 7000 architecture and to produce a system \ncapable of evolving with the new integrated circuit technology [PADE81, GIFF87]. \nThe strategy paid off both financially and technically. The 360 was the success of \nthe decade and cemented IBM as the overwhelmingly dominant computer vendor, \nwith a market share above 70%. And, with some modifications and extensions, the \narchitecture of the 360 remains to this day the architecture of IBM’s mainframe9 \ncomputers. Examples using this architecture can be found throughout this text.\nThe System/360 was the industry’s first planned family of computers. The family \ncovered a wide range of performance and cost. The models were compatible in the \n1\n1947\nFirst working\ntransistor\nMoore’s law\npromulgated\nInvention of\nintegrated circuit\n50\n55\n60\n65\n70\n75\n80\n85\n90\n95\n2000\n05\n11\n10\n100\n1,000\n10,000\n100,000\n10 m\n100 m\n1 bn\n10 bn\n100 bn\nFigure 1.12  Growth in Transistor Count on Integrated Circuits\n9The term mainframe is used for the larger, most powerful computers other than supercomputers. Typical \ncharacteristics of a mainframe are that it supports a large database, has elaborate I/O hardware, and is \nused in a central data processing facility.\nsense that a program written for one model should be capable of being executed by \nanother model in the series, with only a difference in the time it takes to execute.\nThe concept of a family of compatible computers was both novel and extremely \nsuccessful. A customer with modest requirements and a budget to match could start \nwith the relatively inexpensive Model 30. Later, if the customer’s needs grew, it was \npossible to upgrade to a faster machine with more memory without sacrificing the \ninvestment in ­\nalready-​\n­\ndeveloped software. The characteristics of a family are as follows:\n■\n■Similar or identical instruction set: In many cases, the exact same set of \nmachine instructions is supported on all members of the family. Thus, a pro-\ngram that executes on one machine will also execute on any other. In some \ncases, the lower end of the family has an instruction set that is a subset of \nthat of the top end of the family. This means that programs can move up but \nnot down.\n■\n■Similar or identical operating system: The same basic operating system is \navailable for all family members. In some cases, additional features are added \nto the ­\nhigher-​\n­\nend members.\n■\n■Increasing speed: The rate of instruction execution increases in going from \nlower to higher family members.\n■\n■Increasing number of I/O ports: The number of I/O ports increases in going \nfrom lower to higher family members.\n■\n■Increasing memory size: The size of main memory increases in going from \nlower to higher family members.\n■\n■Increasing cost: At a given point in time, the cost of a system increases in going \nfrom lower to higher family members.\nHow could such a family concept be implemented? Differences were achieved \nbased on three factors: basic speed, size, and degree of simultaneity [STEV64]. For \nexample, greater speed in the execution of a given instruction could be gained by \nthe use of more complex circuitry in the ALU, allowing suboperations to be car-\nried out in parallel. Another way of increasing speed was to increase the width of \nthe data path between main memory and the CPU. On the Model 30, only 1 byte \n(8 bits) could be fetched from main memory at a time, whereas 8 bytes could be \nfetched at a time on the Model 75.\nThe System/360 not only dictated the future course of IBM but also had a pro-\nfound impact on the entire industry. Many of its features have become standard on \nother large computers.\ndec ­\npdp-​\n­\n8 In the same year that IBM shipped its first System/360, another \nmomentous first shipment occurred: ­\nPDP-​\n­\n8 from Digital Equipment Corporation \n(DEC). At a time when the average computer required an ­\nair-​\n­\nconditioned room, \nthe ­\nPDP-​\n­\n8 (dubbed a minicomputer by the industry, after the miniskirt of the day) \nwas small enough that it could be placed on top of a lab bench or be built into \nother equipment. It could not do everything the mainframe could, but at $16,000, it \nwas cheap enough for each lab technician to have one. In contrast, the System/360 \nseries of mainframe computers introduced just a few months before cost hundreds \nof thousands of dollars.\n1.3 / A Brief History of Computers   23\n24    Chapter 1 / Basic Concepts and Computer Evolution \nThe low cost and small size of the ­\nPDP-​\n­\n8 enabled another manufacturer to \npurchase a ­\nPDP-​\n­\n8 and integrate it into a total system for resale. These other manu-\nfacturers came to be known as original equipment manufacturers (OEMs), and the \nOEM market became and remains a major segment of the computer marketplace.\nIn contrast to the ­\ncentral-​\n­\nswitched architecture (Figure 1.9) used by IBM on its \n700/7000 and 360 systems, later models of the ­\nPDP-​\n­\n8 used a structure that became vir-\ntually universal for microcomputers: the bus structure. This is illustrated in Figure 1.13. \nThe ­\nPDP-​\n­\n8 bus, called the Omnibus, consists of 96 separate signal paths, used to carry \ncontrol, address, and data signals. Because all system components share a common \nset of signal paths, their use can be controlled by the CPU. This architecture is highly \nflexible, allowing modules to be plugged into the bus to create various configurations. \nIt is only in recent years that the bus structure has given way to a structure known as \n­\npoint-​\n­\nto-​\n­\npoint interconnect, described in Chapter 3.\nLater Generations\nBeyond the third generation there is less general agreement on defining generations \nof computers. Table 1.2 suggests that there have been a number of later generations, \nbased on advances in integrated circuit technology. With the introduction of ­\nlarge-​\n­\nscale integration (LSI), more than 1,000 components can be placed on a single inte-\ngrated circuit chip. ­\nVery-​\n­\nlarge-​\n­\nscale integration (VLSI) achieved more than 10,000 \ncomponents per chip, while current ­\nultra-​\n­\nlarge-​\n­\nscale integration (ULSI) chips can \ncontain more than one billion components.\nWith the rapid pace of technology, the high rate of introduction of new prod-\nucts, and the importance of software and communications as well as hardware, the \nclassification by generation becomes less clear and less meaningful. In this section, \nwe mention two of the most important of developments in later generations.\nsemiconductor memory The first application of integrated circuit technology \nto computers was the construction of the processor (the control unit and the \narithmetic and logic unit) out of integrated circuit chips. But it was also found that \nthis same technology could be used to construct memories.\nIn the 1950s and 1960s, most computer memory was constructed from tiny \nrings of ferromagnetic material, each about a sixteenth of an inch in diameter. \nThese rings were strung up on grids of fine wires suspended on small screens inside \nthe computer. Magnetized one way, a ring (called a core) represented a one; mag-\nnetized the other way, it stood for a zero. ­\nMagnetic-​\n­\ncore memory was rather fast; \nit took as little as a millionth of a second to read a bit stored in memory. But it was \nConsole\ncontroller\nCPU\nOmnibus\nMain\nmemory\nI/O\nmodule\nI/O\nmodule\n• • •\nFigure 1.13  ­\nPDP-​\n­\n8 Bus Structure\nexpensive and bulky, and used destructive readout: The simple act of reading a core \nerased the data stored in it. It was therefore necessary to install circuits to restore \nthe data as soon as it had been extracted.\nThen, in 1970, Fairchild produced the first relatively capacious semiconductor \nmemory. This chip, about the size of a single core, could hold 256 bits of memory. It \nwas nondestructive and much faster than core. It took only 70 billionths of a second \nto read a bit. However, the cost per bit was higher than for that of core.\nIn 1974, a seminal event occurred: The price per bit of semiconductor memory \ndropped below the price per bit of core memory. Following this, there has been a con-\ntinuing and rapid decline in memory cost accompanied by a corresponding increase in \nphysical memory density. This has led the way to smaller, faster machines with mem-\nory sizes of larger and more expensive machines from just a few years earlier. Devel-\nopments in memory technology, together with developments in processor technology \nto be discussed next, changed the nature of computers in less than a decade. Although \nbulky, expensive computers remain a part of the landscape, the computer has also \nbeen brought out to the “end user,” with office machines and personal computers.\nSince 1970, semiconductor memory has been through 13 generations: 1k, 4k, \n16k, 64k, 256k, 1M, 4M, 16M, 64M, 256M, 1G, 4G, and, as of this writing, 8 Gb \non a single chip (1 k = 210, 1 M = 220, 1 G = 230). Each generation has provided \nincreased storage density, accompanied by declining cost per bit and declining \naccess time. Densities are projected to reach 16 Gb by 2018 and 32 Gb by 2023 \n[ITRS14].\nmicroprocessors Just as the density of elements on memory chips has continued \nto rise, so has the density of elements on processor chips. As time went on, more \nand more elements were placed on each chip, so that fewer and fewer chips were \nneeded to construct a single computer processor.\nA breakthrough was achieved in 1971, when Intel developed its 4004. The \n4004 was the first chip to contain all of the components of a CPU on a single chip: \nThe microprocessor was born.\nThe 4004 can add two 4-bit numbers and can multiply only by repeated addi-\ntion. By today’s standards, the 4004 is hopelessly primitive, but it marked the begin-\nning of a continuing evolution of microprocessor capability and power.\nThis evolution can be seen most easily in the number of bits that the processor \ndeals with at a time. There is no ­\nclear-​\n­\ncut measure of this, but perhaps the best meas-\nure is the data bus width: the number of bits of data that can be brought into or sent \nout of the processor at a time. Another measure is the number of bits in the accumu-\nlator or in the set of ­\ngeneral-​\n­\npurpose registers. Often, these measures coincide, but \nnot always. For example, a number of microprocessors were developed that operate \non 16-bit numbers in registers but can only read and write 8 bits at a time.\nThe next major step in the evolution of the microprocessor was the introduc-\ntion in 1972 of the Intel 8008. This was the first 8-bit microprocessor and was almost \ntwice as complex as the 4004.\nNeither of these steps was to have the impact of the next major event: the \nintroduction in 1974 of the Intel 8080. This was the first ­\ngeneral-​\n­\npurpose micropro-\ncessor. Whereas the 4004 and the 8008 had been designed for specific applications, \nthe 8080 was designed to be the CPU of a ­\ngeneral-​\n­\npurpose microcomputer. Like the \n1.3 / A Brief History of Computers   25\n26    Chapter 1 / Basic Concepts and Computer Evolution \nTable 1.3  Evolution of Intel Microprocessors (page 1 of 2)\n(a) 1970s Processors\n4004\n8008\n8080\n8086\n8088\nIntroduced\n1971\n1972\n1974\n1978\n1979\nClock speeds\n108 kHz\n108 kHz\n2 MHz\n5 MHz, 8 MHz, 10 MHz\n5 MHz, 8 MHz\nBus width\n4 bits\n8 bits\n8 bits\n16 bits\n8 bits\nNumber of transistors\n2,300\n3,500\n6,000\n29,000\n29,000\nFeature size (mm)\n10\n8\n6\n3\n6\nAddressable memory\n640 bytes\n16 KB\n64 KB\n1 MB\n1 MB\n(b) 1980s Processors\n80286\n386TM DX\n386TM SX\n486TM DX CPU\nIntroduced\n1982\n1985\n1988\n1989\nClock speeds\n6–12.5 MHz\n16–33 MHz\n16–33 MHz\n25–50 MHz\nBus width\n16 bits\n32 bits\n16 bits\n32 bits\nNumber of transistors\n134,000\n275,000\n275,000\n1.2 million\nFeature size ( µm)\n1.5\n1\n1\n0.8–1\nAddressable memory\n16 MB\n4 GB\n16 MB\n4 GB\nVirtual memory\n1 GB\n64 TB\n64 TB\n64 TB\nCache\n—\n—\n—\n8 kB\n(c) 1990s Processors\n486TM SX\nPentium\nPentium Pro\nPentium II\nIntroduced\n1991\n1993\n1995\n1997\nClock speeds\n16–33 MHz\n60–166 MHz,\n150–200 MHz\n200–300 MHz\nBus width\n32 bits\n32 bits\n64 bits\n64 bits\nNumber of transistors\n1.185 million\n3.1 million\n5.5 million\n7.5 million\nFeature size ( µm)\n1\n0.8\n0.6\n0.35\nAddressable memory\n4 GB\n4 GB\n64 GB\n64 GB\nVirtual memory\n64 TB\n64 TB\n64 TB\n64 TB\nCache\n8 kB\n8 kB\n512 kB L1 and  \n1 MB L2\n512 kB L2\n8008, the 8080 is an 8-bit microprocessor. The 8080, however, is faster, has a richer \ninstruction set, and has a large addressing capability.\nAbout the same time, 16-bit microprocessors began to be developed. How-\never, it was not until the end of the 1970s that powerful, ­\ngeneral-​\n­\npurpose 16-bit \nmicroprocessors appeared. One of these was the 8086. The next step in this trend \noccurred in 1981, when both Bell Labs and ­\nHewlett-​\n­\nPackard developed 32-bit, \n­\nsingle-​\n­\nchip microprocessors. Intel introduced its own 32-bit microprocessor, the \n80386, in 1985 (Table 1.3).\n1.4 / The Evolution of the Intel x86 Architecture   27\n(d) Recent Processors\nPentium III\nPentium 4\nCore 2 Duo\nCore i7 EE 4960X\nIntroduced\n1999\n2000\n2006\n2013\nClock speeds\n450–660 MHz\n1.3–1.8 GHz\n1.06–1.2 GHz\n4 GHz\nBus width\n64 bits\n64 bits\n64 bits\n64 bits\nNumber of transistors\n9.5 million\n42 million\n167 million\n1.86 billion\nFeature size (nm)\n250\n180\n65\n22\nAddressable memory\n64 GB\n64 GB\n64 GB\n64 GB\nVirtual memory\n64 TB\n64 TB\n64 TB\n64 TB\nCache\n512 kB L2\n256 kB L2\n2 MB L2\n1.5 MB L2/15 MB L3\nNumber of cores\n1\n1\n2\n6\n\t 1.4\t The Evolution of the Intel x86 Architecture\nThroughout this book, we rely on many concrete examples of computer design and \nimplementation to illustrate concepts and to illuminate ­\ntrade-​\n­\noffs. Numerous sys-\ntems, both contemporary and historical, provide examples of important computer \narchitecture design features. But the book relies principally on examples from two \nprocessor families: the Intel x86 and the ARM architectures. The current x86 offer-\nings represent the results of decades of design effort on complex instruction set com-\nputers (CISCs). The x86 incorporates the sophisticated design principles once found \nonly on mainframes and supercomputers and serves as an excellent example of CISC \ndesign. An alternative approach to processor design is the reduced instruction set \ncomputer (RISC). The ARM architecture is used in a wide variety of embedded sys-\ntems and is one of the most powerful and ­\nbest-​\n­\ndesigned ­\nRISC-​\n­\nbased systems on the \nmarket. In this section and the next, we provide a brief overview of these two systems.\nIn terms of market share, Intel has ranked as the number one maker of micro-\nprocessors for ­\nnon-​\n­\nembedded systems for decades, a position it seems unlikely to \nyield. The evolution of its flagship microprocessor product serves as a good indica-\ntor of the evolution of computer technology in general.\nTable 1.3 shows that evolution. Interestingly, as microprocessors have grown \nfaster and much more complex, Intel has actually picked up the pace. Intel used \nto develop microprocessors one after another, every four years. But Intel hopes \nto keep rivals at bay by trimming a year or two off this development time, and has \ndone so with the most recent x86 generations.10\n10Intel refers to this as the ­\ntick-​\n­\ntock model. Using this model, Intel has successfully delivered ­\nnext-​\n­\ngeneration silicon technology as well as new processor microarchitecture on alternating years for the \npast several years. See http://www.intel.com/content/www/us/en/­\nsilicon-­\ninnovations/intel-tick-tock- \nmodel-general.html.\n28    Chapter 1 / Basic Concepts and Computer Evolution \nIt is worthwhile to list some of the highlights of the evolution of the Intel prod-\nuct line:\n■\n■8080: The world’s first ­\ngeneral-​\n­\npurpose microprocessor. This was an 8-bit \nmachine, with an 8-bit data path to memory. The 8080 was used in the first \npersonal computer, the Altair.\n■\n■8086: A far more powerful, 16-bit machine. In addition to a wider data path \nand larger registers, the 8086 sported an instruction cache, or queue, that \nprefetches a few instructions before they are executed. A variant of this pro-\ncessor, the 8088, was used in IBM’s first personal computer, securing the suc-\ncess of Intel. The 8086 is the first appearance of the x86 architecture.\n■\n■80286: This extension of the 8086 enabled addressing a 16-MB memory instead \nof just 1 MB.\n■\n■80386: Intel’s first 32-bit machine, and a major overhaul of the product. With \na 32-bit architecture, the 80386 rivaled the complexity and power of minicom-\nputers and mainframes introduced just a few years earlier. This was the first \nIntel processor to support multitasking, meaning it could run multiple pro-\ngrams at the same time.\n■\n■80486: The 80486 introduced the use of much more sophisticated and power-\nful cache technology and sophisticated instruction pipelining. The 80486 also \noffered a ­\nbuilt-​\n­\nin math coprocessor, offloading complex math operations from \nthe main CPU.\n■\n■Pentium: With the Pentium, Intel introduced the use of superscalar tech-\nniques, which allow multiple instructions to execute in parallel.\n■\n■Pentium Pro: The Pentium Pro continued the move into superscalar organiza-\ntion begun with the Pentium, with aggressive use of register renaming, branch \nprediction, data flow analysis, and speculative execution.\n■\n■Pentium II: The Pentium II incorporated Intel MMX technology, which is \ndesigned specifically to process video, audio, and graphics data efficiently.\n■\n■Pentium III: The Pentium III incorporates additional ­\nfloating-​\n­\npoint instruc-\ntions: The Streaming SIMD Extensions (SSE) instruction set extension added \n70 new instructions designed to increase performance when exactly the same \noperations are to be performed on multiple data objects. Typical applications \nare digital signal processing and graphics processing.\n■\n■Pentium 4: The Pentium 4 includes additional ­\nfloating-​\n­\npoint and other \nenhancements for multimedia.11\n■\n■Core: This is the first Intel x86 microprocessor with a dual core, referring to \nthe implementation of two cores on a single chip.\n■\n■Core 2: The Core 2 extends the Core architecture to 64 bits. The Core 2 Quad \nprovides four cores on a single chip. More recent Core offerings have up to 10 \ncores per chip. An important addition to the architecture was the Advanced \nVector Extensions instruction set that provided a set of 256-bit, and then 512-\nbit, instructions for efficient processing of vector data.\n11With the Pentium 4, Intel switched from Roman numerals to Arabic numerals for model numbers.\n1.5 / Embedded Systems   29\nAlmost 40 years after its introduction in 1978, the x86 architecture continues to \ndominate the processor market outside of embedded systems. Although the organiza-\ntion and technology of the x86 machines have changed dramatically over the decades, \nthe instruction set architecture has evolved to remain backward compatible with ear-\nlier versions. Thus, any program written on an older version of the x86 architecture \ncan execute on newer versions. All changes to the instruction set architecture have \ninvolved additions to the instruction set, with no subtractions. The rate of change has \nbeen the addition of roughly one instruction per month added to the architecture \n[ANTH08], so that there are now thousands of instructions in the instruction set.\nThe x86 provides an excellent illustration of the advances in computer hard-\nware over the past 35 years. The 1978 8086 was introduced with a clock speed of \n5 MHz and had 29,000 transistors. A ­\nsix-​\n­\ncore Core i7 EE 4960X introduced in 2013 \noperates at 4 GHz, a speedup of a factor of 800, and has 1.86 billion transistors, \nabout 64,000 times as many as the 8086. Yet the Core i7 EE 4960X is in only a \nslightly larger package than the 8086 and has a comparable cost.\n\t 1.5\t Embedded Systems\nThe term embedded system refers to the use of electronics and software within a \nproduct, as opposed to a ­\ngeneral-​\n­\npurpose computer, such as a laptop or desktop sys-\ntem. Millions of computers are sold every year, including laptops, personal comput-\ners, workstations, servers, mainframes, and supercomputers. In contrast, billions of \ncomputer systems are produced each year that are embedded within larger devices. \nToday, many, perhaps most, devices that use electric power have an embedded com-\nputing system. It is likely that in the near future virtually all such devices will have \nembedded computing systems.\nTypes of devices with embedded systems are almost too numerous to list. \nExamples include cell phones, digital cameras, video cameras, calculators, micro-\nwave ovens, home security systems, washing machines, lighting systems, ther-\nmostats, printers, various automotive systems (e.g., transmission control, cruise \ncontrol, fuel injection, ­\nanti-​\n­\nlock brakes, and suspension systems), tennis rack-\nets, toothbrushes, and numerous types of sensors and actuators in automated \nsystems.\nOften, embedded systems are tightly coupled to their environment. This can \ngive rise to ­\nreal-​\n­\ntime constraints imposed by the need to interact with the environ-\nment. Constraints, such as required speeds of motion, required precision of meas-\nurement, and required time durations, dictate the timing of software operations. If \nmultiple activities must be managed simultaneously, this imposes more complex \n­\nreal-​\n­\ntime constraints.\nFigure 1.14 shows in general terms an embedded system organization. In addi-\ntion to the processor and memory, there are a number of elements that differ from \nthe typical desktop or laptop computer:\n■\n■There may be a variety of interfaces that enable the system to measure, manip-\nulate, and otherwise interact with the external environment. Embedded sys-\ntems often interact (sense, manipulate, and communicate) with external world \nthrough sensors and actuators and hence are typically reactive systems; a \n30    Chapter 1 / Basic Concepts and Computer Evolution \nreactive system is in continual interaction with the environment and executes \nat a pace determined by that environment.\n■\n■The human interface may be as simple as a flashing light or as complicated as \n­\nreal-​\n­\ntime robotic vision. In many cases, there is no human interface.\n■\n■The diagnostic port may be used for diagnosing the system that is being \n­\ncontrolled—​\n­\nnot just for diagnosing the computer.\n■\n■­\nSpecial-​\n­\npurpose field programmable (FPGA), ­\napplication-​\n­\nspecific (ASIC), or \neven nondigital hardware may be used to increase performance or reliability.\n■\n■Software often has a fixed function and is specific to the application.\n■\n■Efficiency is of paramount importance for embedded systems. They are opti-\nmized for energy, code size, execution time, weight and dimensions, and cost.\nThere are several noteworthy areas of similarity to ­\ngeneral-​\n­\npurpose computer \nsystems as well:\n■\n■Even with nominally fixed function software, the ability to field upgrade to fix \nbugs, to improve security, and to add functionality, has become very important \nfor embedded systems, and not just in consumer devices.\n■\n■One comparatively recent development has been of embedded system plat-\nforms that support a wide variety of apps. Good examples of this are smart-\nphones and audio/visual devices, such as smart TVs.\nThe Internet of Things\nIt is worthwhile to separately callout  one of the major drivers in the ­\nproliferation of \nembedded systems. The Internet of things (IoT) is a term that refers to the expanding \nMemory\nCustom\nlogic\nHuman\ninterface\nDiagnostic\nport\nProcessor\nD/A\nConversion\nActuators/\nindicators\nA/D\nconversion\nSensors\nFigure 1.14  Possible Organization of an Embedded \nSystem\n1.5 / Embedded Systems   31\ninterconnection of smart devices, ranging from appliances to tiny sensors. A domi-\nnant theme is the embedding of ­\nshort-​\n­\nrange mobile transceivers into a wide array of \ngadgets and everyday items, enabling new forms of communication between people \nand things, and between things themselves. The Internet now supports the intercon-\nnection of billions of industrial and personal objects, usually through cloud systems. \nThe objects deliver sensor information, act on their environment, and, in some cases, \nmodify themselves, to create overall management of a larger system, like a factory \nor city.\nThe IoT is primarily driven by deeply embedded devices (defined below). \nThese devices are ­\nlow-​\n­\nbandwidth, ­\nlow-​\n­\nrepetition ­\ndata-​\n­\ncapture, and ­\nlow-​\n­\nbandwidth \n­\ndata-​\n­\nusage appliances that communicate with each other and provide data via user \ninterfaces. Embedded appliances, such as ­\nhigh-​\n­\nresolution video security cameras, \nvideo VoIP phones, and a handful of others, require ­\nhigh-​\n­\nbandwidth streaming \ncapabilities. Yet countless products simply require packets of data to be intermit-\ntently delivered.\nWith reference to the end systems supported, the Internet has gone through \nroughly four generations of deployment culminating in the IoT:\n1.\t Information technology (IT): PCs, servers, routers, firewalls, and so on, bought \nas IT devices by enterprise IT people and primarily using wired connectivity.\n2.\t Operational technology (OT): Machines/appliances with embedded IT built \nby ­\nnon-​\n­\nIT companies, such as medical machinery, SCADA (supervisory con-\ntrol and data acquisition), process control, and kiosks, bought as appliances by \nenterprise OT people and primarily using wired connectivity.\n3.\t Personal technology: Smartphones, tablets, and eBook readers bought as IT \ndevices by consumers (employees) exclusively using wireless connectivity and \noften multiple forms of wireless connectivity.\n4.\t Sensor/actuator technology: ­\nSingle-​\n­\npurpose devices bought by consumers, IT, \nand OT people exclusively using wireless connectivity, generally of a single \nform, as part of larger systems.\nIt is the fourth generation that is usually thought of as the IoT, and it is marked \nby the use of billions of embedded devices.\nEmbedded Operating Systems\nThere are two general approaches to developing an embedded operating system \n(OS). The first approach is to take an existing OS and adapt it for the embedded \napplication. For example, there are embedded versions of Linux, Windows, and \nMac, as well as other commercial and proprietary operating systems specialized for \nembedded systems. The other approach is to design and implement an OS intended \nsolely for embedded use. An example of the latter is TinyOS, widely used in wireless \nsensor networks. This topic is explored in depth in [STAL15].\nApplication Processors versus Dedicated Processors\nIn this subsection, and the next two, we briefly introduce some terms commonly \nfound in the literature on embedded systems. Application processors are defined \n32    Chapter 1 / Basic Concepts and Computer Evolution \nby the processor’s ability to execute complex operating systems, such as Linux, \nAndroid, and Chrome. Thus, the application processor is ­\ngeneral-​\n­\npurpose in nature. \nA good example of the use of an embedded application processor is the smartphone. \nThe embedded system is designed to support numerous apps and perform a wide \nvariety of functions.\nMost embedded systems employ a dedicated processor, which, as the name \nimplies, is dedicated to one or a small number of specific tasks required by the host \ndevice. Because such an embedded system is dedicated to a specific task or tasks, \nthe processor and associated components can be engineered to reduce size and cost.\nMicroprocessors versus Microcontrollers\nAs we have seen, early microprocessor chips included registers, an ALU, and some \nsort of control unit or instruction processing logic. As transistor density increased, it \nbecame possible to increase the complexity of the instruction set architecture, and \nultimately to add memory and more than one processor. Contemporary micropro-\ncessor chips, as shown in Figure 1.2, include multiple cores and a substantial amount \nof cache memory.\nA microcontroller chip makes a substantially different use of the logic space \navailable. Figure 1.15 shows in general terms the elements typically found on a \nmicrocontroller chip. As shown, a microcontroller is a single chip that contains the \nprocessor, ­\nnon-​\n­\nvolatile memory for the program (ROM), volatile memory for input \nand output (RAM), a clock, and an I/O control unit. The processor portion of the \nmicrocontroller has a much lower silicon area than other microprocessors and much \nhigher energy efficiency. We examine microcontroller organization in more detail \nin Section 1.6.\nAlso called a “computer on a chip,” billions of microcontroller units are \nembedded each year in myriad products from toys to appliances to automobiles. For \nexample, a single vehicle can use 70 or more microcontrollers. Typically, especially \nfor the smaller, less expensive microcontrollers, they are used as dedicated proces-\nsors for specific tasks. For example, microcontrollers are heavily utilized in automa-\ntion processes. By providing simple reactions to input, they can control machinery, \nturn fans on and off, open and close valves, and so forth. They are integral parts of \nmodern industrial technology and are among the most inexpensive ways to produce \nmachinery that can handle extremely complex functionalities.\nMicrocontrollers come in a range of physical sizes and processing power. Pro-\ncessors range from 4-bit to 32-bit architectures. Microcontrollers tend to be much \nslower than microprocessors, typically operating in the MHz range rather than the \nGHz speeds of microprocessors. Another typical feature of a microcontroller is that \nit does not provide for human interaction. The microcontroller is programmed for a \nspecific task, embedded in its device, and executes as and when required.\nEmbedded versus Deeply Embedded Systems\nWe have, in this section, defined the concept of an embedded system. A subset of \nembedded systems, and a quite numerous subset, is referred to as deeply embed-\nded systems. Although this term is widely used in the technical and commercial \n1.6 / ARM Architecture   33\nliterature, you will search the Internet in vain (or at least I did) for a straightfor-\nward definition. Generally, we can say that a deeply embedded system has a proces-\nsor whose behavior is difficult to observe both by the programmer and the user. \n \nA deeply embedded system uses a microcontroller rather than a microprocessor, is \nnot programmable once the program logic for the device has been burned into ROM \n(­\nread-​\n­\nonly memory), and has no interaction with a user.\nDeeply embedded systems are dedicated, ­\nsingle-​\n­\npurpose devices that detect \nsomething in the environment, perform a basic level of processing, and then do some-\nthing with the results. Deeply embedded systems often have wireless capability and \nappear in networked configurations, such as networks of sensors deployed over a large \narea (e.g., factory, agricultural field). The Internet of things depends heavily on deeply \nembedded systems. Typically, deeply embedded systems have extreme resource con-\nstraints in terms of memory, processor size, time, and power consumption.\n\t 1.6\t ARM Architecture\nThe ARM architecture refers to a processor architecture that has evolved from \nRISC design principles and is used in embedded systems. Chapter  15 examines \nRISC design principles in detail. In this section, we give a brief overview of the \nARM architecture.\nA/D\nconverter\nAnalog data\nacquisition\nTemporary\ndata\nProcessor\nSystem\nbus\nRAM\nD/A\nconverter\nROM\nSerial I/O\nports\nEEPROM\nParallel I/O\nports\nTIMER\nProgram\nand data\nPermanent\ndata\nTiming\nfunctions\nAnalog data\ntransmission\nSend/receive\ndata\nPeripheral\ninterfaces\nFigure 1.15  Typical Microcontroller Chip Elements\n34    Chapter 1 / Basic Concepts and Computer Evolution \nARM Evolution\nARM is a family of ­\nRISC-​\n­\nbased microprocessors and microcontrollers designed by \nARM Holdings, Cambridge, England. The company doesn’t make processors but \ninstead designs microprocessor and multicore architectures and licenses them to man-\nufacturers. Specifically, ARM Holdings has two types of licensable products: proces-\nsors and processor architectures. For processors, the customer buys the rights to use \n­\nARM-​\n­\nsupplied design in their own chips. For a processor architecture, the customer \nbuys the rights to design their own processor compliant with ARM’s architecture.\nARM chips are ­\nhigh-​\n­\nspeed processors that are known for their small die size \nand low power requirements. They are widely used in smartphones and other hand-\nheld devices, including game systems, as well as a large variety of consumer prod-\nucts. ARM chips are the processors in Apple’s popular iPod and iPhone devices, \nand are used in virtually all Android smartphones as well. ARM is probably the \nmost widely used embedded processor architecture and indeed the most widely \nused processor architecture of any kind in the world [VANC14].\nThe origins of ARM technology can be traced back to the ­\nBritish-​\n­\nbased Acorn \nComputers company. In the early 1980s, Acorn was awarded a contract by the Brit-\nish Broadcasting Corporation (BBC) to develop a new microcomputer architecture \nfor the BBC Computer Literacy Project. The success of this contract enabled Acorn \nto go on to develop the first commercial RISC processor, the Acorn RISC Machine \n(ARM). The first version, ARM1, became operational in 1985 and was used for \ninternal research and development as well as being used as a coprocessor in the \nBBC machine.\nIn this early stage, Acorn used the company VLSI Technology to do the actual \nfabrication of the processor chips. VLSI was licensed to market the chip on its own \nand had some success in getting other companies to use the ARM in their products, \nparticularly as an embedded processor.\nThe ARM design matched a growing commercial need for a ­\nhigh-​\n­\nperformance, \n­\nlow-​\n­\npower-​\n­\nconsumption, ­\nsmall-​\n­\nsize, and ­\nlow-​\n­\ncost processor for embedded appli-\ncations. But further development was beyond the scope of Acorn’s capabilities. \nAccordingly, a new company was organized, with Acorn, VLSI, and Apple Com-\nputer as founding partners, known as ARM Ltd. The Acorn RISC Machine became \nAdvanced RISC Machines.12\nInstruction Set Architecture\nThe ARM instruction set is highly regular, designed for efficient implementation of \nthe processor and efficient execution. All instructions are 32 bits long and follow a \nregular format. This makes the ARM ISA suitable for implementation over a wide \nrange of products.\nAugmenting the basic ARM ISA is the Thumb instruction set, which is a ­\nre-​\n­\nencoded subset of the ARM instruction set. Thumb is designed to increase the per-\nformance of ARM implementations that use a 16-bit or narrower memory data bus, \n12The company dropped the designation Advanced RISC Machines in the late 1990s. It is now simply \nknown as the ARM architecture.\n1.6 / ARM Architecture   35\nand to allow better code density than provided by the ARM instruction set. The \nThumb instruction set contains a subset of the ARM 32-bit instruction set recoded \ninto 16-bit instructions. The current defined version is ­\nThumb-​\n­\n2.\nThe ARM and ­\nThumb-​\n­\n2 ISAs are discussed in Chapters 12 and 13.\nARM Products\nARM Holdings licenses a number of specialized microprocessors and related tech-\nnologies, but the bulk of their product line is the Cortex family of microprocessor \narchitectures. There are three Cortex architectures, conveniently labeled with the \ninitials A, R, and M.\n­\ncortex-​\n­\na/­\ncortex-​\n­\na50 The ­\nCortex-​\n­\nA and ­\nCortex-​\n­\nA50 are application \nprocessors, intended for mobile devices such as smartphones and eBook readers, \nas well as consumer devices such as digital TV and home gateways (e.g., DSL and \ncable Internet modems). These processors run at higher clock frequency (over \n \n1 GHz), and support a memory management unit (MMU), which is required for full \nfeature OSs such as Linux, Android, MS Windows, and mobile OSs. An MMU is \na hardware module that supports virtual memory and paging by translating virtual \naddresses into physical addresses; this topic is explored in Chapter 8.\nThe two architectures use both the ARM and ­\nThumb-​\n­\n2 instruction sets; the \nprincipal difference is that the ­\nCortex-​\n­\nA is a 32-bit machine, and the ­\nCortex-​\n­\nA50 is \na 64-bit machine.\n­\ncortex-​\n­\nr The ­\nCortex-​\n­\nR is designed to support ­\nreal-​\n­\ntime applications, in which \nthe timing of events needs to be controlled with rapid response to events. They can \nrun at a fairly high clock frequency (e.g., 200MHz to 800MHz) and have very low \nresponse latency. The ­\nCortex-​\n­\nR includes enhancements both to the instruction set \nand to the processor organization to support deeply embedded ­\nreal-​\n­\ntime devices. \nMost of these processors do not have MMU; the limited data requirements and \nthe limited number of simultaneous processes eliminates the need for elaborate \nhardware and software support for virtual memory. The ­\nCortex-​\n­\nR does have a \nMemory Protection Unit (MPU), cache, and other memory features designed for \nindustrial applications. An MPU is a hardware module that prohibits one program \nin memory from accidentally accessing memory assigned to another active program. \nUsing various methods, a protective boundary is created around the program, and \ninstructions within the program are prohibited from referencing data outside of that \nboundary.\nExamples of embedded systems that would use the ­\nCortex-​\n­\nR are automotive \nbraking systems, mass storage controllers, and networking and printing devices.\n­\ncortex-​\n­\nm ­\nCortex-​\n­\nM series processors have been developed primarily for the \nmicrocontroller domain where the need for fast, highly deterministic interrupt \nmanagement is coupled with the desire for extremely low gate count and \nlowest possible power consumption. As with the ­\nCortex-​\n­\nR series, the ­\nCortex-​\n­\nM \narchitecture has an MPU but no MMU. The ­\nCortex-​\n­\nM uses only the ­\nThumb-​\n­\n2 \ninstruction set. The market for the ­\nCortex-​\n­\nM includes IoT devices, wireless \nsensor/actuator networks used in factories and other enterprises, automotive \nbody electronics, and so on.\n36    Chapter 1 / Basic Concepts and Computer Evolution \nThere are currently four versions of the ­\nCortex-​\n­\nM series:\n■\n■­\nCortex-​\n­\nM0: Designed for 8- and 16-bit applications, this model emphasizes low \ncost, ultra low power, and simplicity. It is optimized for small silicon die size \n(starting from 12k gates) and use in the lowest cost chips.\n■\n■­\nCortex-​\n­\nM0+: An enhanced version of the M0 that is more energy efficient.\n■\n■­\nCortex-​\n­\nM3: Designed for 16- and 32-bit applications, this model emphasizes \nperformance and energy efficiency. It also has comprehensive debug and trace \nfeatures to enable software developers to develop their applications quickly.\n■\n■­\nCortex-​\n­\nM4: This model provides all the features of the ­\nCortex-​\n­\nM3, with addi-\ntional instructions to support digital signal processing tasks.\nIn this text, we will primarily use the ARM ­\nCortex-​\n­\nM3 as our example embed-\nded system processor. It is the best suited of all ARM models for ­\ngeneral-​\n­\npurpose \nmicrocontroller use. The ­\nCortex-​\n­\nM3 is used by a variety of manufacturers of micro-\ncontroller products. Initial microcontroller devices from lead partners already \ncombine the ­\nCortex-​\n­\nM3 processor with flash, SRAM, and multiple peripherals to \nprovide a competitive offering at the price of just $1.\nFigure 1.16 provides a block diagram of the EFM32 microcontroller from Sil-\nicon Labs. The figure also shows detail of the ­\nCortex-​\n­\nM3 processor and core com-\nponents. We examine each level in turn.\nThe ­\nCortex-​\n­\nM3 core makes use of separate buses for instructions and data. \nThis arrangement is sometimes referred to as a Harvard architecture, in contrast \nwith the von Neumann architecture, which uses the same signal buses and mem-\nory for both instructions and data. By being able to read both an instruction and \ndata from memory at the same time, the ­\nCortex-​\n­\nM3 processor can perform many \noperations in parallel, speeding application execution. The core contains a decoder \nfor Thumb instructions, an advanced ALU with support for hardware multiply and \ndivide, control logic, and interfaces to the other components of the processor. In \nparticular, there is an interface to the nested vector interrupt controller (NVIC) and \nthe embedded trace macrocell (ETM) module.\nThe core is part of a module called the ­\nCortex-​\n­\nM3 processor. This term is \nsomewhat misleading, because typically in the literature, the terms core and pro-\ncessor are viewed as equivalent. In addition to the core, the processor includes the \nfollowing elements:\n■\n■NVIC: Provides configurable interrupt handling abilities to the processor. It \nfacilitates ­\nlow-​\n­\nlatency exception and interrupt handling, and controls power \nmanagement.\n■\n■ETM: An optional debug component that enables reconstruction of program \nexecution. The ETM is designed to be a ­\nhigh-​\n­\nspeed, ­\nlow-​\n­\npower debug tool \nthat only supports instruction trace.\n■\n■Debug access port (DAP): This provides an interface for external debug \naccess to the processor.\n■\n■Debug logic: Basic debug functionality includes processor halt, ­\nsingle-​\n­\nstep, \nprocessor core register access, unlimited software breakpoints, and full system \nmemory access.\nCortex-M3 Core\nMicrocontroller Chip\nCortex-M3\nProcessor \nNVIC\ninterface\nETM\ninterface\nHardware\ndivider\n32-bit\nmultiplier\n32-bit ALU\nControl\nlogic\nThumb\ndecode\nInstruction\ninterface\nData\ninterface\nICode\ninterface\nDebug logic\nARM\ncore\nDAP\nNVIC\nETM\nMemory\nprotection unit\nBus matrix\nSRAM &\nperipheral I/F\nSecurity\nAnalog Interfaces\nTimers & Triggers\nParallel I/O Ports\nSerial Interfaces\nPeripheral bus\nCore and memory\nClock management\nEnergy management\nCortex-M3 processor\nMemory\nprotec-\ntion unit\nFlash\nmemory\n64 kB\nVoltage\nregula-\ntor\nPower-\non reset\nBrown-\nout de-\ntector\nVoltage\ncompar-\nator\nHigh fre-\nquency RC\noscillator\nLow fre-\nquency RC\noscillator\nHigh freq\ncrystal\noscillator\nLow freq\ncrystal\noscillator\nSRAM\nmemory\n64 kB\nDebug\ninter-\nface\nDMA\ncontrol-\nler\nPulse\ncounter\nWatch-\ndog tmr\nLow\nenergy\nReal\ntime ctr\nPeriph\nbus int\nTimer/\ncounter\nGeneral\npurpose\nI/O\nExternal\nInter-\nrupts\nUART\nUSART\nLow-\nenergy\nUART\nUSB\nPin\nreset\n32-bit bus\nA/D\ncon-\nverter\nHard-\nware\nAES\nD/A\ncon-\nverter\nFigure 1.16  Typical Microcontroller Chip Based on ­\nCortex-​\n­\nM3\n37\n38    Chapter 1 / Basic Concepts and Computer Evolution \n■\n■ICode interface: Fetches instructions from the code memory space.\n■\n■SRAM & peripheral interface: Read/write interface to data memory and \nperipheral devices.\n■\n■Bus matrix: Connects the core and debug interfaces to external buses on the \nmicrocontroller.\n■\n■Memory protection unit: Protects critical data used by the operating system \nfrom user applications, separating processing tasks by disallowing access \nto each other’s data, disabling access to memory regions, allowing memory \nregions to be defined as ­\nread-​\n­\nonly, and detecting unexpected memory accesses \nthat could potentially break the system.\nThe upper part of Figure 1.16 shows the block diagram of a typical micro-\ncontroller built with the ­\nCortex-​\n­\nM3, in this case the EFM32 microcontroller. This \nmicrocontroller is marketed for use in a wide variety of devices, including energy, \ngas, and water metering; alarm and security systems; industrial automation devices; \nhome automation devices; smart accessories; and health and fitness devices. The sil-\nicon chip consists of 10 main areas:13\n■\n■Core and memory: This region includes the ­\nCortex-​\n­\nM3 processor, static RAM \n(SRAM) data memory,14 and flash memory15 for storing program instructions \nand nonvarying application data. Flash memory is nonvolatile (data is not lost \nwhen power is shut off) and so is ideal for this purpose. The SRAM stores \nvariable data. This area also includes a debug interface, which makes it easy to \nreprogram and update the system in the field.\n■\n■Parallel I/O ports: Configurable for a variety of parallel I/O schemes.\n■\n■Serial interfaces: Supports various serial I/O schemes.\n■\n■Analog interfaces: ­\nAnalog-​\n­\nto-​\n­\ndigital and ­\ndigital-​\n­\nto-​\n­\nanalog logic to support \nsensors and actuators.\n■\n■Timers and triggers: Keeps track of timing and counts events, generates out-\nput waveforms, and triggers timed actions in other peripherals.\n■\n■Clock management: Controls the clocks and oscillators on the chip. Multiple \nclocks and oscillators are used to minimize power consumption and provide \nshort startup times.\n■\n■Energy management: Manages the various ­\nlow-​\n­\nenergy modes of operation of \nthe processor and peripherals to provide ­\nreal-​\n­\ntime management of the energy \nneeds so as to minimize energy consumption.\n■\n■Security: The chip includes a hardware implementation of the Advanced \nEncryption Standard (AES).\n13This discussion does not go into details about all of the individual modules; for the interested reader, an \n­\nin-​\n­\ndepth discussion is provided in the document EFM32G200.pdf, available at box.com/COA10e.\n14Static RAM (SRAM) is a form of ­\nrandom-​\n­\naccess memory used for cache memory; see Chapter 5.\n15Flash memory is a versatile form of memory used both in microcontrollers and as external memory; it \nis discussed in Chapter 6.\n1.7 / Cloud Computing   39\n■\n■32-bit bus: Connects all of the components on the chip.\n■\n■Peripheral bus: A network which lets the different peripheral module commu-\nnicate directly with each other without involving the processor. This supports \n­\ntiming-​\n­\ncritical operation and reduces software overhead.\nComparing Figure 1.16 with Figure 1.2, you will see many similarities and \nthe same general hierarchical structure. Note, however, that the top level of a \nmicrocontroller computer system is a single chip, whereas for a multicore com-\nputer, the top level is a motherboard containing a number of chips. Another note-\nworthy difference is that there is no cache, neither in the ­\nCortex-​\n­\nM3 processor \nnor in the microcontroller as a whole, which plays an important role if the code or \ndata resides in external memory. Though the number of cycles to read the instruc-\ntion or data varies depending on cache hit or miss, the cache greatly improves the \nperformance when external memory is used. Such overhead is not needed for a \nmicrocontroller.\n\t 1.7\t Cloud Computing\nAlthough the general concepts for cloud computing go back to the 1950s, cloud \ncomputing services first became available in the early 2000s, particularly targeted \nat large enterprises. Since then, cloud computing has spread to small and medium \nsize businesses, and most recently to consumers. Apple’s iCloud was launched in \n2012 and had 20 million users within a week of launch. Evernote, the ­\ncloud-​\n­\nbased \nnotetaking and archiving service, launched in 2008, approached 100 million users \nin less than 6 years. In this section, we provide a brief overview. Cloud computing is \nexamined in more detail in Chapter 17\n.\nBasic Concepts\nThere is an increasingly prominent trend in many organizations to move a substantial \nportion or even all information technology (IT) operations to an ­\nInternet-​\n­\nconnected \ninfrastructure known as enterprise cloud computing. At the same time, individual \nusers of PCs and mobile devices are relying more and more on cloud computing \nservices to backup data, synch devices, and share, using personal cloud computing. \nNIST defines cloud computing, in NIST ­\nSP-​\n­\n800-145 (The NIST Definition of Cloud \nComputing), as follows:\nCloud computing: A model for enabling ubiquitous, convenient, ­\non-​\n­\ndemand network \naccess to a shared pool of configurable computing resources (e.g., networks, servers, \nstorage, applications, and services) that can be rapidly provisioned and released with \nminimal management effort or service provider interaction.\nBasically, with cloud computing, you get economies of scale, professional \nnetwork management, and professional security management. These features can \nbe attractive to companies large and small, government agencies, and individual \nPC and mobile users. The individual or company only needs to pay for the storage \n40    Chapter 1 / Basic Concepts and Computer Evolution \ncapacity and services they need. The user, be it company or individual, doesn’t have \nthe hassle of setting up a database system, acquiring the hardware they need, doing \nmaintenance, and backing up the ­\ndata—​\n­\nall these are part of the cloud service.\nIn theory, another big advantage of using cloud computing to store your data \nand share it with others is that the cloud provider takes care of security. Alas, the \ncustomer is not always protected. There have been a number of security failures \namong cloud providers. Evernote made headlines in early 2013 when it told all of its \nusers to reset their passwords after an intrusion was discovered.\nCloud networking refers to the networks and network management function-\nality that must be in place to enable cloud computing. Most cloud computing solu-\ntions rely on the Internet, but that is only a piece of the networking infrastructure. \nOne example of cloud networking is the provisioning of ­\nhigh-​\n­\nperformance and/or \n­\nhigh-​\n­\nreliability networking between the provider and subscriber. In this case, some \nor all of the traffic between an enterprise and the cloud bypasses the Internet and \nuses dedicated private network facilities owned or leased by the cloud service pro-\nvider. More generally, cloud networking refers to the collection of network capa-\nbilities required to access a cloud, including making use of specialized services over \nthe Internet, linking enterprise data centers to a cloud, and using firewalls and other \nnetwork security devices at critical points to enforce access security policies.\nWe can think of cloud storage as a subset of cloud computing. In essence, cloud \nstorage consists of database storage and database applications hosted remotely on \ncloud servers. Cloud storage enables small businesses and individual users to take \nadvantage of data storage that scales with their needs and to take advantage of a \nvariety of database applications without having to buy, maintain, and manage the \nstorage assets.\nCloud Services\nThe essential purpose of cloud computing is to provide for the convenient rental \nof computing resources. A cloud service provider (CSP) maintains computing and \ndata storage resources that are available over the Internet or private networks. \nCustomers can rent a portion of these resources as needed. Virtually all cloud ser-\nvice is provided using one of three models (Figure 1.17): SaaS, PaaS, and IaaS, which \nwe examine in this section.\nsoftware as a service (SaaS) As the name implies, a SaaS cloud provides \nservice to customers in the form of software, specifically application software, \nrunning on and accessible in the cloud. SaaS follows the familiar model of Web \nservices, in this case applied to cloud resources. SaaS enables the customer to use \nthe cloud provider’s applications running on the provider’s cloud infrastructure. The \napplications are accessible from various client devices through a simple interface \nsuch as a Web browser. Instead of obtaining desktop and server licenses for \nsoftware products it uses, an enterprise obtains the same functions from the cloud \nservice. SaaS saves the complexity of software installation, maintenance, upgrades, \nand patches. Examples of services at this level are Gmail, Google’s ­\ne-​\n­\nmail service, \nand Salesforce.com, which help firms keep track of their customers.\nCommon subscribers to SaaS are organizations that want to provide their \nemployees with access to typical office productivity software, such as document \n1.7 / Cloud Computing   41\nmanagement and email. Individuals also commonly use the SaaS model to acquire \ncloud resources. Typically, subscribers use specific applications on demand. The \ncloud provider also usually offers ­\ndata-​\n­\nrelated features such as automatic backup \nand data sharing between subscribers.\nplatform as a service (PaaS) A PaaS cloud provides service to customers in \nthe form of a platform on which the customer’s applications can run. PaaS enables \nthe customer to deploy onto the cloud infrastructure containing ­\ncustomer-​\n­\ncreated \nor acquired applications. A PaaS cloud provides useful software building blocks, \nplus a number of development tools, such as programming languages, ­\nrun-​\n­\ntime \nenvironments, and other tools that assist in deploying new applications. In effect, \nPaaS is an operating system in the cloud. PaaS is useful for an organization that \nwants to develop new or tailored applications while paying for the needed computing \nresources only as needed and only for as long as needed. Google App Engine and \nthe Salesforce1 Platform from Salesforce.com are examples of PaaS.\nApplications\nInfrastructure as\na service (IaaS)\nTraditional IT\narchitecture\nPlatform as a\nservice (PaaS)\nSoftware as a\nservice (SaaS)\nManaged by client\nApplication\nFramework\nCompilers\nRun-time\nenvironment\nDatabases\nOperating\nsystem\nVirtual\nmachine\nServer\nhardware\nStorage\nNetworking\nApplications\nApplication\nFramework\nCompilers\nRun-time\nenvironment\nDatabases\nOperating\nsystem\nVirtual\nmachine\nServer\nhardware\nStorage\nNetworking\nMore complex\nMore upfront cost\nLess scalable\nMore customizable\nLess complex\nLower upfront cost\nMore scalable\nLess customizable\nIT = information technology\nCSP = cloud service provider\nManaged by CSP\nApplications\nManaged\nby client\nApplication\nFramework\nCompilers\nRun-time\nenvironment\nDatabases\nOperating\nsystem\nVirtual\nmachine\nServer\nhardware\nStorage\nNetworking\nManaged by CSP\nApplications\nApplication\nFramework\nCompilers\nRun-time\nenvironment\nDatabases\nOperating\nsystem\nVirtual\nmachine\nServer\nhardware\nStorage\nNetworking\nManaged by CSP\nFigure 1.17  Alternative Information Technology Architectures\n42    Chapter 1 / Basic Concepts and Computer Evolution \ninfrastructure as a service (IaaS) With IaaS, the customer has access to the \nunderlying cloud infrastructure. IaaS provides virtual machines and other abstracted \nhardware and operating systems, which may be controlled through a service \napplication programming interface (API). IaaS offers the customer processing, \nstorage, networks, and other fundamental computing resources so that the customer \nis able to deploy and run arbitrary software, which can include operating systems \nand applications. IaaS enables customers to combine basic computing services, \nsuch as number crunching and data storage, to build highly adaptable computer \nsystems. Examples of IaaS are Amazon Elastic Compute Cloud (Amazon EC2) and \nWindows Azure.\n\t 1.8\t Key Terms, Review Questions, and Problems\nKey Terms\napplication processor\narithmetic and logic unit \n(ALU)\nARM\ncentral processing unit  \n(CPU)\nchip\ncloud computing\ncloud networking\ncloud storage\ncomputer architecture\ncomputer organization\ncontrol unit\ncore\ndedicated processor\ndeeply embedded system\nembedded system\ngate\ninfrastructure as a service \n(IaaS)\n­\ninput–​\n­\noutput (I/O)\ninstruction set architecture \n(ISA)\nintegrated circuit\nIntel x86\nInternet of things (IoT)\nmain memory\nmemory cell\nmemory management unit \n(MMU)\nmemory protection unit  \n(MPU)\nmicrocontroller\nmicroelectronics\nmicroprocessor\nmotherboard\nmulticore\nmulticore processor\noriginal equipment  \nmanufacturer (OEM)\nplatform as a service  \n(PaaS)\nprinted circuit board\nprocessor\nregisters\nsemiconductor\nsemiconductor memory\nsoftware as a service (SaaS)\nsystem bus\nsystem interconnection\nvacuum tubes\nReview Questions\n\t 1.1\t\nWhat, in general terms, is the distinction between computer organization and com-\nputer architecture?\n\t 1.2\t\nWhat, in general terms, is the distinction between computer structure and computer \nfunction?\n\t 1.3\t\nWhat are the four main functions of a computer?\n\t 1.4\t\nList and briefly define the main structural components of a computer.\n\t 1.5\t\nList and briefly define the main structural components of a processor.\n\t 1.6\t\nWhat is a stored program computer?\n\t 1.7\t\nExplain Moore’s law.\n\t 1.8\t\nList and explain the key characteristics of a computer family.\n\t 1.9\t\nWhat is the key distinguishing feature of a microprocessor?\n1.8 / Key Terms, Review Questions, and Problems   43\nProblems\n\t 1.1\t\nYou are to write an IAS program to compute the results of the following equation.\nY = a\nN\nX=1\nX\nAssume that the computation does not result in an arithmetic overflow and that X, Y, \nand N are positive integers with N ≥ 1. Note: The IAS did not have assembly language, \nonly machine language.\na.\t Use the equation Sum(Y) =\nN(N + 1)\n2\n when writing the IAS program.\nb.\t Do it the “hard way,” without using the equation from part (a).\n\t\n1.2\t\na.\t \u0007\nOn the IAS, what would the machine code instruction look like to load the con-\ntents of memory address 2 to the accumulator?\n\t\n\t\nb.\t \u0007\nHow many trips to memory does the CPU need to make to complete this instruc-\ntion during the instruction cycle?\n\t 1.3\t\nOn the IAS, describe in English the process that the CPU must undertake to read a \nvalue from memory and to write a value to memory in terms of what is put into the \nMAR, MBR, address bus, data bus, and control bus.\n\t 1.4\t\nGiven the memory contents of the IAS computer shown below,\nAddress\nContents\n08A\n010FA210FB\n08B\n010FA0F08D\n08C\n020FA210FB\nshow the assembly language code for the program, starting at address 08A. Explain \nwhat this program does.\n\t 1.5\t\nIn Figure 1.6, indicate the width, in bits, of each data path (e.g., between AC and ALU).\n\t 1.6\t\nIn the IBM 360 Models 65 and 75, addresses are staggered in two separate main mem-\nory units (e.g., all ­\neven-​\n­\nnumbered words in one unit and all ­\nodd-​\n­\nnumbered words in \nanother). What might be the purpose of this technique?\n\t 1.7\t\nThe relative performance of the IBM 360 Model 75 is 50 times that of the 360 Model \n30, yet the instruction cycle time is only 5 times as fast. How do you account for this \ndiscrepancy?\n\t 1.8\t\nWhile browsing at Billy Bob’s computer store, you overhear a customer asking Billy \nBob what is the fastest computer in the store that he can buy. Billy Bob replies, “You’re \nlooking at our Macintoshes. The fastest Mac we have runs at a clock speed of 1.2 GHz. \nIf you really want the fastest machine, you should buy our 2.4-GHz Intel Pentium IV \ninstead.” Is Billy Bob correct? What would you say to help this customer?\n\t 1.9\t\nThe ENIAC, a precursor to the ISA machine, was a decimal machine, in which each \nregister was represented by a ring of 10 vacuum tubes. At any time, only one vacuum \ntube was in the ON state, representing one of the 10 decimal digits. Assuming that \nENIAC had the capability to have multiple vacuum tubes in the ON and OFF state \nsimultaneously, why is this representation “wasteful” and what range of integer values \ncould we represent using the 10 vacuum tubes?\n\t 1.10\t\nFor each of the following examples, determine whether this is an embedded system, \nexplaining why or why not.\na.\t Are programs that understand physics and/or hardware embedded? For example, \none that uses ­\nfinite-​\n­\nelement methods to predict fluid flow over airplane wings?\nb.\t Is the internal microprocessor controlling a disk drive an example of an embedded \nsystem?\n44    Chapter 1 / Basic Concepts and Computer Evolution \nc.\t I/O drivers control hardware, so does the presence of an I/O driver imply that the \ncomputer executing the driver is embedded?\nd.\t Is a PDA (Personal Digital Assistant) an embedded system?\ne.\t Is the microprocessor controlling a cell phone an embedded system?\nf.\t Are the computers in a big ­\nphased-​\n­\narray radar considered embedded? These \nradars are 10-story buildings with one to three 100-foot diameter radiating patches \non the sloped sides of the building.\ng.\t Is a traditional flight management system (FMS) built into an airplane cockpit \nconsidered embedded?\nh.\t Are the computers in a ­\nhardware-​\n­\nin-​\n­\nthe-​\n­\nloop (HIL) simulator embedded?\ni.\t\nIs the computer controlling a pacemaker in a person’s chest an embedded \ncomputer?\nj.\t Is the computer controlling fuel injection in an automobile engine embedded?\n45\nChapter\nPerformance Issues\n2.1\t\nDesigning for Performance\t\nMicroprocessor Speed\nPerformance Balance\nImprovements in Chip Organization and Architecture\n2.2\t\nMulticore, MICs, and GPGPUs\t\n2.3\t\nTwo Laws that Provide Insight: Amdahl’s Law and Little’s Law\t\nAmdahl’s Law\nLittle’s Law\n2.4\t\nBasic Measures of Computer Performance\t\nClock Speed\nInstruction Execution Rate\n2.5\t\nCalculating the Mean\t\nArithmetic Mean\nHarmonic Mean\nGeometric Mean\n2.6\t\nBenchmarks and SPEC\t\nBenchmark Principles\nSPEC Benchmarks\n2.7\t\nKey Terms, Review Questions, and Problems\t"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-30T12:20:18.912218611Z",
     "start_time": "2024-04-30T12:20:18.854408527Z"
    }
   },
   "id": "3a578575e65670d",
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "742c5020bfaaad54"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

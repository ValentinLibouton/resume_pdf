{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-28T16:28:26.098633934Z",
     "start_time": "2024-04-28T16:28:25.970663806Z"
    }
   },
   "outputs": [],
   "source": [
    "import fitz"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "doc = fitz.open(\"book.pdf\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-28T16:28:26.342111374Z",
     "start_time": "2024-04-28T16:28:26.251923003Z"
    }
   },
   "id": "9459a8b10c472851",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "chapter_starts_d = {\"Chapter 1\": 26,\n",
    "                  \"Chapter 2\": 70,\n",
    "                  \"Chapter 3\": 105,\n",
    "                  \"Chapter 4\": 145,\n",
    "                  \"Chapter 5\": 190,\n",
    "                  \"Chapter 6\": 219,\n",
    "                  \"Chapter 7\": 253,\n",
    "                  \"Chapter 8\": 300,\n",
    "                  \"Chapter 9\": 343,\n",
    "                  \"Chapter 10\": 353,\n",
    "                  \"Chapter 11\": 397,\n",
    "                  \"Chapter 12\": 437,\n",
    "                  \"Chapter 13\": 481,\n",
    "                  \"Chapter 14\": 513,\n",
    "                  \"Chapter 15\": 560,\n",
    "                  \"Chapter 16\": 600,\n",
    "                  \"Chapter 17\": 638,\n",
    "                  \"Chapter 18\": 681,\n",
    "                  \"Chapter 19\": 713,\n",
    "                  \"Chapter 20\": 732,\n",
    "                  \"Chapter 21\": 748,\n",
    "                  \"Out of chapter 21\" : 793}\n",
    "chapter_starts = list(chapter_starts_d.values())\n",
    "def extract_chapters_from_pdf(pdf_path, chapter_starts):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    chapter_dict = {}\n",
    "    num_chapters = len(chapter_starts) - 1  # On suppose que la dernière entrée est la fin du dernier chapitre\n",
    "\n",
    "    # Extraire le texte pour chaque chapitre\n",
    "    for i in range(num_chapters):\n",
    "        start_page = chapter_starts[i]\n",
    "        end_page = chapter_starts[i + 1] - 1  # La page de fin est la page de début du prochain chapitre moins 1\n",
    "        text = \"\"\n",
    "\n",
    "        # Concaténer le texte de chaque page du chapitre\n",
    "        for page_num in range(start_page, end_page + 1):  # +1 pour inclure la page de fin\n",
    "            page = doc.load_page(page_num)\n",
    "            text += page.get_text()\n",
    "\n",
    "        chapter_dict[i + 1] = text  # Utiliser un nom générique avec numéro de chapitre\n",
    "\n",
    "    return chapter_dict\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-28T16:28:27.092571152Z",
     "start_time": "2024-04-28T16:28:27.078455276Z"
    }
   },
   "id": "6062c81e9d55518c",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "chapter_dict = extract_chapters_from_pdf(pdf_path=\"book.pdf\", chapter_starts=chapter_starts)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-28T16:28:30.758145064Z",
     "start_time": "2024-04-28T16:28:28.775520543Z"
    }
   },
   "id": "b627c456383107bb",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "'2\\u2002 \\u2002 Chapter 1 / Basic Concepts and Computer Evolution \\n\\t 1.1\\t Organization and Architecture\\nIn describing computers, a distinction is often made between computer architec-\\nture and computer organization. Although it is difficult to give precise definitions \\nfor these terms, a consensus exists about the general areas covered by each. For \\nexample, see [VRAN80], [SIEW82], and [BELL78a]; an interesting alternative view \\nis presented in [REDD76].\\nComputer architecture refers to those attributes of a system visible to a pro-\\ngrammer or, put another way, those attributes that have a direct impact on the \\nlogical execution of a program. A term that is often used interchangeably with com-\\nputer architecture is instruction set architecture (ISA). The ISA defines instruction \\nformats, instruction opcodes, registers, instruction and data memory; the effect of \\nexecuted instructions on the registers and memory; and an algorithm for control-\\nling instruction execution. Computer organization refers to the operational units \\nand their interconnections that realize the architectural specifications. Examples of \\narchitectural attributes include the instruction set, the number of bits used to repre-\\nsent various data types (e.g., numbers, characters), I/O mechanisms, and techniques \\nfor addressing memory. Organizational attributes include those hardware details \\ntransparent to the programmer, such as control signals; interfaces between the com-\\nputer and peripherals; and the memory technology used.\\nFor example, it is an architectural design issue whether a computer will have \\na multiply instruction. It is an organizational issue whether that instruction will be \\nimplemented by a special multiply unit or by a mechanism that makes repeated \\nuse of the add unit of the system. The organizational decision may be based on the \\nanticipated frequency of use of the multiply instruction, the relative speed of the \\ntwo approaches, and the cost and physical size of a special multiply unit.\\nHistorically, and still today, the distinction between architecture and organ-\\nization has been an important one. Many computer manufacturers offer a family of \\ncomputer models, all with the same architecture but with differences in organization. \\nConsequently, the different models in the family have different price and perform-\\nance characteristics. Furthermore, a particular architecture may span many years \\nand encompass a number of different computer models, its organization changing \\nwith changing technology. A prominent example of both these phenomena is the \\nIBM System/370 architecture. This architecture was first introduced in 1970 and \\nLearning Objectives\\nAfter studying this chapter, you should be able to:\\nr\\nr Explain the general functions and structure of a digital computer.\\nr\\nr Present an overview of the evolution of computer technology from early \\ndigital computers to the latest microprocessors.\\nr\\nr Present an overview of the evolution of the x86 architecture.\\nr\\nr Define embedded systems and list some of the requirements and constraints \\nthat various embedded systems must meet.\\n1.2 / Structure and Function\\u2002 \\u20023\\nincluded a number of models. The customer with modest requirements could buy a \\ncheaper, slower model and, if demand increased, later upgrade to a more expensive, \\nfaster model without having to abandon software that had already been developed. \\nOver the years, IBM has introduced many new models with improved technology \\nto replace older models, offering the customer greater speed, lower cost, or both. \\nThese newer models retained the same architecture so that the customer’s soft-\\nware investment was protected. Remarkably, the System/370 architecture, with a \\nfew enhancements, has survived to this day as the architecture of IBM’s mainframe \\nproduct line.\\nIn a class of computers called microcomputers, the relationship between archi-\\ntecture and organization is very close. Changes in technology not only influence \\norganization but also result in the introduction of more powerful and more complex \\narchitectures. Generally, there is less of a requirement for \\xad\\ngeneration-\\u200b\\n\\xad\\nto-\\u200b\\n\\xad\\ngeneration \\ncompatibility for these smaller machines. Thus, there is more interplay between \\norganizational and architectural design decisions. An intriguing example of this is \\nthe reduced instruction set computer (RISC), which we examine in Chapter\\xa015.\\nThis book examines both computer organization and computer architecture. \\nThe emphasis is perhaps more on the side of organization. However, because a \\ncomputer organization must be designed to implement a particular architectural \\nspecification, a thorough treatment of organization requires a detailed examination \\nof architecture as well.\\n\\t 1.2\\t Structure and Function\\nA computer is a complex system; contemporary computers contain millions of \\nelementary electronic components. How, then, can one clearly describe them? The \\nkey is to recognize the hierarchical nature of most complex systems, including the \\ncomputer [SIMO96]. A hierarchical system is a set of interrelated subsystems, each \\nof the latter, in turn, hierarchical in structure until we reach some lowest level of \\nelementary subsystem.\\nThe hierarchical nature of complex systems is essential to both their design \\nand their description. The designer need only deal with a particular level of the \\nsystem at a time. At each level, the system consists of a set of components and \\ntheir interrelationships. The behavior at each level depends only on a simplified, \\nabstracted characterization of the system at the next lower level. At each level, the \\ndesigner is concerned with structure and function:\\n■\\n■Structure: The way in which the components are interrelated.\\n■\\n■Function: The operation of each individual component as part of the structure.\\nIn terms of description, we have two choices: starting at the bottom and build-\\ning up to a complete description, or beginning with a top view and decomposing the \\nsystem into its subparts. Evidence from a number of fields suggests that the \\xad\\ntop-\\u200b\\n\\xad\\ndown approach is the clearest and most effective [WEIN75].\\nThe approach taken in this book follows from this viewpoint. The computer \\nsystem will be described from the top down. We begin with the major components \\nof a computer, describing their structure and function, and proceed to successively \\n4\\u2002 \\u2002 Chapter 1 / Basic Concepts and Computer Evolution \\nlower layers of the hierarchy. The remainder of this section provides a very brief \\noverview of this plan of attack.\\nFunction\\nBoth the structure and functioning of a computer are, in essence, simple. In general \\nterms, there are only four basic functions that a computer can perform:\\n■\\n■Data processing: Data may take a wide variety of forms, and the range of pro-\\ncessing requirements is broad. However, we shall see that there are only a few \\nfundamental methods or types of data processing.\\n■\\n■Data storage: Even if the computer is processing data on the fly (i.e., data \\ncome in and get processed, and the results go out immediately), the computer \\nmust temporarily store at least those pieces of data that are being worked on \\nat any given moment. Thus, there is at least a \\xad\\nshort-\\u200b\\n\\xad\\nterm data storage function. \\nEqually important, the computer performs a \\xad\\nlong-\\u200b\\n\\xad\\nterm data storage function. \\nFiles of data are stored on the computer for subsequent retrieval and update.\\n■\\n■Data movement: The computer’s operating environment consists of devices \\nthat serve as either sources or destinations of data. When data are received \\nfrom or delivered to a device that is directly connected to the computer, the \\nprocess is known as \\xad\\ninput–\\u200b\\n\\xad\\noutput (I/O), and the device is referred to as a \\nperipheral. When data are moved over longer distances, to or from a remote \\ndevice, the process is known as data communications.\\n■\\n■Control: Within the computer, a control unit manages the computer’s \\nresources and orchestrates the performance of its functional parts in response \\nto instructions.\\nThe preceding discussion may seem absurdly generalized. It is certainly \\npossible, even at a top level of computer structure, to differentiate a variety of func-\\ntions, but to quote [SIEW82]:\\nThere is remarkably little shaping of computer structure to fit the \\nfunction to be performed. At the root of this lies the \\xad\\ngeneral-\\u200b\\n\\xad\\npurpose \\nnature of computers, in which all the functional specialization occurs \\nat the time of programming and not at the time of design.\\nStructure\\nWe now look in a general way at the internal structure of a computer. We begin with \\na traditional computer with a single processor that employs a microprogrammed \\ncontrol unit, then examine a typical multicore structure.\\nsimple \\xad\\nsingle-\\u200b\\n\\xad\\nprocessor computer Figure\\xa01.1 provides a hierarchical view \\nof the internal structure of a traditional \\xad\\nsingle-\\u200b\\n\\xad\\nprocessor computer. There are four \\nmain structural components:\\n■\\n■Central processing unit (CPU): Controls the operation of the computer and \\nperforms its data processing functions; often simply referred to as processor.\\n■\\n■Main memory: Stores data.\\n1.2 / Structure and Function\\u2002 \\u20025\\n■\\n■I/O: Moves data between the computer and its external environment.\\n■\\n■System interconnection: Some mechanism that provides for communication \\namong CPU, main memory, and I/O.\\xa0A common example of system intercon-\\nnection is by means of a system bus, consisting of a number of conducting \\nwires to which all the other components attach.\\nThere may be one or more of each of the aforementioned components. Tra-\\nditionally, there has been just a single processor. In recent years, there has been \\nincreasing use of multiple processors in a single computer. Some design issues relat-\\ning to multiple processors crop up and are discussed as the text proceeds; Part Five \\nfocuses on such computers.\\nMain\\nmemory\\nI/O\\nCPU\\nCOMPUTER\\nSystem\\nbus\\nALU\\nRegisters\\nControl\\nunit\\nCPU\\nInternal\\nbus\\nControl unit\\nregisters and\\ndecoders\\nCONTROL\\nUNIT\\nSequencing\\nlogic\\nControl\\nmemory\\nFigure\\xa01.1\\u2003 The Computer: \\xad\\nTop-\\u200b\\n\\xad\\nLevel Structure\\n6\\u2002 \\u2002 Chapter 1 / Basic Concepts and Computer Evolution \\nEach of these components will be examined in some detail in Part Two. How-\\never, for our purposes, the most interesting and in some ways the most complex \\ncomponent is the\\xa0CPU.\\xa0Its major structural components are as follows:\\n■\\n■Control unit: Controls the operation of the CPU and hence the computer.\\n■\\n■Arithmetic and logic unit (ALU): Performs the computer’s data processing \\nfunctions.\\n■\\n■Registers: Provides storage internal to the CPU.\\n■\\n■CPU interconnection: Some mechanism that provides for communication \\namong the control unit, ALU, and registers.\\nPart Three covers these components, where we will see that complexity is added by \\nthe use of parallel and pipelined organizational techniques. Finally, there are sev-\\neral approaches to the implementation of the control unit; one common approach is \\na microprogrammed implementation. In essence, a microprogrammed control unit \\noperates by executing microinstructions that define the functionality of the control \\nunit. With this approach, the structure of the control unit can be depicted, as in \\n\\xad\\nFigure\\xa01.1. This structure is examined in Part Four.\\nmulticore computer structure As was mentioned, contemporary \\ncomputers generally have multiple processors. When these processors all reside \\non a single chip, the term multicore computer is used, and each processing unit \\n(consisting of a control unit, ALU, registers, and perhaps cache) is called a core. To \\nclarify the terminology, this text will use the following definitions.\\n■\\n■Central processing unit (CPU): That portion of a computer that fetches and \\nexecutes instructions. It consists of an ALU, a control unit, and registers. \\nIn a system with a single processing unit, it is often simply referred to as a \\nprocessor.\\n■\\n■Core: An individual processing unit on a processor chip. A core may be equiv-\\nalent in functionality to a CPU on a \\xad\\nsingle-\\u200b\\n\\xad\\nCPU system. Other specialized pro-\\ncessing units, such as one optimized for vector and matrix operations, are also \\nreferred to as cores.\\n■\\n■Processor: A physical piece of silicon containing one or more cores. The \\nprocessor is the computer component that interprets and executes instruc-\\ntions. If a processor contains multiple cores, it is referred to as a multicore \\nprocessor.\\nAfter about a decade of discussion, there is broad industry consensus on this usage.\\nAnother prominent feature of contemporary computers is the use of multiple \\nlayers of memory, called cache memory, between the processor and main memory. \\nChapter\\xa04 is devoted to the topic of cache memory. For our purposes in this section, \\nwe simply note that a cache memory is smaller and faster than main memory and is \\nused to speed up memory access, by placing in the cache data from main memory, \\nthat is likely to be used in the near future. A greater performance improvement may \\nbe obtained by using multiple levels of cache, with level 1 (L1) closest to the core \\nand additional levels (L2, L3, and so on) progressively farther from the core. In this \\nscheme, level n is smaller and faster than level n + 1.\\n1.2 / Structure and Function\\u2002 \\u20027\\nFigure\\xa01.2 is a simplified view of the principal components of a typical mul-\\nticore computer. Most computers, including embedded computers in smartphones \\nand tablets, plus personal computers, laptops, and workstations, are housed on a \\nmotherboard. Before describing this arrangement, we need to define some terms. \\nA printed circuit board (PCB) is a rigid, flat board that holds and interconnects \\nchips and other electronic components. The board is made of layers, typically two \\nto ten, that interconnect components via copper pathways that are etched into \\nthe board. The main printed circuit board in a computer is called a system board \\nor motherboard, while smaller ones that plug into the slots in the main board are \\ncalled expansion boards.\\nThe most prominent elements on the motherboard are the chips. A chip is \\na single piece of semiconducting material, typically silicon, upon which electronic \\ncircuits and logic gates are fabricated. The resulting product is referred to as an \\nintegrated circuit.\\nMOTHERBOARD\\nPROCESSOR CHIP\\nCORE\\nProcessor\\nchip\\nMain memory chips\\nI/O chips\\nCore\\nL3 cache\\nInstruction\\nlogic\\nL1 I-cache\\nL2 instruction\\ncache\\nL2 data\\ncache\\nL1 data cache\\nArithmetic\\nand logic\\nunit (ALU)\\nLoad/\\nstore logic\\nL3 cache\\nCore\\nCore\\nCore\\nCore\\nCore\\nCore\\nCore\\nFigure\\xa01.2\\u2003 Simplified View of Major Elements of a Multicore Computer\\n8\\u2002 \\u2002 Chapter 1 / Basic Concepts and Computer Evolution \\nThe motherboard contains a slot or socket for the processor chip, which typ-\\nically contains multiple individual cores, in what is known as a multicore processor. \\nThere are also slots for memory chips, I/O controller chips, and other key computer \\ncomponents. For desktop computers, expansion slots enable the inclusion of more \\ncomponents on expansion boards. Thus, a modern motherboard connects only a \\nfew individual chip components, with each chip containing from a few thousand up \\nto hundreds of millions of transistors.\\nFigure\\xa01.2 shows a processor chip that contains eight cores and an L3 cache. \\nNot shown is the logic required to control operations between the cores and the \\ncache and between the cores and the external circuitry on the motherboard. The \\nfigure indicates that the L3 cache occupies two distinct portions of the chip surface. \\nHowever, typically, all cores have access to the entire L3 cache via the aforemen-\\ntioned control circuits. The processor chip shown in Figure\\xa01.2 does not represent \\nany specific product, but provides a general idea of how such chips are laid out.\\nNext, we zoom in on the structure of a single core, which occupies a portion of \\nthe processor chip. In general terms, the functional elements of a core are:\\n■\\n■Instruction logic: This includes the tasks involved in fetching instructions, \\nand decoding each instruction to determine the instruction operation and the \\nmemory locations of any operands.\\n■\\n■Arithmetic and logic unit (ALU): Performs the operation specified by an \\ninstruction.\\n■\\n■Load/store logic: Manages the transfer of data to and from main memory via \\ncache.\\nThe core also contains an L1 cache, split between an instruction cache \\n \\n(\\xad\\nI-\\u200b\\n\\xad\\ncache) that is used for the transfer of instructions to and from main memory, and \\nan L1 data cache, for the transfer of operands and results. Typically, today’s pro-\\ncessor chips also include an L2 cache as part of the core. In many cases, this cache \\nis also split between instruction and data caches, although a combined, single L2 \\ncache is also used.\\nKeep in mind that this representation of the layout of the core is only intended \\nto give a general idea of internal core structure. In a given product, the functional \\nelements may not be laid out as the three distinct elements shown in Figure\\xa01.2, \\nespecially if some or all of these functions are implemented as part of a micropro-\\ngrammed control unit.\\nexamples It will be instructive to look at some \\xad\\nreal-\\u200b\\n\\xad\\nworld examples that \\nillustrate the hierarchical structure of computers. Figure\\xa01.3 is a photograph of the \\nmotherboard for a computer built around two Intel \\xad\\nQuad-\\u200b\\n\\xad\\nCore Xeon processor \\nchips. Many of the elements labeled on the photograph are discussed subsequently \\nin this book. Here, we mention the most important, in addition to the processor \\nsockets:\\n■\\n■\\xad\\nPCI-\\u200b\\n\\xad\\nExpress slots for a \\xad\\nhigh-\\u200b\\n\\xad\\nend display adapter and for additional peripher-\\nals (Section\\xa03.6 describes PCIe).\\n■\\n■Ethernet controller and Ethernet ports for network connections.\\n■\\n■USB sockets for peripheral devices.\\n1.2 / Structure and Function\\u2002 \\u20029\\n2x Quad-Core Intel® Xeon® Processors\\nwith Integrated Memory Controllers\\nSix Channel DDR3-1333 Memory\\nInterfaces Up to 48GB\\nIntel® 3420\\nChipset\\nSerial ATA/300 (SATA)\\nInterfaces\\n2x USB 2.0\\nInternal\\n2x Ethernet Ports\\n10/100/1000Base-T\\nEthernet Controller\\nClock\\nPCI Express®\\nConnector A\\nPCI Express®\\nConnector B\\nPower & Backplane I/O\\nConnector C\\nVGA Video Output\\nBIOS\\n2x USB 2.0\\nExternal\\nFigure\\xa01.3\\u2003 Motherboard with Two Intel \\xad\\nQuad-\\u200b\\n\\xad\\nCore Xeon Processors\\nSource: Chassis Plans, www.chassis-plans.com\\n■\\n■Serial ATA (SATA) sockets for connection to disk memory (Section\\xa07.7 \\n\\xad\\ndiscusses Ethernet, USB, and SATA).\\n■\\n■Interfaces for DDR (double data rate) main memory chips (Section\\xa0 5.3 \\n\\xad\\ndiscusses DDR).\\n■\\n■Intel 3420 chipset is an I/O controller for direct memory access operations \\nbetween peripheral devices and main memory (Section\\xa07.5 discusses DDR).\\nFollowing our \\xad\\ntop-\\u200b\\n\\xad\\ndown strategy, as illustrated in Figures 1.1 and 1.2, we can \\nnow zoom in and look at the internal structure of a processor chip. For variety, we \\nlook at an IBM chip instead of the Intel processor chip. Figure\\xa01.4 is a photograph \\nof the processor chip for the IBM zEnterprise EC12 mainframe computer. This chip \\nhas 2.75\\xa0billion transistors. The superimposed labels indicate how the silicon real \\nestate of the chip is allocated. We see that this chip has six cores, or processors. \\nIn addition, there are two large areas labeled L3 cache, which are shared by all six \\nprocessors. The L3 control logic controls traffic between the L3 cache and the cores \\nand between the L3 cache and the external environment. Additionally, there is stor-\\nage control (SC) logic between the cores and the L3 cache. The memory controller \\n(MC) function controls access to memory external to the chip. The GX I/O bus \\ncontrols the interface to the channel adapters \\xad\\naccessing the I/O.\\nGoing down one level deeper, we examine the internal structure of a single \\ncore, as shown in the photograph of Figure\\xa01.5. Keep in mind that this is a portion \\nof the silicon surface area making up a \\xad\\nsingle-\\u200b\\n\\xad\\nprocessor chip. The main \\xad\\nsub-\\u200b\\n\\xad\\nareas \\nwithin this core area are the following:\\n■\\n■ISU (instruction sequence unit): Determines the sequence in which instructions \\nare executed in what is referred to as a superscalar architecture (Chapter\\xa016).\\n■\\n■IFU (instruction fetch unit): Logic for fetching instructions.\\n10\\u2002 \\u2002 Chapter 1 / Basic Concepts and Computer Evolution \\n■\\n■IDU (instruction decode unit): The IDU is fed from the IFU buffers, and is \\nresponsible for the parsing and decoding of all z/Architecture operation codes.\\n■\\n■LSU (\\xad\\nload-\\u200b\\n\\xad\\nstore unit): The LSU contains the 96-kB L1 data cache,1 and man-\\nages data traffic between the L2 data cache and the functional execution \\nunits. It is responsible for handling all types of operand accesses of all lengths, \\nmodes, and formats as defined in the z/Architecture.\\n■\\n■XU (translation unit):  This unit translates logical addresses from instructions \\ninto physical addresses in main memory. The XU also contains a translation \\nlookaside buffer (TLB) used to speed up memory access. TLBs are discussed \\nin Chapter\\xa08.\\n■\\n■FXU (\\xad\\nfixed-\\u200b\\n\\xad\\npoint unit): The FXU executes \\xad\\nfixed-\\u200b\\n\\xad\\npoint arithmetic operations.\\n■\\n■BFU (binary \\xad\\nfloating-\\u200b\\n\\xad\\npoint unit): The BFU handles all binary and hexadeci-\\nmal \\xad\\nfloating-\\u200b\\n\\xad\\npoint operations, as well as \\xad\\nfixed-\\u200b\\n\\xad\\npoint multiplication operations.\\n■\\n■DFU (decimal \\xad\\nfloating-\\u200b\\n\\xad\\npoint unit): The DFU handles both \\xad\\nfixed-\\u200b\\n\\xad\\npoint and \\n\\xad\\nfloating-\\u200b\\n\\xad\\npoint operations on numbers that are stored as decimal digits.\\n■\\n■RU (recovery unit): The RU keeps a copy of the complete state of the sys-\\ntem that includes all registers, collects hardware fault signals, and manages the \\nhardware recovery actions.\\nFigure\\xa01.4\\u2003 zEnterprise EC12 Processor Unit \\n(PU) chip diagram\\nSource: IBM zEnterprise EC12 Technical Guide, \\nDecember\\xa02013, SG24-8049-01. IBM, Reprinted by \\nPermission\\nFigure\\xa01.5\\u2003 zEnterprise EC12 Core layout\\nSource: IBM zEnterprise EC12 Technical Guide, \\nDecember\\xa02013, SG24-8049-01. IBM, Reprinted by \\nPermission\\n1kB = kilobyte = 2048 bytes. Numerical prefixes are explained in a document under the “Other Useful” \\ntab at ComputerScienceStudent.com.\\n1.3 / A Brief History of Computers\\u2002 \\u200211\\n■\\n■COP (dedicated \\xad\\nco-\\u200b\\n\\xad\\nprocessor): The COP is responsible for data compression \\nand encryption functions for each core.\\n■\\n■\\xad\\nI-\\u200b\\n\\xad\\ncache: This is a 64-kB L1 instruction cache, allowing the IFU to prefetch \\ninstructions before they are needed.\\n■\\n■L2 control: This is the control logic that manages the traffic through the two \\nL2 caches.\\n■\\n■\\xad\\nData-\\u200b\\n\\xad\\nL2: A 1-MB L2 data cache for all memory traffic other than instructions.\\n■\\n■\\xad\\nInstr-\\u200b\\n\\xad\\nL2: A 1-MB L2 instruction cache.\\nAs we progress through the book, the concepts introduced in this section will \\nbecome clearer.\\n\\t 1.3\\t A Brief History of Computers2\\nIn this section, we provide a brief overview of the history of the development of \\ncomputers. This history is interesting in itself, but more importantly, provides a basic \\nintroduction to many important concepts that we deal with throughout the book.\\nThe First Generation: Vacuum Tubes\\nThe first generation of computers used vacuum tubes for digital logic elements and \\nmemory. A number of research and then commercial computers were built using \\nvacuum tubes. For our purposes, it will be instructive to examine perhaps the most \\nfamous \\xad\\nfirst-\\u200b\\n\\xad\\ngeneration computer, known as the IAS computer.\\nA fundamental design approach first implemented in the IAS computer is \\nknown as the \\xad\\nstored-\\u200b\\n\\xad\\nprogram concept. This idea is usually attributed to the mathem-\\natician John von Neumann. Alan Turing developed the idea at about the same time. \\nThe first publication of the idea was in a 1945 proposal by von Neumann for a new \\ncomputer, the EDVAC (Electronic Discrete Variable Computer).3\\nIn 1946, von Neumann and his colleagues began the design of a new \\xad\\nstored-\\u200b\\n\\xad\\nprogram computer, referred to as the IAS computer, at the Princeton Institute for \\nAdvanced Studies. The IAS computer, although not completed until 1952, is the \\nprototype of all subsequent \\xad\\ngeneral-\\u200b\\n\\xad\\npurpose computers.4\\nFigure\\xa01.6 shows the structure of the IAS computer (compare with Figure\\xa01.1). \\nIt consists of\\n■\\n■A main memory, which stores both data and instructions5\\n■\\n■An arithmetic and logic unit (ALU) capable of operating on binary data\\n2\\u200a\\nThis book’s Companion Web site (WilliamStallings.com/ComputerOrganization) contains several links \\nto sites that provide photographs of many of the devices and components discussed in this section.\\n4A 1954 report [GOLD54] describes the implemented IAS machine and lists the final instruction set. It \\nis available at box.com/COA10e.\\n3The 1945 report on EDVAC is available at box.com/COA10e.\\n5In this book, unless otherwise noted, the term instruction refers to a machine instruction that is directly \\ninterpreted and executed by the processor, in contrast to a statement in a \\xad\\nhigh-\\u200b\\n\\xad\\nlevel language, such as Ada \\nor C++, which must first be compiled into a series of machine instructions before being executed.\\n12\\u2002 \\u2002 Chapter 1 / Basic Concepts and Computer Evolution \\n■\\n■A control unit, which interprets the instructions in memory and causes them \\nto be executed\\n■\\n■\\xad\\nInput–\\u200b\\n\\xad\\noutput (I/O) equipment operated by the control unit\\nThis structure was outlined in von Neumann’s earlier proposal, which is worth \\nquoting in part at this point [VONN45]:\\n2.2 First: Since the device is primarily a computer, it will \\nhave to perform the elementary operations of arithmetic most fre-\\nquently. These are addition, subtraction, multiplication, and divi-\\nsion. It is therefore reasonable that it should contain specialized \\norgans for just these operations.\\nControl\\ncircuits\\nAddresses\\nControl\\nsignals\\nInstructions\\nand data\\nAC: Accumulator register\\nMQ: multiply-quotient register\\nMBR: memory buffer register\\nIBR: instruction buffer register\\nPC: program counter\\nMAR: memory address register\\nIR: insruction register\\nInstructions\\nand data\\nM(0)\\nM(1)\\nM(2)\\nM(3)\\nM(4)\\nM(4095)\\nM(4093)\\nM(4092)\\nMBR\\nArithmetic-logic unit (CA)\\nCentral processing unit (CPU)\\nProgram control unit (CC)\\nInput-\\noutput\\nequipment\\n(I, O)\\nMain\\nmemory\\n(M)\\nAC\\nMQ\\nArithmetic-logic\\ncircuits\\nIBR\\nPC\\nIR\\nMAR\\nFigure\\xa01.6\\u2003 IAS Structure\\nIt must be observed, however, that while this principle as such \\nis probably sound, the specific way in which it is realized requires \\nclose scrutiny. At any rate a central arithmetical part of the device will \\nprobably have to exist, and this constitutes the first specific part:\\xa0CA.\\n2.3\\xa0 Second: The logical control of the device, that is, the \\nproper sequencing of its operations, can be most efficiently car-\\nried out by a central control organ. If the device is to be elastic, \\nthat is, as nearly as possible all purpose, then a distinction must \\nbe made between the specific instructions given for and defining \\na particular problem, and the general control organs that see to it \\nthat these \\xad\\ninstructions—\\u200b\\n\\xad\\nno matter what they \\xad\\nare—\\u200b\\n\\xad\\nare carried out. \\nThe former must be stored in some way; the latter are represented \\nby definite operating parts of the device. By the central control we \\nmean this latter function only, and the organs that perform it form \\nthe second specific part:\\xa0CC.\\n2.4 Third: Any device that is to carry out long and compli-\\ncated sequences of operations (specifically of calculations) must \\nhave a considerable memory . . .\\nThe instructions which govern a complicated problem may \\nconstitute considerable material, particularly so if the code is cir-\\ncumstantial (which it is in most arrangements). This material must \\nbe remembered.\\nAt any rate, the total memory constitutes the third specific \\npart of the device:\\xa0M.\\n2.6 The three specific parts CA, CC (together C), and M cor-\\nrespond to the associative neurons in the human nervous system. It \\nremains to discuss the equivalents of the sensory or afferent and the \\nmotor or efferent neurons. These are the input and output organs of \\nthe device.\\nThe device must be endowed with the ability to maintain \\ninput and output (sensory and motor) contact with some specific \\nmedium of this type. The medium will be called the outside record-\\ning medium of the device:\\xa0R.\\n2.7 Fourth: The device must have organs to transfer informa-\\ntion from R into its specific parts C and\\xa0M.\\xa0These organs form its \\ninput, the fourth specific part:\\xa0I.\\xa0It will be seen that it is best to make \\nall transfers from R (by I) into M and never directly from\\xa0C.\\n2.8 Fifth: The device must have organs to transfer from its \\nspecific parts C and M into\\xa0R.\\xa0These organs form its output, the \\nfifth specific part:\\xa0O.\\xa0It will be seen that it is again best to make all \\ntransfers from M (by O) into R, and never directly from\\xa0C.\\nWith rare exceptions, all of today’s computers have this same general structure \\nand function and are thus referred to as von Neumann machines. Thus, it is worth-\\nwhile at this point to describe briefly the operation of the IAS computer [BURK46, \\nGOLD54]. Following [HAYE98], the terminology and notation of von Neumann \\n1.3 / A Brief History of Computers\\u2002 \\u200213\\n14\\u2002 \\u2002 Chapter 1 / Basic Concepts and Computer Evolution \\nare changed in the following to conform more closely to modern usage; the exam-\\nples accompanying this discussion are based on that latter text.\\nThe memory of the IAS consists of 4,096 storage locations, called words, of \\n40\\xa0binary digits (bits) each.6 Both data and instructions are stored there. Numbers are \\nrepresented in binary form, and each instruction is a binary code. Figure\\xa01.7 illustrates \\nthese formats. Each number is represented by a sign bit and a 39-bit value. A\\xa0word \\nmay alternatively contain two 20-bit instructions, with each instruction consisting \\nof an 8-bit operation code (opcode) specifying the operation to be performed and \\na 12-bit address designating one of the words in memory (numbered from 0 to 999).\\nThe control unit operates the IAS by fetching instructions from memory \\nand executing them one at a time. We explain these operations with reference to \\n\\xad\\nFigure\\xa01.6. This figure reveals that both the control unit and the ALU contain stor-\\nage locations, called registers, defined as follows:\\n■\\n■Memory buffer register (MBR):  Contains a word to be stored in memory or sent \\nto the I/O unit, or is used to receive a word from memory or from the I/O unit.\\n■\\n■Memory address register (MAR): Specifies the address in memory of the word \\nto be written from or read into the\\xa0MBR.\\n■\\n■Instruction register (IR):  Contains the 8-bit opcode instruction being executed.\\n■\\n■Instruction buffer register (IBR): Employed to hold temporarily the \\xad\\nright-\\u200b\\n\\xad\\nhand instruction from a word in memory.\\n■\\n■Program counter (PC): Contains the address of the next instruction pair to be \\nfetched from memory.\\n■\\n■Accumulator (AC) and multiplier quotient (MQ): Employed to hold tem-\\nporarily operands and results of ALU operations. For example, the result \\n6There is no universal definition of the term word. In general, a word is an ordered set of bytes or bits \\nthat is the normal unit in which information may be stored, transmitted, or operated on within a given \\ncomputer. Typically, if a processor has a \\xad\\nfixed-\\u200b\\n\\xad\\nlength instruction set, then the instruction length equals \\nthe word length.\\n(a) Number word\\nsign bit\\n0\\n39\\n(b) Instruction word\\nopcode (8 bits)\\naddress (12 bits)\\nleft instruction (20 bits)\\n0\\n8\\n20\\n28\\n39\\n1\\nright instruction (20 bits)\\nopcode (8 bits)\\naddress (12 bits)\\nFigure\\xa01.7\\u2003 IAS Memory Formats\\nof multiplying two 40-bit numbers is an 80-bit number; the most significant \\n40\\xa0bits are stored in the AC and the least significant in the\\xa0MQ.\\nThe IAS operates by repetitively performing an instruction cycle, as shown in \\nFigure\\xa01.8. Each instruction cycle consists of two subcycles. During the fetch cycle, \\nthe opcode of the next instruction is loaded into the IR and the address portion is \\nloaded into the\\xa0MAR.\\xa0This instruction may be taken from the IBR, or it can be \\nobtained from memory by loading a word into the MBR, and then down to the IBR, \\nIR, and\\xa0MAR.\\nWhy the indirection? These operations are controlled by electronic circuitry \\nand result in the use of data paths. To simplify the electronics, there is only one reg-\\nister that is used to specify the address in memory for a read or write and only one \\nregister used for the source or destination.\\n1.3 / A Brief History of Computers\\u2002 \\u200215\\nStart\\nIs next\\ninstruction\\nin IBR?\\nMAR \\n   \\n    PC\\nMBR \\n   \\n    M(MAR)\\nIR    \\n    IBR (0:7)\\nMAR \\n  \\n     IBR (8:19)\\nIR    \\n     MBR (20:27)\\nMAR    \\n    MBR (28:39)\\nLeft\\ninstruction\\nrequired?\\nIBR \\n    \\n   MBR (20:39)\\nIR     \\n   MBR (0:7)\\nMAR \\n   \\n    MBR (8:19)\\nPC    \\n    PC + 1\\nYes\\nYes\\nYes\\nNo\\nNo\\nNo\\nM(X) = contents of memory location whose address is X\\n(i:j) = bits i through j\\nNo memory\\naccess\\nrequired\\nDecode instruction in IR\\nAC   \\n     M(X)\\nGo to M(X, 0:19)\\nIf AC > 0 then\\ngo to M(X, 0:19)\\nAC \\n   \\n    AC + M(X)\\nIs AC > 0?\\nMBR   \\n    M(MAR)\\nMBR \\n  \\n    M(MAR)\\nPC    \\n    MAR\\nAC    \\n    MBR\\nAC \\n   \\n    AC + MBR\\nFetch\\ncycle\\nExecution\\ncycle\\nFigure\\xa01.8\\u2003 Partial Flowchart of IAS Operation\\n16\\u2002 \\u2002 Chapter 1 / Basic Concepts and Computer Evolution \\nOnce the opcode is in the IR, the execute cycle is performed. Control circuitry \\ninterprets the opcode and executes the instruction by sending out the appropri-\\nate control signals to cause data to be moved or an operation to be performed by \\nthe\\xa0ALU.\\nThe IAS computer had a total of 21 instructions, which are listed in Table\\xa01.1. \\nThese can be grouped as follows:\\n■\\n■Data transfer: Move data between memory and ALU registers or between two \\nALU registers.\\n■\\n■Unconditional branch: Normally, the control unit executes instructions in \\nsequence from memory. This sequence can be changed by a branch instruc-\\ntion, which facilitates repetitive operations.\\nTable\\xa01.1\\u2003 The IAS Instruction Set\\nInstruction \\nType\\nOpcode\\nSymbolic \\nRepresentation\\nDescription\\nData transfer\\n00001010\\nLOAD MQ\\nTransfer contents of register MQ to the accumulator AC\\n00001001\\nLOAD MQ,M(X)\\nTransfer contents of memory location X to MQ\\n00100001\\nSTOR M(X)\\nTransfer contents of accumulator to memory location X\\n00000001\\nLOAD M(X)\\nTransfer M(X) to the accumulator\\n00000010\\nLOAD –M(X)\\nTransfer –M(X) to the accumulator\\n00000011\\nLOAD |M(X)|\\nTransfer absolute value of M(X) to the accumulator\\n00000100\\nLOAD –|M(X)|\\nTransfer –|M(X)| to the accumulator\\nUnconditional  \\nbranch\\n00001101\\nJUMP M(X,0:19)\\nTake next instruction from left half of M(X)\\n00001110\\nJUMP M(X,20:39)\\nTake next instruction from right half of M(X)\\nConditional \\nbranch\\n00001111\\nJUMP + M(X,0:19)\\nIf number in the accumulator is nonnegative, take next \\ninstruction from left half of M(X)\\n00010000\\nJUMP + M(X,20:39)\\nIf number in the accumulator is nonnegative, take next \\ninstruction from right half of M(X)\\nArithmetic\\n00000101\\nADD M(X)\\nAdd M(X) to AC; put the result in AC\\n00000111\\nADD |M(X)|\\nAdd |M(X)| to AC; put the result in AC\\n00000110\\nSUB M(X)\\nSubtract M(X) from AC; put the result in AC\\n00001000\\nSUB |M(X)|\\nSubtract |M(X)| from AC; put the remainder in AC\\n00001011\\nMUL M(X)\\nMultiply M(X) by MQ; put most significant bits of result \\nin AC, put least significant bits in MQ\\n00001100\\nDIV M(X)\\nDivide AC by M(X); put the quotient in MQ and the \\nremainder in AC\\n00010100\\nLSH\\nMultiply accumulator by 2; that is, shift left one bit position\\n00010101\\nRSH\\nDivide accumulator by 2; that is, shift right one position\\nAddress \\nmodify\\n00010010\\nSTOR M(X,8:19)\\nReplace left address field at M(X) by 12 rightmost bits \\nof AC\\n00010011\\nSTOR M(X,28:39)\\nReplace right address field at M(X) by 12 rightmost bits \\nof AC\\n■\\n■Conditional branch: The branch can be made dependent on a condition, thus \\nallowing decision points.\\n■\\n■Arithmetic: Operations performed by the\\xa0ALU.\\n■\\n■Address modify: Permits addresses to be computed in the ALU and then \\ninserted into instructions stored in memory. This allows a program consider-\\nable addressing flexibility.\\nTable\\xa01.1 presents instructions (excluding I/O instructions) in a symbolic, \\n\\xad\\neasy-\\u200b\\n\\xad\\nto-\\u200b\\n\\xad\\nread form. In binary form, each instruction must conform to the format of \\nFigure\\xa01.7b. The opcode portion (first 8 bits) specifies which of the 21 instructions is \\nto be executed. The address portion (remaining 12 bits) specifies which of the 4,096 \\nmemory locations is to be involved in the execution of the instruction.\\nFigure\\xa01.8 shows several examples of instruction execution by the control unit. \\nNote that each operation requires several steps, some of which are quite elaborate. \\nThe multiplication operation requires 39 suboperations, one for each bit position \\nexcept that of the sign bit.\\nThe Second Generation: Transistors\\nThe first major change in the electronic computer came with the replacement of the \\nvacuum tube by the transistor. The transistor, which is smaller, cheaper, and gener-\\nates less heat than a vacuum tube, can be used in the same way as a vacuum tube to \\nconstruct computers. Unlike the vacuum tube, which requires wires, metal plates, a \\nglass capsule, and a vacuum, the transistor is a \\xad\\nsolid-\\u200b\\n\\xad\\nstate device, made from silicon.\\nThe transistor was invented at Bell Labs in 1947 and by the 1950s had launched \\nan electronic revolution. It was not until the late 1950s, however, that fully transis-\\ntorized computers were commercially available. The use of the transistor defines \\nthe second generation of computers. It has become widely accepted to classify com-\\nputers into generations based on the fundamental hardware technology employed \\n(Table\\xa01.2). Each new generation is characterized by greater processing perfor-\\nmance, larger memory capacity, and smaller size than the previous one.\\nBut there are other changes as well. The second generation saw the intro-\\nduction of more complex arithmetic and logic units and control units, the use of \\n\\xad\\nhigh-\\u200b\\n\\xad\\nlevel programming languages, and the provision of system software with the \\n1.3 / A Brief History of Computers\\u2002 \\u200217\\nTable\\xa01.2\\u2003 Computer Generations\\nGeneration\\nApproximate \\nDates\\nTechnology\\nTypical Speed  \\n(operations per second)\\n1\\n1946–1957\\nVacuum tube\\n40,000\\n2\\n1957–1964\\nTransistor\\n200,000\\n3\\n1965–1971\\n\\xad\\nSmall-\\u200b\\n\\xad\\n and \\xad\\nmedium-\\u200b\\n\\xad\\nscale \\nintegration\\n1,000,000\\n4\\n1972–1977\\nLarge scale integration\\n10,000,000\\n5\\n1978–1991\\nVery large scale integration\\n100,000,000\\n6\\n1991–\\nUltra large scale integration\\n>1,000,000,000\\n18\\u2002 \\u2002 Chapter 1 / Basic Concepts and Computer Evolution \\ncomputer. In broad terms, system software provided the ability to load programs, \\nmove data to peripherals, and libraries to perform common computations, similar \\nto what modern operating systems, such as Windows and Linux, do.\\nIt will be useful to examine an important member of the second generation: the \\nIBM 7094 [BELL71]. From the introduction of the 700 series in 1952 to the introduc-\\ntion of the last member of the 7000 series in 1964, this IBM product line underwent \\nan evolution that is typical of computer products. Successive members of the product \\nline showed increased performance, increased capacity, and/or lower cost.\\nThe size of main memory, in multiples of 210 36-bit words, grew from \\n \\n2k (1k = 210) to 32k words,7 while the time to access one word of memory, the mem-\\nory cycle time, fell from 30 ms to 1.4 ms. The number of opcodes grew from a modest \\n24 to 185.\\nAlso, over the lifetime of this series of computers, the relative speed of the \\nCPU increased by a factor of 50. Speed improvements are achieved by improved \\nelectronics (e.g., a transistor implementation is faster than a vacuum tube imple-\\nmentation) and more complex circuitry. For example, the IBM 7094 includes an \\nInstruction Backup Register, used to buffer the next instruction. The control unit \\nfetches two adjacent words from memory for an instruction fetch. Except for the \\noccurrence of a branching instruction, which is relatively infrequent (perhaps 10 to \\n15%), this means that the control unit has to access memory for an instruction on \\nonly half the instruction cycles. This prefetching significantly reduces the average \\ninstruction cycle time.\\nFigure\\xa01.9 shows a large (many peripherals) configuration for an IBM 7094, \\nwhich is representative of \\xad\\nsecond-\\u200b\\n\\xad\\ngeneration computers. Several differences from \\nthe IAS computer are worth noting. The most important of these is the use of data \\nchannels. A data channel is an independent I/O module with its own processor and \\ninstruction set. In a computer system with such devices, the CPU does not execute \\ndetailed I/O instructions. Such instructions are stored in a main memory to be \\nexecuted by a \\xad\\nspecial-\\u200b\\n\\xad\\npurpose processor in the data channel itself. The CPU initi-\\nates an I/O transfer by sending a control signal to the data channel, instructing it to \\nexecute a sequence of instructions in memory. The data channel performs its task \\nindependently of the CPU and signals the CPU when the operation is complete. \\nThis arrangement relieves the CPU of a considerable processing burden.\\nAnother new feature is the multiplexor, which is the central termination \\npoint for data channels, the CPU, and memory. The multiplexor schedules access \\nto the memory from the CPU and data channels, allowing these devices to act \\nindependently.\\nThe Third Generation: Integrated Circuits\\nA single, \\xad\\nself-\\u200b\\n\\xad\\ncontained transistor is called a discrete component. Throughout \\n \\nthe 1950s and early 1960s, electronic equipment was composed largely of discrete \\n\\xad\\ncomponents—\\u200b\\n\\xad\\ntransistors, resistors, capacitors, and so on. Discrete components were \\nmanufactured separately, packaged in their own containers, and soldered or wired \\n7A discussion of the uses of numerical prefixes, such as kilo and giga, is contained in a supporting docu-\\nment at the Computer Science Student Resource Site at ComputerScienceStudent.com.\\ntogether onto \\xad\\nMasonite-\\u200b\\n\\xad\\nlike circuit boards, which were then installed in computers, \\noscilloscopes, and other electronic equipment. Whenever an electronic device called \\nfor a transistor, a little tube of metal containing a \\xad\\npinhead-\\u200b\\n\\xad\\nsized piece of silicon had \\nto be soldered to a circuit board. The entire manufacturing process, from transistor \\nto circuit board, was expensive and cumbersome.\\nThese facts of life were beginning to create problems in the computer indus-\\ntry. Early \\xad\\nsecond-\\u200b\\n\\xad\\ngeneration computers contained about 10,000 transistors. This \\nfigure grew to the hundreds of thousands, making the manufacture of newer, more \\npowerful machines increasingly difficult.\\nIn 1958 came the achievement that revolutionized electronics and started the \\nera of microelectronics: the invention of the integrated circuit. It is the integrated \\ncircuit that defines the third generation of computers. In this section, we provide a \\nbrief introduction to the technology of integrated circuits. Then we look at perhaps \\nthe two most important members of the third generation, both of which were intro-\\nduced at the beginning of that era: the IBM System/360 and the DEC \\xad\\nPDP-\\u200b\\n\\xad\\n8.\\nmicroelectronics Microelectronics means, literally, “small electronics.” Since the \\nbeginnings of digital electronics and the computer industry, there has been a persistent \\nand consistent trend toward the reduction in size of digital electronic circuits. Before \\nexamining the implications and benefits of this trend, we need to say something about \\nthe nature of digital electronics. A more detailed discussion is found in Chapter\\xa011.\\nCPU\\nMemory\\nIBM 7094 computer\\nPeripheral devices\\nData\\nchannel\\nMag tape\\nunits\\nCard\\npunch\\nLine\\nprinter\\nCard\\nreader\\nDrum\\nDisk\\nDisk\\nHyper-\\ntapes\\nTeleprocessing\\nequipment\\nData\\nchannel\\nData\\nchannel\\nData\\nchannel\\nMulti-\\nplexor\\nFigure\\xa01.9\\u2003 An IBM 7094 Configuration\\n1.3 / A Brief History of Computers\\u2002 \\u200219\\n20\\u2002 \\u2002 Chapter 1 / Basic Concepts and Computer Evolution \\nThe basic elements of a digital computer, as we know, must perform data stor-\\nage, movement, processing, and control functions. Only two fundamental types of \\ncomponents are required (Figure\\xa01.10): gates and memory cells. A gate is a device \\nthat implements a simple Boolean or logical function. For example, an AND gate \\nwith inputs A and B and output C implements the expression IF A AND B ARE \\nTRUE THEN C IS\\xa0TRUE.\\xa0Such devices are called gates because they control data \\nflow in much the same way that canal gates control the flow of water. The memory \\ncell is a device that can store 1 bit of data; that is, the device can be in one of two \\nstable states at any time. By interconnecting large numbers of these fundamental \\ndevices, we can construct a computer. We can relate this to our four basic functions \\nas follows:\\n■\\n■Data storage:  Provided by memory cells.\\n■\\n■Data processing:  Provided by gates.\\n■\\n■Data movement:  The paths among components are used to move data from \\nmemory to memory and from memory through gates to memory.\\n■\\n■Control:  The paths among components can carry control signals. For example, \\na gate will have one or two data inputs plus a control signal input that activates \\nthe gate. When the control signal is ON, the gate performs its function on the \\ndata inputs and produces a data output. Conversely, when the control signal \\nis OFF, the output line is null, such as the one produced by a high impedance \\nstate. Similarly, the memory cell will store the bit that is on its input lead when \\nthe WRITE control signal is ON and will place the bit that is in the cell on its \\noutput lead when the READ control signal is\\xa0ON.\\nThus, a computer consists of gates, memory cells, and interconnections among \\nthese elements. The gates and memory cells are, in turn, constructed of simple elec-\\ntronic components, such as transistors and capacitors.\\nThe integrated circuit exploits the fact that such components as transistors, \\nresistors, and conductors can be fabricated from a semiconductor such as silicon. \\nIt is merely an extension of the \\xad\\nsolid-\\u200b\\n\\xad\\nstate art to fabricate an entire circuit in a tiny \\npiece of silicon rather than assemble discrete components made from separate \\npieces of silicon into the same circuit. Many transistors can be produced at the same \\ntime on a single wafer of silicon. Equally important, these transistors can be con-\\nnected with a process of metallization to form circuits.\\nBoolean\\nlogic\\nfunction\\nInput\\nActivate\\nsignal\\n(a) Gate\\nOutput\\n•\\n•\\n•\\nBinary\\nstorage\\ncell\\nInput\\nRead\\nWrite\\n(b) Memory cell\\nOutput\\nFigure\\xa01.10\\u2003 Fundamental Computer Elements\\nFigure\\xa01.11 depicts the key concepts in an integrated circuit. A thin wafer of \\nsilicon is divided into a matrix of small areas, each a few millimeters square. The \\nidentical circuit pattern is fabricated in each area, and the wafer is broken up into \\nchips. Each chip consists of many gates and/or memory cells plus a number of input \\nand output attachment points. This chip is then packaged in housing that protects \\nit and provides pins for attachment to devices beyond the chip. A number of these \\npackages can then be interconnected on a printed circuit board to produce larger \\nand more complex circuits.\\nInitially, only a few gates or memory cells could be reliably manufactured and \\npackaged together. These early integrated circuits are referred to as \\xad\\nsmall-\\u200b\\n\\xad\\nscale \\nintegration (SSI). As time went on, it became possible to pack more and more com-\\nponents on the same chip. This growth in density is illustrated in Figure\\xa01.12; it is \\none of the most remarkable technological trends ever recorded.8 This figure reflects \\nthe famous Moore’s law, which was propounded by Gordon Moore, cofounder of \\nIntel, in 1965 [MOOR65]. Moore observed that the number of transistors that could \\nbe put on a single chip was doubling every year, and correctly predicted that this \\npace would continue into the near future. To the surprise of many, including Moore, \\nthe pace continued year after year and decade after decade. The pace slowed to a \\ndoubling every 18\\xa0months in the 1970s but has sustained that rate ever since.\\nThe consequences of Moore’s law are profound:\\n1.\\t The cost of a chip has remained virtually unchanged during this period of rapid \\ngrowth in density. This means that the cost of computer logic and memory cir-\\ncuitry has fallen at a dramatic rate.\\nWafer\\nChip\\nGate\\nPackaged\\nchip\\nFigure\\xa01.11\\u2003 Relationship among \\nWafer, Chip, and Gate\\n1.3 / A Brief History of Computers\\u2002 \\u200221\\n8Note that the vertical axis uses a log scale. A basic review of log scales is in the math refresher document \\nat the Computer Science Student Resource Site at ComputerScienceStudent.com.\\n22\\u2002 \\u2002 Chapter 1 / Basic Concepts and Computer Evolution \\n2.\\t Because logic and memory elements are placed closer together on more \\ndensely packed chips, the electrical path length is shortened, increasing oper-\\nating speed.\\n3.\\t The computer becomes smaller, making it more convenient to place in a vari-\\nety of environments.\\n4.\\t There is a reduction in power requirements.\\n5.\\t The interconnections on the integrated circuit are much more reliable than \\nsolder connections. With more circuitry on each chip, there are fewer inter-\\nchip connections.\\nibm system/360 By 1964, IBM had a firm grip on the computer market with \\nits 7000 series of machines. In that year, IBM announced the System/360, a new \\nfamily of computer products. Although the announcement itself was no surprise, it \\ncontained some unpleasant news for current IBM customers: the 360 product line \\nwas incompatible with older IBM machines. Thus, the transition to the 360 would \\nbe difficult for the current customer base, but IBM felt this was necessary to break \\nout of some of the constraints of the 7000 architecture and to produce a system \\ncapable of evolving with the new integrated circuit technology [PADE81, GIFF87]. \\nThe strategy paid off both financially and technically. The 360 was the success of \\nthe decade and cemented IBM as the overwhelmingly dominant computer vendor, \\nwith a market share above 70%. And, with some modifications and extensions, the \\narchitecture of the 360 remains to this day the architecture of IBM’s mainframe9 \\ncomputers. Examples using this architecture can be found throughout this text.\\nThe System/360 was the industry’s first planned family of computers. The family \\ncovered a wide range of performance and cost. The models were compatible in the \\n1\\n1947\\nFirst working\\ntransistor\\nMoore’s law\\npromulgated\\nInvention of\\nintegrated circuit\\n50\\n55\\n60\\n65\\n70\\n75\\n80\\n85\\n90\\n95\\n2000\\n05\\n11\\n10\\n100\\n1,000\\n10,000\\n100,000\\n10 m\\n100 m\\n1 bn\\n10 bn\\n100 bn\\nFigure\\xa01.12\\u2003 Growth in Transistor Count on Integrated Circuits\\n9The term mainframe is used for the larger, most powerful computers other than supercomputers. Typical \\ncharacteristics of a mainframe are that it supports a large database, has elaborate I/O hardware, and is \\nused in a central data processing facility.\\nsense that a program written for one model should be capable of being executed by \\nanother model in the series, with only a difference in the time it takes to execute.\\nThe concept of a family of compatible computers was both novel and extremely \\nsuccessful. A customer with modest requirements and a budget to match could start \\nwith the relatively inexpensive Model 30. Later, if the customer’s needs grew, it was \\npossible to upgrade to a faster machine with more memory without sacrificing the \\ninvestment in \\xad\\nalready-\\u200b\\n\\xad\\ndeveloped software. The characteristics of a family are as follows:\\n■\\n■Similar or identical instruction set: In many cases, the exact same set of \\nmachine instructions is supported on all members of the family. Thus, a pro-\\ngram that executes on one machine will also execute on any other. In some \\ncases, the lower end of the family has an instruction set that is a subset of \\nthat of the top end of the family. This means that programs can move up but \\nnot down.\\n■\\n■Similar or identical operating system: The same basic operating system is \\navailable for all family members. In some cases, additional features are added \\nto the \\xad\\nhigher-\\u200b\\n\\xad\\nend members.\\n■\\n■Increasing speed: The rate of instruction execution increases in going from \\nlower to higher family members.\\n■\\n■Increasing number of I/O ports: The number of I/O ports increases in going \\nfrom lower to higher family members.\\n■\\n■Increasing memory size: The size of main memory increases in going from \\nlower to higher family members.\\n■\\n■Increasing cost: At a given point in time, the cost of a system increases in going \\nfrom lower to higher family members.\\nHow could such a family concept be implemented? Differences were achieved \\nbased on three factors: basic speed, size, and degree of simultaneity [STEV64]. For \\nexample, greater speed in the execution of a given instruction could be gained by \\nthe use of more complex circuitry in the ALU, allowing suboperations to be car-\\nried out in parallel. Another way of increasing speed was to increase the width of \\nthe data path between main memory and the\\xa0CPU.\\xa0On the Model 30, only 1 byte \\n(8 bits) could be fetched from main memory at a time, whereas 8 bytes could be \\nfetched at a time on the Model 75.\\nThe System/360 not only dictated the future course of IBM but also had a pro-\\nfound impact on the entire industry. Many of its features have become standard on \\nother large computers.\\ndec \\xad\\npdp-\\u200b\\n\\xad\\n8 In the same year that IBM shipped its first System/360, another \\nmomentous first shipment occurred: \\xad\\nPDP-\\u200b\\n\\xad\\n8 from Digital Equipment Corporation \\n(DEC). At a time when the average computer required an \\xad\\nair-\\u200b\\n\\xad\\nconditioned room, \\nthe \\xad\\nPDP-\\u200b\\n\\xad\\n8 (dubbed a minicomputer by the industry, after the miniskirt of the day) \\nwas small enough that it could be placed on top of a lab bench or be built into \\nother equipment. It could not do everything the mainframe could, but at $16,000, it \\nwas cheap enough for each lab technician to have one. In contrast, the System/360 \\nseries of mainframe computers introduced just a few months before cost hundreds \\nof thousands of dollars.\\n1.3 / A Brief History of Computers\\u2002 \\u200223\\n24\\u2002 \\u2002 Chapter 1 / Basic Concepts and Computer Evolution \\nThe low cost and small size of the \\xad\\nPDP-\\u200b\\n\\xad\\n8 enabled another manufacturer to \\npurchase a \\xad\\nPDP-\\u200b\\n\\xad\\n8 and integrate it into a total system for resale. These other manu-\\nfacturers came to be known as original equipment manufacturers (OEMs), and the \\nOEM market became and remains a major segment of the computer marketplace.\\nIn contrast to the \\xad\\ncentral-\\u200b\\n\\xad\\nswitched architecture (Figure\\xa01.9) used by IBM on its \\n700/7000 and 360 systems, later models of the \\xad\\nPDP-\\u200b\\n\\xad\\n8 used a structure that became vir-\\ntually universal for microcomputers: the bus structure. This is illustrated in Figure\\xa01.13. \\nThe \\xad\\nPDP-\\u200b\\n\\xad\\n8 bus, called the Omnibus, consists of 96 separate signal paths, used to carry \\ncontrol, address, and data signals. Because all system components share a common \\nset of signal paths, their use can be controlled by the\\xa0CPU.\\xa0This architecture is highly \\nflexible, allowing modules to be plugged into the bus to create various configurations. \\nIt is only in recent years that the bus structure has given way to a structure known as \\n\\xad\\npoint-\\u200b\\n\\xad\\nto-\\u200b\\n\\xad\\npoint interconnect, described in Chapter\\xa03.\\nLater Generations\\nBeyond the third generation there is less general agreement on defining generations \\nof computers. Table\\xa01.2 suggests that there have been a number of later generations, \\nbased on advances in integrated circuit technology. With the introduction of \\xad\\nlarge-\\u200b\\n\\xad\\nscale integration (LSI), more than 1,000 components can be placed on a single inte-\\ngrated circuit chip. \\xad\\nVery-\\u200b\\n\\xad\\nlarge-\\u200b\\n\\xad\\nscale integration (VLSI) achieved more than 10,000 \\ncomponents per chip, while current \\xad\\nultra-\\u200b\\n\\xad\\nlarge-\\u200b\\n\\xad\\nscale integration (ULSI) chips can \\ncontain more than one billion components.\\nWith the rapid pace of technology, the high rate of introduction of new prod-\\nucts, and the importance of software and communications as well as hardware, the \\nclassification by generation becomes less clear and less meaningful. In this section, \\nwe mention two of the most important of developments in later generations.\\nsemiconductor memory The first application of integrated circuit technology \\nto computers was the construction of the processor (the control unit and the \\narithmetic and logic unit) out of integrated circuit chips. But it was also found that \\nthis same technology could be used to construct memories.\\nIn the 1950s and 1960s, most computer memory was constructed from tiny \\nrings of ferromagnetic material, each about a sixteenth of an inch in diameter. \\nThese rings were strung up on grids of fine wires suspended on small screens inside \\nthe computer. Magnetized one way, a ring (called a core) represented a one; mag-\\nnetized the other way, it stood for a zero. \\xad\\nMagnetic-\\u200b\\n\\xad\\ncore memory was rather fast; \\nit took as little as a millionth of a second to read a bit stored in memory. But it was \\nConsole\\ncontroller\\nCPU\\nOmnibus\\nMain\\nmemory\\nI/O\\nmodule\\nI/O\\nmodule\\n• • •\\nFigure\\xa01.13\\u2003 \\xad\\nPDP-\\u200b\\n\\xad\\n8 Bus Structure\\nexpensive and bulky, and used destructive readout: The simple act of reading a core \\nerased the data stored in it. It was therefore necessary to install circuits to restore \\nthe data as soon as it had been extracted.\\nThen, in 1970, Fairchild produced the first relatively capacious semiconductor \\nmemory. This chip, about the size of a single core, could hold 256 bits of memory. It \\nwas nondestructive and much faster than core. It took only 70\\xa0billionths of a second \\nto read a bit. However, the cost per bit was higher than for that of core.\\nIn 1974, a seminal event occurred: The price per bit of semiconductor memory \\ndropped below the price per bit of core memory. Following this, there has been a con-\\ntinuing and rapid decline in memory cost accompanied by a corresponding increase in \\nphysical memory density. This has led the way to smaller, faster machines with mem-\\nory sizes of larger and more expensive machines from just a few years earlier. Devel-\\nopments in memory technology, together with developments in processor technology \\nto be discussed next, changed the nature of computers in less than a decade. Although \\nbulky, expensive computers remain a part of the landscape, the computer has also \\nbeen brought out to the “end user,” with office machines and personal computers.\\nSince 1970, semiconductor memory has been through 13 generations: 1k, 4k, \\n16k, 64k, 256k, 1M, 4M, 16M, 64M, 256M, 1G, 4G, and, as of this writing, 8 Gb \\non a single chip (1 k = 210, 1 M = 220, 1 G = 230). Each generation has provided \\nincreased storage density, accompanied by declining cost per bit and declining \\naccess time. Densities are projected to reach 16 Gb by 2018 and 32 Gb by 2023 \\n[ITRS14].\\nmicroprocessors Just as the density of elements on memory chips has continued \\nto rise, so has the density of elements on processor chips. As time went on, more \\nand more elements were placed on each chip, so that fewer and fewer chips were \\nneeded to construct a single computer processor.\\nA breakthrough was achieved in 1971, when Intel developed its 4004. The \\n4004 was the first chip to contain all of the components of a CPU on a single chip: \\nThe microprocessor was born.\\nThe 4004 can add two 4-bit numbers and can multiply only by repeated addi-\\ntion. By today’s standards, the 4004 is hopelessly primitive, but it marked the begin-\\nning of a continuing evolution of microprocessor capability and power.\\nThis evolution can be seen most easily in the number of bits that the processor \\ndeals with at a time. There is no \\xad\\nclear-\\u200b\\n\\xad\\ncut measure of this, but perhaps the best meas-\\nure is the data bus width: the number of bits of data that can be brought into or sent \\nout of the processor at a time. Another measure is the number of bits in the accumu-\\nlator or in the set of \\xad\\ngeneral-\\u200b\\n\\xad\\npurpose registers. Often, these measures coincide, but \\nnot always. For example, a number of microprocessors were developed that operate \\non 16-bit numbers in registers but can only read and write 8 bits at a time.\\nThe next major step in the evolution of the microprocessor was the introduc-\\ntion in 1972 of the Intel 8008. This was the first 8-bit microprocessor and was almost \\ntwice as complex as the 4004.\\nNeither of these steps was to have the impact of the next major event: the \\nintroduction in 1974 of the Intel 8080. This was the first \\xad\\ngeneral-\\u200b\\n\\xad\\npurpose micropro-\\ncessor. Whereas the 4004 and the 8008 had been designed for specific applications, \\nthe 8080 was designed to be the CPU of a \\xad\\ngeneral-\\u200b\\n\\xad\\npurpose microcomputer. Like the \\n1.3 / A Brief History of Computers\\u2002 \\u200225\\n26\\u2002 \\u2002 Chapter 1 / Basic Concepts and Computer Evolution \\nTable\\xa01.3\\u2003 Evolution of Intel Microprocessors (page 1 of 2)\\n(a) 1970s Processors\\n4004\\n8008\\n8080\\n8086\\n8088\\nIntroduced\\n1971\\n1972\\n1974\\n1978\\n1979\\nClock speeds\\n108 kHz\\n108 kHz\\n2 MHz\\n5 MHz, 8 MHz, 10 MHz\\n5 MHz, 8 MHz\\nBus width\\n4 bits\\n8 bits\\n8 bits\\n16 bits\\n8 bits\\nNumber of transistors\\n2,300\\n3,500\\n6,000\\n29,000\\n29,000\\nFeature size (mm)\\n10\\n8\\n6\\n3\\n6\\nAddressable memory\\n640 bytes\\n16 KB\\n64 KB\\n1 MB\\n1 MB\\n(b) 1980s Processors\\n80286\\n386TM DX\\n386TM SX\\n486TM DX CPU\\nIntroduced\\n1982\\n1985\\n1988\\n1989\\nClock speeds\\n6–12.5 MHz\\n16–33 MHz\\n16–33 MHz\\n25–50 MHz\\nBus width\\n16 bits\\n32 bits\\n16 bits\\n32 bits\\nNumber of transistors\\n134,000\\n275,000\\n275,000\\n1.2\\xa0million\\nFeature size (\\u2009µm)\\n1.5\\n1\\n1\\n0.8–1\\nAddressable memory\\n16 MB\\n4 GB\\n16 MB\\n4 GB\\nVirtual memory\\n1 GB\\n64 TB\\n64 TB\\n64 TB\\nCache\\n—\\n—\\n—\\n8 kB\\n(c) 1990s Processors\\n486TM SX\\nPentium\\nPentium Pro\\nPentium II\\nIntroduced\\n1991\\n1993\\n1995\\n1997\\nClock speeds\\n16–33 MHz\\n60–166 MHz,\\n150–200 MHz\\n200–300 MHz\\nBus width\\n32 bits\\n32 bits\\n64 bits\\n64 bits\\nNumber of transistors\\n1.185\\xa0million\\n3.1\\xa0million\\n5.5\\xa0million\\n7.5\\xa0million\\nFeature size (\\u2009µm)\\n1\\n0.8\\n0.6\\n0.35\\nAddressable memory\\n4 GB\\n4 GB\\n64 GB\\n64 GB\\nVirtual memory\\n64 TB\\n64 TB\\n64 TB\\n64 TB\\nCache\\n8 kB\\n8 kB\\n512 kB L1 and  \\n1 MB L2\\n512 kB L2\\n8008, the 8080 is an 8-bit microprocessor. The 8080, however, is faster, has a richer \\ninstruction set, and has a large addressing capability.\\nAbout the same time, 16-bit microprocessors began to be developed. How-\\never, it was not until the end of the 1970s that powerful, \\xad\\ngeneral-\\u200b\\n\\xad\\npurpose 16-bit \\nmicroprocessors appeared. One of these was the 8086. The next step in this trend \\noccurred in 1981, when both Bell Labs and \\xad\\nHewlett-\\u200b\\n\\xad\\nPackard developed 32-bit, \\n\\xad\\nsingle-\\u200b\\n\\xad\\nchip microprocessors. Intel introduced its own 32-bit microprocessor, the \\n80386, in 1985 (Table\\xa01.3).\\n1.4 / The Evolution of the Intel x86 Architecture\\u2002 \\u200227\\n(d) Recent Processors\\nPentium III\\nPentium 4\\nCore 2 Duo\\nCore i7 EE 4960X\\nIntroduced\\n1999\\n2000\\n2006\\n2013\\nClock speeds\\n450–660 MHz\\n1.3–1.8 GHz\\n1.06–1.2 GHz\\n4 GHz\\nBus width\\n64 bits\\n64 bits\\n64 bits\\n64 bits\\nNumber of transistors\\n9.5\\xa0million\\n42\\xa0million\\n167\\xa0million\\n1.86\\xa0billion\\nFeature size (nm)\\n250\\n180\\n65\\n22\\nAddressable memory\\n64 GB\\n64 GB\\n64 GB\\n64 GB\\nVirtual memory\\n64 TB\\n64 TB\\n64 TB\\n64 TB\\nCache\\n512 kB L2\\n256 kB L2\\n2 MB L2\\n1.5 MB L2/15 MB L3\\nNumber of cores\\n1\\n1\\n2\\n6\\n\\t 1.4\\t The Evolution of the Intel x86 Architecture\\nThroughout this book, we rely on many concrete examples of computer design and \\nimplementation to illustrate concepts and to illuminate \\xad\\ntrade-\\u200b\\n\\xad\\noffs. Numerous sys-\\ntems, both contemporary and historical, provide examples of important computer \\narchitecture design features. But the book relies principally on examples from two \\nprocessor families: the Intel x86 and the ARM architectures. The current x86 offer-\\nings represent the results of decades of design effort on complex instruction set com-\\nputers (CISCs). The x86 incorporates the sophisticated design principles once found \\nonly on mainframes and supercomputers and serves as an excellent example of CISC \\ndesign. An alternative approach to processor design is the reduced instruction set \\ncomputer (RISC). The ARM architecture is used in a wide variety of embedded sys-\\ntems and is one of the most powerful and \\xad\\nbest-\\u200b\\n\\xad\\ndesigned \\xad\\nRISC-\\u200b\\n\\xad\\nbased systems on the \\nmarket. In this section and the next, we provide a brief overview of these two systems.\\nIn terms of market share, Intel has ranked as the number one maker of micro-\\nprocessors for \\xad\\nnon-\\u200b\\n\\xad\\nembedded systems for decades, a position it seems unlikely to \\nyield. The evolution of its flagship microprocessor product serves as a good indica-\\ntor of the evolution of computer technology in general.\\nTable\\xa01.3 shows that evolution. Interestingly, as microprocessors have grown \\nfaster and much more complex, Intel has actually picked up the pace. Intel used \\nto develop microprocessors one after another, every four years. But Intel hopes \\nto keep rivals at bay by trimming a year or two off this development time, and has \\ndone so with the most recent x86 generations.10\\n10Intel refers to this as the \\xad\\ntick-\\u200b\\n\\xad\\ntock model. Using this model, Intel has successfully delivered \\xad\\nnext-\\u200b\\n\\xad\\ngeneration silicon technology as well as new processor microarchitecture on alternating years for the \\npast several years. See http://www.intel.com/content/www/us/en/\\xad\\nsilicon-\\xad\\ninnovations/intel-tick-tock- \\nmodel-general.html.\\n28\\u2002 \\u2002 Chapter 1 / Basic Concepts and Computer Evolution \\nIt is worthwhile to list some of the highlights of the evolution of the Intel prod-\\nuct line:\\n■\\n■8080: The world’s first \\xad\\ngeneral-\\u200b\\n\\xad\\npurpose microprocessor. This was an 8-bit \\nmachine, with an 8-bit data path to memory. The 8080 was used in the first \\npersonal computer, the Altair.\\n■\\n■8086: A far more powerful, 16-bit machine. In addition to a wider data path \\nand larger registers, the 8086 sported an instruction cache, or queue, that \\nprefetches a few instructions before they are executed. A variant of this pro-\\ncessor, the 8088, was used in IBM’s first personal computer, securing the suc-\\ncess of Intel. The 8086 is the first appearance of the x86 architecture.\\n■\\n■80286: This extension of the 8086 enabled addressing a 16-MB memory instead \\nof just 1\\xa0MB.\\n■\\n■80386: Intel’s first 32-bit machine, and a major overhaul of the product. With \\na 32-bit architecture, the 80386 rivaled the complexity and power of minicom-\\nputers and mainframes introduced just a few years earlier. This was the first \\nIntel processor to support multitasking, meaning it could run multiple pro-\\ngrams at the same time.\\n■\\n■80486: The 80486 introduced the use of much more sophisticated and power-\\nful cache technology and sophisticated instruction pipelining. The 80486 also \\noffered a \\xad\\nbuilt-\\u200b\\n\\xad\\nin math coprocessor, offloading complex math operations from \\nthe main\\xa0CPU.\\n■\\n■Pentium: With the Pentium, Intel introduced the use of superscalar tech-\\nniques, which allow multiple instructions to execute in parallel.\\n■\\n■Pentium Pro: The Pentium Pro continued the move into superscalar organiza-\\ntion begun with the Pentium, with aggressive use of register renaming, branch \\nprediction, data flow analysis, and speculative execution.\\n■\\n■Pentium II: The Pentium II incorporated Intel MMX technology, which is \\ndesigned specifically to process video, audio, and graphics data efficiently.\\n■\\n■Pentium III: The Pentium III incorporates additional \\xad\\nfloating-\\u200b\\n\\xad\\npoint instruc-\\ntions: The Streaming SIMD Extensions (SSE) instruction set extension added \\n70 new instructions designed to increase performance when exactly the same \\noperations are to be performed on multiple data objects. Typical applications \\nare digital signal processing and graphics processing.\\n■\\n■Pentium 4: The Pentium 4 includes additional \\xad\\nfloating-\\u200b\\n\\xad\\npoint and other \\nenhancements for multimedia.11\\n■\\n■Core: This is the first Intel x86 microprocessor with a dual core, referring to \\nthe implementation of two cores on a single chip.\\n■\\n■Core 2: The Core 2 extends the Core architecture to 64 bits. The Core 2 Quad \\nprovides four cores on a single chip. More recent Core offerings have up to 10 \\ncores per chip. An important addition to the architecture was the Advanced \\nVector Extensions instruction set that provided a set of 256-bit, and then 512-\\nbit, instructions for efficient processing of vector data.\\n11With the Pentium 4, Intel switched from Roman numerals to Arabic numerals for model numbers.\\n1.5 / Embedded Systems\\u2002 \\u200229\\nAlmost 40 years after its introduction in 1978, the x86 architecture continues to \\ndominate the processor market outside of embedded systems. Although the organiza-\\ntion and technology of the x86 machines have changed dramatically over the decades, \\nthe instruction set architecture has evolved to remain backward compatible with ear-\\nlier versions. Thus, any program written on an older version of the x86 architecture \\ncan execute on newer versions. All changes to the instruction set architecture have \\ninvolved additions to the instruction set, with no subtractions. The rate of change has \\nbeen the addition of roughly one instruction per month added to the architecture \\n[ANTH08], so that there are now thousands of instructions in the instruction set.\\nThe x86 provides an excellent illustration of the advances in computer hard-\\nware over the past 35 years. The 1978 8086 was introduced with a clock speed of \\n5\\xa0MHz and had 29,000 transistors. A \\xad\\nsix-\\u200b\\n\\xad\\ncore Core i7 EE 4960X introduced in 2013 \\noperates at 4 GHz, a speedup of a factor of 800, and has 1.86\\xa0billion transistors, \\nabout 64,000 times as many as the 8086. Yet the Core i7 EE 4960X is in only a \\nslightly larger package than the 8086 and has a comparable cost.\\n\\t 1.5\\t Embedded Systems\\nThe term embedded system refers to the use of electronics and software within a \\nproduct, as opposed to a \\xad\\ngeneral-\\u200b\\n\\xad\\npurpose computer, such as a laptop or desktop sys-\\ntem. Millions of computers are sold every year, including laptops, personal comput-\\ners, workstations, servers, mainframes, and supercomputers. In contrast, billions of \\ncomputer systems are produced each year that are embedded within larger devices. \\nToday, many, perhaps most, devices that use electric power have an embedded com-\\nputing system. It is likely that in the near future virtually all such devices will have \\nembedded computing systems.\\nTypes of devices with embedded systems are almost too numerous to list. \\nExamples include cell phones, digital cameras, video cameras, calculators, micro-\\nwave ovens, home security systems, washing machines, lighting systems, ther-\\nmostats, printers, various automotive systems (e.g., transmission control, cruise \\ncontrol, fuel injection, \\xad\\nanti-\\u200b\\n\\xad\\nlock brakes, and suspension systems), tennis rack-\\nets, toothbrushes, and numerous types of sensors and actuators in automated \\nsystems.\\nOften, embedded systems are tightly coupled to their environment. This can \\ngive rise to \\xad\\nreal-\\u200b\\n\\xad\\ntime constraints imposed by the need to interact with the environ-\\nment. Constraints, such as required speeds of motion, required precision of meas-\\nurement, and required time durations, dictate the timing of software operations. If \\nmultiple activities must be managed simultaneously, this imposes more complex \\n\\xad\\nreal-\\u200b\\n\\xad\\ntime constraints.\\nFigure\\xa01.14 shows in general terms an embedded system organization. In addi-\\ntion to the processor and memory, there are a number of elements that differ from \\nthe typical desktop or laptop computer:\\n■\\n■There may be a variety of interfaces that enable the system to measure, manip-\\nulate, and otherwise interact with the external environment. Embedded sys-\\ntems often interact (sense, manipulate, and communicate) with external world \\nthrough sensors and actuators and hence are typically reactive systems; a \\n30\\u2002 \\u2002 Chapter 1 / Basic Concepts and Computer Evolution \\nreactive system is in continual interaction with the environment and executes \\nat a pace determined by that environment.\\n■\\n■The human interface may be as simple as a flashing light or as complicated as \\n\\xad\\nreal-\\u200b\\n\\xad\\ntime robotic vision. In many cases, there is no human interface.\\n■\\n■The diagnostic port may be used for diagnosing the system that is being \\n\\xad\\ncontrolled—\\u200b\\n\\xad\\nnot just for diagnosing the computer.\\n■\\n■\\xad\\nSpecial-\\u200b\\n\\xad\\npurpose field programmable (FPGA), \\xad\\napplication-\\u200b\\n\\xad\\nspecific (ASIC), or \\neven nondigital hardware may be used to increase performance or reliability.\\n■\\n■Software often has a fixed function and is specific to the application.\\n■\\n■Efficiency is of paramount importance for embedded systems. They are opti-\\nmized for energy, code size, execution time, weight and dimensions, and cost.\\nThere are several noteworthy areas of similarity to \\xad\\ngeneral-\\u200b\\n\\xad\\npurpose computer \\nsystems as well:\\n■\\n■Even with nominally fixed function software, the ability to field upgrade to fix \\nbugs, to improve security, and to add functionality, has become very important \\nfor embedded systems, and not just in consumer devices.\\n■\\n■One comparatively recent development has been of embedded system plat-\\nforms that support a wide variety of apps. Good examples of this are smart-\\nphones and audio/visual devices, such as smart TVs.\\nThe Internet of Things\\nIt is worthwhile to separately callout  one of the major drivers in the \\xad\\nproliferation of \\nembedded systems. The Internet of things (IoT) is a term that refers to the expanding \\nMemory\\nCustom\\nlogic\\nHuman\\ninterface\\nDiagnostic\\nport\\nProcessor\\nD/A\\nConversion\\nActuators/\\nindicators\\nA/D\\nconversion\\nSensors\\nFigure\\xa01.14\\u2003 Possible Organization of an Embedded \\nSystem\\n1.5 / Embedded Systems\\u2002 \\u200231\\ninterconnection of smart devices, ranging from appliances to tiny sensors. A domi-\\nnant theme is the embedding of \\xad\\nshort-\\u200b\\n\\xad\\nrange mobile transceivers into a wide array of \\ngadgets and everyday items, enabling new forms of communication between people \\nand things, and between things themselves. The Internet now supports the intercon-\\nnection of billions of industrial and personal objects, usually through cloud systems. \\nThe objects deliver sensor information, act on their environment, and, in some cases, \\nmodify themselves, to create overall management of a larger system, like a factory \\nor city.\\nThe IoT is primarily driven by deeply embedded devices (defined below). \\nThese devices are \\xad\\nlow-\\u200b\\n\\xad\\nbandwidth, \\xad\\nlow-\\u200b\\n\\xad\\nrepetition \\xad\\ndata-\\u200b\\n\\xad\\ncapture, and \\xad\\nlow-\\u200b\\n\\xad\\nbandwidth \\n\\xad\\ndata-\\u200b\\n\\xad\\nusage appliances that communicate with each other and provide data via user \\ninterfaces. Embedded appliances, such as \\xad\\nhigh-\\u200b\\n\\xad\\nresolution video security cameras, \\nvideo VoIP phones, and a handful of others, require \\xad\\nhigh-\\u200b\\n\\xad\\nbandwidth streaming \\ncapabilities. Yet countless products simply require packets of data to be intermit-\\ntently delivered.\\nWith reference to the end systems supported, the Internet has gone through \\nroughly four generations of deployment culminating in the IoT:\\n1.\\t Information technology (IT): PCs, servers, routers, firewalls, and so on, bought \\nas IT devices by enterprise IT people and primarily using wired connectivity.\\n2.\\t Operational technology (OT): Machines/appliances with embedded IT built \\nby \\xad\\nnon-\\u200b\\n\\xad\\nIT companies, such as medical machinery, SCADA (supervisory con-\\ntrol and data acquisition), process control, and kiosks, bought as appliances by \\nenterprise OT people and primarily using wired connectivity.\\n3.\\t Personal technology: Smartphones, tablets, and eBook readers bought as IT \\ndevices by consumers (employees) exclusively using wireless connectivity and \\noften multiple forms of wireless connectivity.\\n4.\\t Sensor/actuator technology: \\xad\\nSingle-\\u200b\\n\\xad\\npurpose devices bought by consumers, IT, \\nand OT people exclusively using wireless connectivity, generally of a single \\nform, as part of larger systems.\\nIt is the fourth generation that is usually thought of as the IoT, and it is marked \\nby the use of billions of embedded devices.\\nEmbedded Operating Systems\\nThere are two general approaches to developing an embedded operating system \\n(OS). The first approach is to take an existing OS and adapt it for the embedded \\napplication. For example, there are embedded versions of Linux, Windows, and \\nMac, as well as other commercial and proprietary operating systems specialized for \\nembedded systems. The other approach is to design and implement an OS intended \\nsolely for embedded use. An example of the latter is TinyOS, widely used in wireless \\nsensor networks. This topic is explored in depth in [STAL15].\\nApplication Processors versus Dedicated Processors\\nIn this subsection, and the next two, we briefly introduce some terms commonly \\nfound in the literature on embedded systems. Application processors are defined \\n32\\u2002 \\u2002 Chapter 1 / Basic Concepts and Computer Evolution \\nby the processor’s ability to execute complex operating systems, such as Linux, \\nAndroid, and Chrome. Thus, the application processor is \\xad\\ngeneral-\\u200b\\n\\xad\\npurpose in nature. \\nA good example of the use of an embedded application processor is the smartphone. \\nThe embedded system is designed to support numerous apps and perform a wide \\nvariety of functions.\\nMost embedded systems employ a dedicated processor, which, as the name \\nimplies, is dedicated to one or a small number of specific tasks required by the host \\ndevice. Because such an embedded system is dedicated to a specific task or tasks, \\nthe processor and associated components can be engineered to reduce size and cost.\\nMicroprocessors versus Microcontrollers\\nAs we have seen, early microprocessor chips included registers, an ALU, and some \\nsort of control unit or instruction processing logic. As transistor density increased, it \\nbecame possible to increase the complexity of the instruction set architecture, and \\nultimately to add memory and more than one processor. Contemporary micropro-\\ncessor chips, as shown in Figure\\xa01.2, include multiple cores and a substantial amount \\nof cache memory.\\nA microcontroller chip makes a substantially different use of the logic space \\navailable. Figure\\xa01.15 shows in general terms the elements typically found on a \\nmicrocontroller chip. As shown, a microcontroller is a single chip that contains the \\nprocessor, \\xad\\nnon-\\u200b\\n\\xad\\nvolatile memory for the program (ROM), volatile memory for input \\nand output (RAM), a clock, and an I/O control unit. The processor portion of the \\nmicrocontroller has a much lower silicon area than other microprocessors and much \\nhigher energy efficiency. We examine microcontroller organization in more detail \\nin Section\\xa01.6.\\nAlso called a “computer on a chip,” billions of microcontroller units are \\nembedded each year in myriad products from toys to appliances to automobiles. For \\nexample, a single vehicle can use 70 or more microcontrollers. Typically, especially \\nfor the smaller, less expensive microcontrollers, they are used as dedicated proces-\\nsors for specific tasks. For example, microcontrollers are heavily utilized in automa-\\ntion processes. By providing simple reactions to input, they can control machinery, \\nturn fans on and off, open and close valves, and so forth. They are integral parts of \\nmodern industrial technology and are among the most inexpensive ways to produce \\nmachinery that can handle extremely complex functionalities.\\nMicrocontrollers come in a range of physical sizes and processing power. Pro-\\ncessors range from 4-bit to 32-bit architectures. Microcontrollers tend to be much \\nslower than microprocessors, typically operating in the MHz range rather than the \\nGHz speeds of microprocessors. Another typical feature of a microcontroller is that \\nit does not provide for human interaction. The microcontroller is programmed for a \\nspecific task, embedded in its device, and executes as and when required.\\nEmbedded versus Deeply Embedded Systems\\nWe have, in this section, defined the concept of an embedded system. A subset of \\nembedded systems, and a quite numerous subset, is referred to as deeply embed-\\nded systems. Although this term is widely used in the technical and commercial \\n1.6 / ARM Architecture\\u2002 \\u200233\\nliterature, you will search the Internet in vain (or at least I did) for a straightfor-\\nward definition. Generally, we can say that a deeply embedded system has a proces-\\nsor whose behavior is difficult to observe both by the programmer and the user. \\n \\nA deeply embedded system uses a microcontroller rather than a microprocessor, is \\nnot programmable once the program logic for the device has been burned into ROM \\n(\\xad\\nread-\\u200b\\n\\xad\\nonly memory), and has no interaction with a user.\\nDeeply embedded systems are dedicated, \\xad\\nsingle-\\u200b\\n\\xad\\npurpose devices that detect \\nsomething in the environment, perform a basic level of processing, and then do some-\\nthing with the results. Deeply embedded systems often have wireless capability and \\nappear in networked configurations, such as networks of sensors deployed over a large \\narea (e.g., factory, agricultural field). The Internet of things depends heavily on deeply \\nembedded systems. Typically, deeply embedded systems have extreme resource con-\\nstraints in terms of memory, processor size, time, and power consumption.\\n\\t 1.6\\t ARM Architecture\\nThe ARM architecture refers to a processor architecture that has evolved from \\nRISC design principles and is used in embedded systems. Chapter\\xa0 15 examines \\nRISC design principles in detail. In this section, we give a brief overview of the \\nARM architecture.\\nA/D\\nconverter\\nAnalog data\\nacquisition\\nTemporary\\ndata\\nProcessor\\nSystem\\nbus\\nRAM\\nD/A\\nconverter\\nROM\\nSerial I/O\\nports\\nEEPROM\\nParallel I/O\\nports\\nTIMER\\nProgram\\nand data\\nPermanent\\ndata\\nTiming\\nfunctions\\nAnalog data\\ntransmission\\nSend/receive\\ndata\\nPeripheral\\ninterfaces\\nFigure\\xa01.15\\u2003 Typical Microcontroller Chip Elements\\n34\\u2002 \\u2002 Chapter 1 / Basic Concepts and Computer Evolution \\nARM Evolution\\nARM is a family of \\xad\\nRISC-\\u200b\\n\\xad\\nbased microprocessors and microcontrollers designed by \\nARM Holdings, Cambridge, England. The company doesn’t make processors but \\ninstead designs microprocessor and multicore architectures and licenses them to man-\\nufacturers. Specifically, ARM Holdings has two types of licensable products: proces-\\nsors and processor architectures. For processors, the customer buys the rights to use \\n\\xad\\nARM-\\u200b\\n\\xad\\nsupplied design in their own chips. For a processor architecture, the customer \\nbuys the rights to design their own processor compliant with ARM’s architecture.\\nARM chips are \\xad\\nhigh-\\u200b\\n\\xad\\nspeed processors that are known for their small die size \\nand low power requirements. They are widely used in smartphones and other hand-\\nheld devices, including game systems, as well as a large variety of consumer prod-\\nucts. ARM chips are the processors in Apple’s popular iPod and iPhone devices, \\nand are used in virtually all Android smartphones as well. ARM is probably the \\nmost widely used embedded processor architecture and indeed the most widely \\nused processor architecture of any kind in the world [VANC14].\\nThe origins of ARM technology can be traced back to the \\xad\\nBritish-\\u200b\\n\\xad\\nbased Acorn \\nComputers company. In the early 1980s, Acorn was awarded a contract by the Brit-\\nish Broadcasting Corporation (BBC) to develop a new microcomputer architecture \\nfor the BBC Computer Literacy Project. The success of this contract enabled Acorn \\nto go on to develop the first commercial RISC processor, the Acorn RISC Machine \\n(ARM). The first version, ARM1, became operational in 1985 and was used for \\ninternal research and development as well as being used as a coprocessor in the \\nBBC machine.\\nIn this early stage, Acorn used the company VLSI Technology to do the actual \\nfabrication of the processor chips. VLSI was licensed to market the chip on its own \\nand had some success in getting other companies to use the ARM in their products, \\nparticularly as an embedded processor.\\nThe ARM design matched a growing commercial need for a \\xad\\nhigh-\\u200b\\n\\xad\\nperformance, \\n\\xad\\nlow-\\u200b\\n\\xad\\npower-\\u200b\\n\\xad\\nconsumption, \\xad\\nsmall-\\u200b\\n\\xad\\nsize, and \\xad\\nlow-\\u200b\\n\\xad\\ncost processor for embedded appli-\\ncations. But further development was beyond the scope of Acorn’s capabilities. \\nAccordingly, a new company was organized, with Acorn, VLSI, and Apple Com-\\nputer as founding partners, known as ARM Ltd. The Acorn RISC Machine became \\nAdvanced RISC Machines.12\\nInstruction Set Architecture\\nThe ARM instruction set is highly regular, designed for efficient implementation of \\nthe processor and efficient execution. All instructions are 32 bits long and follow a \\nregular format. This makes the ARM ISA suitable for implementation over a wide \\nrange of products.\\nAugmenting the basic ARM ISA is the Thumb instruction set, which is a \\xad\\nre-\\u200b\\n\\xad\\nencoded subset of the ARM instruction set. Thumb is designed to increase the per-\\nformance of ARM implementations that use a 16-bit or narrower memory data bus, \\n12The company dropped the designation Advanced RISC Machines in the late 1990s. It is now simply \\nknown as the ARM architecture.\\n1.6 / ARM Architecture\\u2002 \\u200235\\nand to allow better code density than provided by the ARM instruction set. The \\nThumb instruction set contains a subset of the ARM 32-bit instruction set recoded \\ninto 16-bit instructions. The current defined version is \\xad\\nThumb-\\u200b\\n\\xad\\n2.\\nThe ARM and \\xad\\nThumb-\\u200b\\n\\xad\\n2 ISAs are discussed in Chapters\\xa012 and 13.\\nARM Products\\nARM Holdings licenses a number of specialized microprocessors and related tech-\\nnologies, but the bulk of their product line is the Cortex family of microprocessor \\narchitectures. There are three Cortex architectures, conveniently labeled with the \\ninitials A, R, and\\xa0M.\\n\\xad\\ncortex-\\u200b\\n\\xad\\na/\\xad\\ncortex-\\u200b\\n\\xad\\na50 The \\xad\\nCortex-\\u200b\\n\\xad\\nA and \\xad\\nCortex-\\u200b\\n\\xad\\nA50 are application \\nprocessors, intended for mobile devices such as smartphones and eBook readers, \\nas well as consumer devices such as digital TV and home gateways (e.g., DSL and \\ncable Internet modems). These processors run at higher clock frequency (over \\n \\n1 GHz), and support a memory management unit (MMU), which is required for full \\nfeature OSs such as Linux, Android, MS Windows, and mobile OSs. An MMU is \\na hardware module that supports virtual memory and paging by translating virtual \\naddresses into physical addresses; this topic is explored in Chapter\\xa08.\\nThe two architectures use both the ARM and \\xad\\nThumb-\\u200b\\n\\xad\\n2 instruction sets; the \\nprincipal difference is that the \\xad\\nCortex-\\u200b\\n\\xad\\nA is a 32-bit machine, and the \\xad\\nCortex-\\u200b\\n\\xad\\nA50 is \\na 64-bit machine.\\n\\xad\\ncortex-\\u200b\\n\\xad\\nr The \\xad\\nCortex-\\u200b\\n\\xad\\nR is designed to support \\xad\\nreal-\\u200b\\n\\xad\\ntime applications, in which \\nthe timing of events needs to be controlled with rapid response to events. They can \\nrun at a fairly high clock frequency (e.g., 200MHz to 800MHz) and have very low \\nresponse latency. The \\xad\\nCortex-\\u200b\\n\\xad\\nR includes enhancements both to the instruction set \\nand to the processor organization to support deeply embedded \\xad\\nreal-\\u200b\\n\\xad\\ntime devices. \\nMost of these processors do not have MMU; the limited data requirements and \\nthe limited number of simultaneous processes eliminates the need for elaborate \\nhardware and software support for virtual memory. The \\xad\\nCortex-\\u200b\\n\\xad\\nR does have a \\nMemory Protection Unit (MPU), cache, and other memory features designed for \\nindustrial applications. An MPU is a hardware module that prohibits one program \\nin memory from accidentally accessing memory assigned to another active program. \\nUsing various methods, a protective boundary is created around the program, and \\ninstructions within the program are prohibited from referencing data outside of that \\nboundary.\\nExamples of embedded systems that would use the \\xad\\nCortex-\\u200b\\n\\xad\\nR are automotive \\nbraking systems, mass storage controllers, and networking and printing devices.\\n\\xad\\ncortex-\\u200b\\n\\xad\\nm \\xad\\nCortex-\\u200b\\n\\xad\\nM series processors have been developed primarily for the \\nmicrocontroller domain where the need for fast, highly deterministic interrupt \\nmanagement is coupled with the desire for extremely low gate count and \\nlowest possible power consumption. As with the \\xad\\nCortex-\\u200b\\n\\xad\\nR series, the \\xad\\nCortex-\\u200b\\n\\xad\\nM \\narchitecture has an MPU but no\\xa0MMU.\\xa0The \\xad\\nCortex-\\u200b\\n\\xad\\nM uses only the \\xad\\nThumb-\\u200b\\n\\xad\\n2 \\ninstruction set. The market for the \\xad\\nCortex-\\u200b\\n\\xad\\nM includes IoT devices, wireless \\nsensor/actuator networks used in factories and other enterprises, automotive \\nbody electronics, and so on.\\n36\\u2002 \\u2002 Chapter 1 / Basic Concepts and Computer Evolution \\nThere are currently four versions of the \\xad\\nCortex-\\u200b\\n\\xad\\nM series:\\n■\\n■\\xad\\nCortex-\\u200b\\n\\xad\\nM0: Designed for 8- and 16-bit applications, this model emphasizes low \\ncost, ultra low power, and simplicity. It is optimized for small silicon die size \\n(starting from 12k gates) and use in the lowest cost chips.\\n■\\n■\\xad\\nCortex-\\u200b\\n\\xad\\nM0+: An enhanced version of the M0 that is more energy efficient.\\n■\\n■\\xad\\nCortex-\\u200b\\n\\xad\\nM3: Designed for 16- and 32-bit applications, this model emphasizes \\nperformance and energy efficiency. It also has comprehensive debug and trace \\nfeatures to enable software developers to develop their applications quickly.\\n■\\n■\\xad\\nCortex-\\u200b\\n\\xad\\nM4: This model provides all the features of the \\xad\\nCortex-\\u200b\\n\\xad\\nM3, with addi-\\ntional instructions to support digital signal processing tasks.\\nIn this text, we will primarily use the ARM \\xad\\nCortex-\\u200b\\n\\xad\\nM3 as our example embed-\\nded system processor. It is the best suited of all ARM models for \\xad\\ngeneral-\\u200b\\n\\xad\\npurpose \\nmicrocontroller use. The \\xad\\nCortex-\\u200b\\n\\xad\\nM3 is used by a variety of manufacturers of micro-\\ncontroller products. Initial microcontroller devices from lead partners already \\ncombine the \\xad\\nCortex-\\u200b\\n\\xad\\nM3 processor with flash, SRAM, and multiple peripherals to \\nprovide a competitive offering at the price of just $1.\\nFigure\\xa01.16 provides a block diagram of the EFM32 microcontroller from Sil-\\nicon Labs. The figure also shows detail of the \\xad\\nCortex-\\u200b\\n\\xad\\nM3 processor and core com-\\nponents. We examine each level in turn.\\nThe \\xad\\nCortex-\\u200b\\n\\xad\\nM3 core makes use of separate buses for instructions and data. \\nThis arrangement is sometimes referred to as a Harvard architecture, in contrast \\nwith the von Neumann architecture, which uses the same signal buses and mem-\\nory for both instructions and data. By being able to read both an instruction and \\ndata from memory at the same time, the \\xad\\nCortex-\\u200b\\n\\xad\\nM3 processor can perform many \\noperations in parallel, speeding application execution. The core contains a decoder \\nfor Thumb instructions, an advanced ALU with support for hardware multiply and \\ndivide, control logic, and interfaces to the other components of the processor. In \\nparticular, there is an interface to the nested vector interrupt controller (NVIC) and \\nthe embedded trace macrocell (ETM) module.\\nThe core is part of a module called the \\xad\\nCortex-\\u200b\\n\\xad\\nM3 processor. This term is \\nsomewhat misleading, because typically in the literature, the terms core and pro-\\ncessor are viewed as equivalent. In addition to the core, the processor includes the \\nfollowing elements:\\n■\\n■NVIC: Provides configurable interrupt handling abilities to the processor. It \\nfacilitates \\xad\\nlow-\\u200b\\n\\xad\\nlatency exception and interrupt handling, and controls power \\nmanagement.\\n■\\n■ETM: An optional debug component that enables reconstruction of program \\nexecution. The ETM is designed to be a \\xad\\nhigh-\\u200b\\n\\xad\\nspeed, \\xad\\nlow-\\u200b\\n\\xad\\npower debug tool \\nthat only supports instruction trace.\\n■\\n■Debug access port (DAP): This provides an interface for external debug \\naccess to the processor.\\n■\\n■Debug logic: Basic debug functionality includes processor halt, \\xad\\nsingle-\\u200b\\n\\xad\\nstep, \\nprocessor core register access, unlimited software breakpoints, and full system \\nmemory access.\\nCortex-M3 Core\\nMicrocontroller Chip\\nCortex-M3\\nProcessor \\nNVIC\\ninterface\\nETM\\ninterface\\nHardware\\ndivider\\n32-bit\\nmultiplier\\n32-bit ALU\\nControl\\nlogic\\nThumb\\ndecode\\nInstruction\\ninterface\\nData\\ninterface\\nICode\\ninterface\\nDebug logic\\nARM\\ncore\\nDAP\\nNVIC\\nETM\\nMemory\\nprotection unit\\nBus matrix\\nSRAM &\\nperipheral I/F\\nSecurity\\nAnalog Interfaces\\nTimers & Triggers\\nParallel I/O Ports\\nSerial Interfaces\\nPeripheral bus\\nCore and memory\\nClock management\\nEnergy management\\nCortex-M3 processor\\nMemory\\nprotec-\\ntion unit\\nFlash\\nmemory\\n64 kB\\nVoltage\\nregula-\\ntor\\nPower-\\non reset\\nBrown-\\nout de-\\ntector\\nVoltage\\ncompar-\\nator\\nHigh fre-\\nquency RC\\noscillator\\nLow fre-\\nquency RC\\noscillator\\nHigh freq\\ncrystal\\noscillator\\nLow freq\\ncrystal\\noscillator\\nSRAM\\nmemory\\n64 kB\\nDebug\\ninter-\\nface\\nDMA\\ncontrol-\\nler\\nPulse\\ncounter\\nWatch-\\ndog tmr\\nLow\\nenergy\\nReal\\ntime ctr\\nPeriph\\nbus int\\nTimer/\\ncounter\\nGeneral\\npurpose\\nI/O\\nExternal\\nInter-\\nrupts\\nUART\\nUSART\\nLow-\\nenergy\\nUART\\nUSB\\nPin\\nreset\\n32-bit bus\\nA/D\\ncon-\\nverter\\nHard-\\nware\\nAES\\nD/A\\ncon-\\nverter\\nFigure\\xa01.16\\u2003 Typical Microcontroller Chip Based on \\xad\\nCortex-\\u200b\\n\\xad\\nM3\\n37\\n38\\u2002 \\u2002 Chapter 1 / Basic Concepts and Computer Evolution \\n■\\n■ICode interface: Fetches instructions from the code memory space.\\n■\\n■SRAM & peripheral interface: Read/write interface to data memory and \\nperipheral devices.\\n■\\n■Bus matrix: Connects the core and debug interfaces to external buses on the \\nmicrocontroller.\\n■\\n■Memory protection unit: Protects critical data used by the operating system \\nfrom user applications, separating processing tasks by disallowing access \\nto each other’s data, disabling access to memory regions, allowing memory \\nregions to be defined as \\xad\\nread-\\u200b\\n\\xad\\nonly, and detecting unexpected memory accesses \\nthat could potentially break the system.\\nThe upper part of Figure\\xa01.16 shows the block diagram of a typical micro-\\ncontroller built with the \\xad\\nCortex-\\u200b\\n\\xad\\nM3, in this case the EFM32 microcontroller. This \\nmicrocontroller is marketed for use in a wide variety of devices, including energy, \\ngas, and water metering; alarm and security systems; industrial automation devices; \\nhome automation devices; smart accessories; and health and fitness devices. The sil-\\nicon chip consists of 10 main areas:13\\n■\\n■Core and memory: This region includes the \\xad\\nCortex-\\u200b\\n\\xad\\nM3 processor, static RAM \\n(SRAM) data memory,14 and flash memory15 for storing program instructions \\nand nonvarying application data. Flash memory is nonvolatile (data is not lost \\nwhen power is shut off) and so is ideal for this purpose. The SRAM stores \\nvariable data. This area also includes a debug interface, which makes it easy to \\nreprogram and update the system in the field.\\n■\\n■Parallel I/O ports: Configurable for a variety of parallel I/O schemes.\\n■\\n■Serial interfaces: Supports various serial I/O schemes.\\n■\\n■Analog interfaces: \\xad\\nAnalog-\\u200b\\n\\xad\\nto-\\u200b\\n\\xad\\ndigital and \\xad\\ndigital-\\u200b\\n\\xad\\nto-\\u200b\\n\\xad\\nanalog logic to support \\nsensors and actuators.\\n■\\n■Timers and triggers: Keeps track of timing and counts events, generates out-\\nput waveforms, and triggers timed actions in other peripherals.\\n■\\n■Clock management: Controls the clocks and oscillators on the chip. Multiple \\nclocks and oscillators are used to minimize power consumption and provide \\nshort startup times.\\n■\\n■Energy management: Manages the various \\xad\\nlow-\\u200b\\n\\xad\\nenergy modes of operation of \\nthe processor and peripherals to provide \\xad\\nreal-\\u200b\\n\\xad\\ntime management of the energy \\nneeds so as to minimize energy consumption.\\n■\\n■Security: The chip includes a hardware implementation of the Advanced \\nEncryption Standard (AES).\\n13This discussion does not go into details about all of the individual modules; for the interested reader, an \\n\\xad\\nin-\\u200b\\n\\xad\\ndepth discussion is provided in the document EFM32G200.pdf, available at box.com/COA10e.\\n14Static RAM (SRAM) is a form of \\xad\\nrandom-\\u200b\\n\\xad\\naccess memory used for cache memory; see Chapter\\xa05.\\n15Flash memory is a versatile form of memory used both in microcontrollers and as external memory; it \\nis discussed in Chapter\\xa06.\\n1.7 / Cloud Computing\\u2002 \\u200239\\n■\\n■32-bit bus: Connects all of the components on the chip.\\n■\\n■Peripheral bus: A network which lets the different peripheral module commu-\\nnicate directly with each other without involving the processor. This supports \\n\\xad\\ntiming-\\u200b\\n\\xad\\ncritical operation and reduces software overhead.\\nComparing Figure\\xa01.16 with Figure\\xa01.2, you will see many similarities and \\nthe same general hierarchical structure. Note, however, that the top level of a \\nmicrocontroller computer system is a single chip, whereas for a multicore com-\\nputer, the top level is a motherboard containing a number of chips. Another note-\\nworthy difference is that there is no cache, neither in the \\xad\\nCortex-\\u200b\\n\\xad\\nM3 processor \\nnor in the microcontroller as a whole, which plays an important role if the code or \\ndata resides in external memory. Though the number of cycles to read the instruc-\\ntion or data varies depending on cache hit or miss, the cache greatly improves the \\nperformance when external memory is used. Such overhead is not needed for a \\nmicrocontroller.\\n\\t 1.7\\t Cloud Computing\\nAlthough the general concepts for cloud computing go back to the 1950s, cloud \\ncomputing services first became available in the early 2000s, particularly targeted \\nat large enterprises. Since then, cloud computing has spread to small and medium \\nsize businesses, and most recently to consumers. Apple’s iCloud was launched in \\n2012 and had 20\\xa0million users within a week of launch. Evernote, the \\xad\\ncloud-\\u200b\\n\\xad\\nbased \\nnotetaking and archiving service, launched in 2008, approached 100\\xa0million users \\nin less than 6\\xa0years. In this section, we provide a brief overview. Cloud computing is \\nexamined in more detail in Chapter\\xa017\\n.\\nBasic Concepts\\nThere is an increasingly prominent trend in many organizations to move a substantial \\nportion or even all information technology (IT) operations to an \\xad\\nInternet-\\u200b\\n\\xad\\nconnected \\ninfrastructure known as enterprise cloud computing. At the same time, individual \\nusers of PCs and mobile devices are relying more and more on cloud computing \\nservices to backup data, synch devices, and share, using personal cloud computing. \\nNIST defines cloud computing, in NIST \\xad\\nSP-\\u200b\\n\\xad\\n800-145 (The NIST Definition of Cloud \\nComputing), as follows:\\nCloud computing: A model for enabling ubiquitous, convenient, \\xad\\non-\\u200b\\n\\xad\\ndemand network \\naccess to a shared pool of configurable computing resources (e.g., networks, servers, \\nstorage, applications, and services) that can be rapidly provisioned and released with \\nminimal management effort or service provider interaction.\\nBasically, with cloud computing, you get economies of scale, professional \\nnetwork management, and professional security management. These features can \\nbe attractive to companies large and small, government agencies, and individual \\nPC and mobile users. The individual or company only needs to pay for the storage \\n40\\u2002 \\u2002 Chapter 1 / Basic Concepts and Computer Evolution \\ncapacity and services they need. The user, be it company or individual, doesn’t have \\nthe hassle of setting up a database system, acquiring the hardware they need, doing \\nmaintenance, and backing up the \\xad\\ndata—\\u200b\\n\\xad\\nall these are part of the cloud service.\\nIn theory, another big advantage of using cloud computing to store your data \\nand share it with others is that the cloud provider takes care of security. Alas, the \\ncustomer is not always protected. There have been a number of security failures \\namong cloud providers. Evernote made headlines in early 2013 when it told all of its \\nusers to reset their passwords after an intrusion was discovered.\\nCloud networking refers to the networks and network management function-\\nality that must be in place to enable cloud computing. Most cloud computing solu-\\ntions rely on the Internet, but that is only a piece of the networking infrastructure. \\nOne example of cloud networking is the provisioning of \\xad\\nhigh-\\u200b\\n\\xad\\nperformance and/or \\n\\xad\\nhigh-\\u200b\\n\\xad\\nreliability networking between the provider and subscriber. In this case, some \\nor all of the traffic between an enterprise and the cloud bypasses the Internet and \\nuses dedicated private network facilities owned or leased by the cloud service pro-\\nvider. More generally, cloud networking refers to the collection of network capa-\\nbilities required to access a cloud, including making use of specialized services over \\nthe Internet, linking enterprise data centers to a cloud, and using firewalls and other \\nnetwork security devices at critical points to enforce access security policies.\\nWe can think of cloud storage as a subset of cloud computing. In essence, cloud \\nstorage consists of database storage and database applications hosted remotely on \\ncloud servers. Cloud storage enables small businesses and individual users to take \\nadvantage of data storage that scales with their needs and to take advantage of a \\nvariety of database applications without having to buy, maintain, and manage the \\nstorage assets.\\nCloud Services\\nThe essential purpose of cloud computing is to provide for the convenient rental \\nof computing resources. A cloud service provider (CSP) maintains computing and \\ndata storage resources that are available over the Internet or private networks. \\nCustomers can rent a portion of these resources as needed. Virtually all cloud ser-\\nvice is provided using one of three models (Figure\\xa01.17): SaaS, PaaS, and IaaS, which \\nwe examine in this section.\\nsoftware as a service (SaaS) As the name implies, a SaaS cloud provides \\nservice to customers in the form of software, specifically application software, \\nrunning on and accessible in the cloud. SaaS follows the familiar model of Web \\nservices, in this case applied to cloud resources. SaaS enables the customer to use \\nthe cloud provider’s applications running on the provider’s cloud infrastructure. The \\napplications are accessible from various client devices through a simple interface \\nsuch as a Web browser. Instead of obtaining desktop and server licenses for \\nsoftware products it uses, an enterprise obtains the same functions from the cloud \\nservice. SaaS saves the complexity of software installation, maintenance, upgrades, \\nand patches. Examples of services at this level are Gmail, Google’s \\xad\\ne-\\u200b\\n\\xad\\nmail service, \\nand Salesforce.com, which help firms keep track of their customers.\\nCommon subscribers to SaaS are organizations that want to provide their \\nemployees with access to typical office productivity software, such as document \\n1.7 / Cloud Computing\\u2002 \\u200241\\nmanagement and email. Individuals also commonly use the SaaS model to acquire \\ncloud resources. Typically, subscribers use specific applications on demand. The \\ncloud provider also usually offers \\xad\\ndata-\\u200b\\n\\xad\\nrelated features such as automatic backup \\nand data sharing between subscribers.\\nplatform as a service (PaaS) A PaaS cloud provides service to customers in \\nthe form of a platform on which the customer’s applications can run. PaaS enables \\nthe customer to deploy onto the cloud infrastructure containing \\xad\\ncustomer-\\u200b\\n\\xad\\ncreated \\nor acquired applications. A PaaS cloud provides useful software building blocks, \\nplus a number of development tools, such as programming languages, \\xad\\nrun-\\u200b\\n\\xad\\ntime \\nenvironments, and other tools that assist in deploying new applications. In effect, \\nPaaS is an operating system in the cloud. PaaS is useful for an organization that \\nwants to develop new or tailored applications while paying for the needed computing \\nresources only as needed and only for as long as needed. Google App Engine and \\nthe Salesforce1 Platform from Salesforce.com are examples of PaaS.\\nApplications\\nInfrastructure as\\na service (IaaS)\\nTraditional IT\\narchitecture\\nPlatform as a\\nservice (PaaS)\\nSoftware as a\\nservice (SaaS)\\nManaged by client\\nApplication\\nFramework\\nCompilers\\nRun-time\\nenvironment\\nDatabases\\nOperating\\nsystem\\nVirtual\\nmachine\\nServer\\nhardware\\nStorage\\nNetworking\\nApplications\\nApplication\\nFramework\\nCompilers\\nRun-time\\nenvironment\\nDatabases\\nOperating\\nsystem\\nVirtual\\nmachine\\nServer\\nhardware\\nStorage\\nNetworking\\nMore complex\\nMore upfront cost\\nLess scalable\\nMore customizable\\nLess complex\\nLower upfront cost\\nMore scalable\\nLess customizable\\nIT = information technology\\nCSP = cloud service provider\\nManaged by CSP\\nApplications\\nManaged\\nby client\\nApplication\\nFramework\\nCompilers\\nRun-time\\nenvironment\\nDatabases\\nOperating\\nsystem\\nVirtual\\nmachine\\nServer\\nhardware\\nStorage\\nNetworking\\nManaged by CSP\\nApplications\\nApplication\\nFramework\\nCompilers\\nRun-time\\nenvironment\\nDatabases\\nOperating\\nsystem\\nVirtual\\nmachine\\nServer\\nhardware\\nStorage\\nNetworking\\nManaged by CSP\\nFigure\\xa01.17\\u2003 Alternative Information Technology Architectures\\n42\\u2002 \\u2002 Chapter 1 / Basic Concepts and Computer Evolution \\ninfrastructure as a service (IaaS) With IaaS, the customer has access to the \\nunderlying cloud infrastructure. IaaS provides virtual machines and other abstracted \\nhardware and operating systems, which may be controlled through a service \\napplication programming interface (API). IaaS offers the customer processing, \\nstorage, networks, and other fundamental computing resources so that the customer \\nis able to deploy and run arbitrary software, which can include operating systems \\nand applications. IaaS enables customers to combine basic computing services, \\nsuch as number crunching and data storage, to build highly adaptable computer \\nsystems. Examples of IaaS are Amazon Elastic Compute Cloud (Amazon EC2) and \\nWindows Azure.\\n\\t 1.8\\t Key Terms, Review Questions, and Problems\\nKey Terms\\napplication processor\\narithmetic and logic unit \\n(ALU)\\nARM\\ncentral processing unit  \\n(CPU)\\nchip\\ncloud computing\\ncloud networking\\ncloud storage\\ncomputer architecture\\ncomputer organization\\ncontrol unit\\ncore\\ndedicated processor\\ndeeply embedded system\\nembedded system\\ngate\\ninfrastructure as a service \\n(IaaS)\\n\\xad\\ninput–\\u200b\\n\\xad\\noutput (I/O)\\ninstruction set architecture \\n(ISA)\\nintegrated circuit\\nIntel x86\\nInternet of things (IoT)\\nmain memory\\nmemory cell\\nmemory management unit \\n(MMU)\\nmemory protection unit  \\n(MPU)\\nmicrocontroller\\nmicroelectronics\\nmicroprocessor\\nmotherboard\\nmulticore\\nmulticore processor\\noriginal equipment  \\nmanufacturer (OEM)\\nplatform as a service  \\n(PaaS)\\nprinted circuit board\\nprocessor\\nregisters\\nsemiconductor\\nsemiconductor memory\\nsoftware as a service (SaaS)\\nsystem bus\\nsystem interconnection\\nvacuum tubes\\nReview Questions\\n\\t 1.1\\t\\nWhat, in general terms, is the distinction between computer organization and com-\\nputer architecture?\\n\\t 1.2\\t\\nWhat, in general terms, is the distinction between computer structure and computer \\nfunction?\\n\\t 1.3\\t\\nWhat are the four main functions of a computer?\\n\\t 1.4\\t\\nList and briefly define the main structural components of a computer.\\n\\t 1.5\\t\\nList and briefly define the main structural components of a processor.\\n\\t 1.6\\t\\nWhat is a stored program computer?\\n\\t 1.7\\t\\nExplain Moore’s law.\\n\\t 1.8\\t\\nList and explain the key characteristics of a computer family.\\n\\t 1.9\\t\\nWhat is the key distinguishing feature of a microprocessor?\\n1.8 / Key Terms, Review Questions, and Problems\\u2002 \\u200243\\nProblems\\n\\t 1.1\\t\\nYou are to write an IAS program to compute the results of the following equation.\\nY = a\\nN\\nX=1\\nX\\nAssume that the computation does not result in an arithmetic overflow and that X, Y, \\nand N are positive integers with N ≥ 1. Note: The IAS did not have assembly language, \\nonly machine language.\\na.\\t Use the equation Sum(Y) =\\nN(N + 1)\\n2\\n when writing the IAS program.\\nb.\\t Do it the “hard way,” without using the equation from part (a).\\n\\t\\n1.2\\t\\na.\\t \\x07\\nOn the IAS, what would the machine code instruction look like to load the con-\\ntents of memory address 2 to the accumulator?\\n\\t\\n\\t\\nb.\\t \\x07\\nHow many trips to memory does the CPU need to make to complete this instruc-\\ntion during the instruction cycle?\\n\\t 1.3\\t\\nOn the IAS, describe in English the process that the CPU must undertake to read a \\nvalue from memory and to write a value to memory in terms of what is put into the \\nMAR, MBR, address bus, data bus, and control bus.\\n\\t 1.4\\t\\nGiven the memory contents of the IAS computer shown below,\\nAddress\\nContents\\n08A\\n010FA210FB\\n08B\\n010FA0F08D\\n08C\\n020FA210FB\\nshow the assembly language code for the program, starting at address 08A.\\xa0Explain \\nwhat this program does.\\n\\t 1.5\\t\\nIn Figure\\xa01.6, indicate the width, in bits, of each data path (e.g., between AC and ALU).\\n\\t 1.6\\t\\nIn the IBM 360 Models 65 and 75, addresses are staggered in two separate main mem-\\nory units (e.g., all \\xad\\neven-\\u200b\\n\\xad\\nnumbered words in one unit and all \\xad\\nodd-\\u200b\\n\\xad\\nnumbered words in \\nanother). What might be the purpose of this technique?\\n\\t 1.7\\t\\nThe relative performance of the IBM 360 Model 75 is 50 times that of the 360 Model \\n30, yet the instruction cycle time is only 5 times as fast. How do you account for this \\ndiscrepancy?\\n\\t 1.8\\t\\nWhile browsing at Billy Bob’s computer store, you overhear a customer asking Billy \\nBob what is the fastest computer in the store that he can buy. Billy Bob replies, “You’re \\nlooking at our Macintoshes. The fastest Mac we have runs at a clock speed of 1.2 GHz. \\nIf you really want the fastest machine, you should buy our 2.4-GHz Intel Pentium IV \\ninstead.” Is Billy Bob correct? What would you say to help this customer?\\n\\t 1.9\\t\\nThe ENIAC, a precursor to the ISA machine, was a decimal machine, in which each \\nregister was represented by a ring of 10 vacuum tubes. At any time, only one vacuum \\ntube was in the ON state, representing one of the 10 decimal digits. Assuming that \\nENIAC had the capability to have multiple vacuum tubes in the ON and OFF state \\nsimultaneously, why is this representation “wasteful” and what range of integer values \\ncould we represent using the 10 vacuum tubes?\\n\\t 1.10\\t\\nFor each of the following examples, determine whether this is an embedded system, \\nexplaining why or why not.\\na.\\t Are programs that understand physics and/or hardware embedded? For example, \\none that uses \\xad\\nfinite-\\u200b\\n\\xad\\nelement methods to predict fluid flow over airplane wings?\\nb.\\t Is the internal microprocessor controlling a disk drive an example of an embedded \\nsystem?\\n44\\u2002 \\u2002 Chapter 1 / Basic Concepts and Computer Evolution \\nc.\\t I/O drivers control hardware, so does the presence of an I/O driver imply that the \\ncomputer executing the driver is embedded?\\nd.\\t Is a PDA (Personal Digital Assistant) an embedded system?\\ne.\\t Is the microprocessor controlling a cell phone an embedded system?\\nf.\\t Are the computers in a big \\xad\\nphased-\\u200b\\n\\xad\\narray radar considered embedded? These \\nradars are 10-story buildings with one to three 100-foot diameter radiating patches \\non the sloped sides of the building.\\ng.\\t Is a traditional flight management system (FMS) built into an airplane cockpit \\nconsidered embedded?\\nh.\\t Are the computers in a \\xad\\nhardware-\\u200b\\n\\xad\\nin-\\u200b\\n\\xad\\nthe-\\u200b\\n\\xad\\nloop (HIL) simulator embedded?\\ni.\\t\\nIs the computer controlling a pacemaker in a person’s chest an embedded \\ncomputer?\\nj.\\t Is the computer controlling fuel injection in an automobile engine embedded?\\n45\\nChapter\\nPerformance Issues\\n2.1\\t\\nDesigning for Performance\\t\\nMicroprocessor Speed\\nPerformance Balance\\nImprovements in Chip Organization and Architecture\\n2.2\\t\\nMulticore, MICs, and GPGPUs\\t\\n2.3\\t\\nTwo Laws that Provide Insight: Amdahl’s Law and Little’s Law\\t\\nAmdahl’s Law\\nLittle’s Law\\n2.4\\t\\nBasic Measures of Computer Performance\\t\\nClock Speed\\nInstruction Execution Rate\\n2.5\\t\\nCalculating the Mean\\t\\nArithmetic Mean\\nHarmonic Mean\\nGeometric Mean\\n2.6\\t\\nBenchmarks and SPEC\\t\\nBenchmark Principles\\nSPEC Benchmarks\\n2.7\\t\\nKey Terms, Review Questions, and Problems\\t\\n'"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chapter_dict[1]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-28T16:28:37.146906698Z",
     "start_time": "2024-04-28T16:28:37.084641011Z"
    }
   },
   "id": "369a7441efec674a",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "109820"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chapter_dict[1])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-28T16:28:54.660343039Z",
     "start_time": "2024-04-28T16:28:54.639364438Z"
    }
   },
   "id": "cb26b7831c5127ef",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-28 18:25:37.832312: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-28 18:25:37.832347: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-28 18:25:37.857186: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-04-28 18:25:37.910734: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "def summarize_chapters(chapters):\n",
    "    summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "    summaries = {}\n",
    "    for chapter, text in chapters.items():\n",
    "        summaries[chapter] = summarizer(text, max_length=130, min_length=30, do_sample=False)[0]['summary_text']\n",
    "    return summaries"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-28T16:25:41.317411132Z",
     "start_time": "2024-04-28T16:25:36.847104959Z"
    }
   },
   "id": "b7368099ec2e86f9",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#summaries = summarize_chapters(chapter_dict)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-28T16:25:41.330125125Z",
     "start_time": "2024-04-28T16:25:41.319674418Z"
    }
   },
   "id": "3d9a9738a9daba0f",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#for chapter, summary in summaries.items():\n",
    "#    print(f\"{chapter}: {summary}\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-28T16:25:41.330553698Z",
     "start_time": "2024-04-28T16:25:41.323317125Z"
    }
   },
   "id": "99f76912fa227cfd",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-28 18:25:41.859669: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-28 18:25:42.070338: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-28 18:25:42.070532: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-28 18:25:42.071545: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-28 18:25:42.071703: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-28 18:25:42.071845: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-28 18:25:42.149165: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-28 18:25:42.149401: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-28 18:25:42.149575: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-28 18:25:42.149686: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5363 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050, pci bus id: 0000:09:00.0, compute capability: 8.6\n",
      "All PyTorch model weights were used when initializing TFBartForConditionalGeneration.\n",
      "\n",
      "All the weights of TFBartForConditionalGeneration were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBartForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-28T16:25:46.012606649Z",
     "start_time": "2024-04-28T16:25:41.326892747Z"
    }
   },
   "id": "23255a96328d775d",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import math\n",
    "def summarize_text(text):\n",
    "    \"\"\"\n",
    "    Summarizes the provided text using a pre-initialized summarizer.\n",
    "    \n",
    "    Args:\n",
    "    text (str): The text to summarize.\n",
    "\n",
    "    Returns:\n",
    "    str: The summarized text.\n",
    "    \"\"\"\n",
    "    input_length = len(text.split())  # Nombre de mots dans le texte d'entrée\n",
    "    max_length = max(30, math.ceil(input_length / 4))  # Ne pas descendre en dessous d'une longueur minimale, par exemple 30 mots\n",
    "    min_length = max(20, math.ceil(input_length / 6))\n",
    "    # Generate summary\n",
    "    summary = summarizer(text, max_length=max_length, min_length=min_length, do_sample=False)\n",
    "    \n",
    "    # Return only the summary text\n",
    "    return summary[0]['summary_text']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-28T16:25:46.020488907Z",
     "start_time": "2024-04-28T16:25:46.016014050Z"
    }
   },
   "id": "ec5f2bdeb587763c",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-28 18:25:48.758342: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 51499948096 exceeds 10% of free system memory.\n",
      "2024-04-28 18:25:57.952588: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 51499948096 exceeds 10% of free system memory.\n",
      "2024-04-28 18:27:19.997731: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 51499948096 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "text_chapter_1 = summarize_text(text=chapter_dict[1])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-28T16:25:46.021394121Z"
    }
   },
   "id": "c6794c164b771475"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "be9d5c96ee66064"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "a55b2b332f9f319a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
